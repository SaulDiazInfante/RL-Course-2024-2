## Summary

Outline: 

1. **$\epsilon$-greedy Method**:
	- Overview of the method.
	- Mathematical formulation.
	- Parameter sensitivity and tuning.
2. **UCB (Upper Confidence Bound) Method**:
    -   Explanation of UCB and its deterministic exploration mechanism.
    -   The mathematical equation governing UCB.
    -   Parameter considerations and impact on performance.
3.  **Gradient Bandit Algorithm**:
    -   Introduction to action preferences and how they differ from action
        values.
    -   Derivation of the softmax probability distribution.
    -   Discussion on parameter choice and influence on outcomes.
4.  **Optimistic Initialization**:
    -   How optimistic estimates influence exploration.
    -   Comparison with $\epsilon$-greedy methods.

Letâ€™s begin with the $\epsilon$ -greedy method:

### 1. $\epsilon$-greedy Method

This is one of the simplest algorithms to balance exploration and exploitation.
The method works by choosing a random action with probability $\epsilon$
(exploration) and the action with the highest estimated value (exploitation)
with probability $1 - \epsilon$.

#### Mathematical Formulation

-   Let $Q(a)$ be the estimated value of action $a$.
-   At each time step $t$, the agent chooses:
    -   A random action $a$ with probability $\epsilon$.
    -   The action with the maximum $Q(a)$,
    -   $\text{argmax}_a Q(a)$, with probability ${1 - \epsilon}.$

#### Update Rule

The estimate for the value of an action, $Q(a)$, is updated using the following
equation after observing a reward $R_t$ for taking action $a$: 
$$
Q_{t+1}(a) = Q_t(a) + \alpha \left( R_t - Q_t(a) \right)
$$ 

Where: 

- $\alpha$ is the **step size** or learning rate, determining how much the estimate is updated based on new information. 

- $R_t$ is the reward received after action $a$ at time $t$.

#### Parameter Sensitivity

- **$\epsilon$**: A small value of $\epsilon$ (e.g., 0.01) results in a mostly greedy policy with occasional exploration, while a larger $\epsilon$ (e.g., 0.1) encourages moreexploration. The optimal value balances sufficient exploration to discover rewarding actions while exploiting known high-value actions effectively.

### 2. UCB (Upper Confidence Bound) Method

UCB addresses the exploration-exploitation dilemma by adding a confidence term
to the action value. The idea is to choose actions that might not have the
highest estimated value but have been less explored, thus increasing exploration
in a systematic way.

#### Mathematical Equation

The action $a_t$ selected at time $t$ is: 

$$a_t = \text{argmax}_a 
	\left(
		Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right)
$$

Where: 

- $Q_t(a)$ is the estimated value of action $a$ at time $t$. 
- $c$ is a parameter controlling the degree of exploration. A larger $c$ increases exploration. 
- $N_t(a)$ is the number of times action $a$ has been selected so far. 
- $\ln t$ scales the confidence bound logarithmically with time.

This approach encourages the selection of actions with high uncertainty (lower (
N_t(a) )), balancing exploration based on how frequently each action has been
tried.

#### Parameter Considerations

-   The parameter $c$ is crucial. If $c$ is too low, the algorithm might not
    explore enough; if too high, it may explore excessively. The optimal $c$
    varies depending on the problem setting.

### 3. Gradient Bandit Algorithm

Unlike $\epsilon$-greedy and UCB methods that estimate action values, gradient bandit algorithms estimate **action preferences**, denoted $H(a)$. These preferences are used to determine the probability of selecting each action.

#### Softmax Distribution

The probability of selecting action $a$ is given by: 
$$
	\pi(a) =\dfrac{e^{H(a)}}{\displaystyle \sum_{b=1} ^{k} e^{H(b)}}
$$ 

Where: 

- $H(a)$ is the preference for action $a$. 
- $\pi(a)$ represents the probability of taking action $a$.

#### Update Rule

The preferences are updated based on the received reward as follows: 

$$
\begin{aligned}
	H_{t+1}(a) &= 
		H_t(a) + \alpha (R_t - \bar{R}_t)(1 - \pi_t(a))
		\quad \text{if action } a \text{ was chosen} 
		\\	
	H_{t+1}(b) &=
		H_t(b) - \alpha (R_t - \bar{R}_t)\pi_t(b) 
		\quad \text{for all other actions } b 
\end{aligned}
$$ 

Where: 

- $\alpha$ is the learning rate. 
- $\bar{R}_t$ is the average reward received so far, acting as a baseline to stabilize learning.

This update rule encourages actions that receive above-average rewards while
discouraging less rewarding ones.

### 4. Optimistic Initialization

This is a simple method where the initial estimates $Q_0(a)$ are set to high
values, encouraging the algorithm to explore different actions because all
initial action values seem promising.

#### Comparison with $\epsilon$-greedy

-   Unlike $\epsilon$-greedy, which uses a random chance for exploration, optimistic
    initialization drives exploration until the agent converges on accurate
    value estimates.
-   It is especially useful when the reward distribution is unknown but expected
    to have some higher values.

### Final Remarks:

To determine which algorithm is most effective in practice: 

- A **parameter study** is essential, as highlighted in the passage. This involves varying parameters (like $\epsilon$, $c$, or $\alpha$) to find the optimal range for each algorithm. 
- **Learning curves** provide insight into how each algorithm performs over time.
Averaging these curves over several runs ensures statistical reliability.


```{.python filename="example_2_6_summary.py"}
import numpy as np
from matplotlib import pyplot as plt
from collections import namedtuple
import tqdm
import pickle

from example_2_2_bandits_algo import run_bandit as e_greedy
from example_2_3_OIV import run_bandit as OIV
from example_2_4_UCB import run_bandit_UCB as UCB
from example_2_5_gradient import run_bandit as gradient

SEED = 200
np.random.seed(SEED)


# A wrapper function for running different algorithms
def run_algorithm(
        fn_name: str,
        fn: 'function',
        params: np.ndarray,
        args: dict,
        total_rounds: int
        ) -> np.ndarray:
    global hyper_param
    if fn_name == 'e_greedy':
        hyper_param = 'epsilon'
    elif fn_name == 'ucb':
        hyper_param = 'c'
    elif fn_name == 'gradient':
        hyper_param = 'alpha'
    elif fn_name == 'oiv':
        hyper_param = 'init_val'
    
    args[hyper_param] = None
    
    rewards_hist = np.zeros(
        shape=(len(params), total_rounds, args['num_steps'])
        )
    optm_acts_hist = np.zeros_like(rewards_hist)
    for i, param in tqdm.tqdm(enumerate(params), desc=fn_name, position=0):
        args[hyper_param] = param
        for curr_round in tqdm.tqdm(
                range(total_rounds),
                desc=f'{fn_name}: {param}',
                position=i+1,
                leave=True
                ):
            fn(
                **args,
                rewards=rewards_hist[i, curr_round],
                optim_acts_ratio=optm_acts_hist[i, curr_round]
                )
    print('\n')
    return rewards_hist.mean(axis=1).mean(axis=1)


if __name__ == "__main__":
    K = 10
    num_steps = 1000
    total_rounds = 2000
    q_star = np.random.normal(loc=0, scale=1.0, size=K)
    
    # Creating parameter array: [1/128, 1/64, 1/32, 1/16, ...]
    multiplier = np.exp2(np.arange(10))
    params = np.ones(10) * (1 / 128)
    params *= multiplier
    x_labels = ['1/128', '1/64', '1/32', '1/16', '1/8', '1/4', '1/2', '1', '2',
                '4']
    
    # Creating a dict to record running histories
    records = {
            'params': params,
            'x_labels': x_labels
            }
    history = namedtuple('history', ['bounds', 'data'])
    
    base_args = {
            'K': K,
            'q_star': q_star,
            'num_steps': num_steps
            }
    
    # ======== e_greedy ========
    eps_bounds = [0, 6]
    fn_params = params[eps_bounds[0]: eps_bounds[1]]
    
    eps_rewards = run_algorithm(
        'e_greedy', e_greedy, fn_params, base_args.copy(), total_rounds
        )
    records['e_greedy'] = history(eps_bounds, eps_rewards)
    
    # ======== UCB ========
    ucb_bounds = [3, 10]
    fn_params = params[ucb_bounds[0]: ucb_bounds[1]]
    
    ucb_rewards = run_algorithm(
        'ucb', UCB, fn_params, base_args.copy(), total_rounds
        )
    records['ucb'] = history(ucb_bounds, ucb_rewards)
    
    # ======== Gradient ========
    gd_bounds = [2, 9]
    fn_params = params[gd_bounds[0]:gd_bounds[1]]
    gd_args = base_args.copy()
    gd_args['baseline'] = True
    
    gd_rewards = run_algorithm(
        'gradient', gradient, fn_params, gd_args, total_rounds
        )
    records['gradient'] = history(gd_bounds, gd_rewards)
    
    # ======== OIV ========
    oiv_bounds = [5, 10]
    fn_params = params[oiv_bounds[0]:oiv_bounds[1]]
    oiv_args = base_args.copy()
    oiv_args['epsilon'] = 0.0
    oiv_rewards = run_algorithm('oiv', OIV, fn_params, oiv_args, total_rounds)
    records['oiv'] = history(oiv_bounds, oiv_rewards)
    
    with open('./history/summary.pkl', 'wb') as f:
        pickle.dump(records, f)
```
## Visualization



```{.python filename="plot_sumary.py"}
import pickle
import numpy as np
import matplotlib.pyplot as plt

from collections import namedtuple

history = namedtuple('history', ['bounds', 'data'])
algos = ['e_greedy', 'gradient', 'ucb', 'oiv']

if __name__ == '__main__':
    
    with open('./history/summary.pkl', 'rb') as f:
        histories = pickle.load(f)
        coords = [[0.95, 1.55], [6.5, 1.45], [3, 1.82], [8.5, 1.82]]
        legend_loc = 2
        filename = './plots/example_2_6_summary.png'
    
    # with open('./history/exercise_2_6.pkl', 'rb') as f:
    #     histories = pickle.load(f)
    #     coords = [[2.5, 6.0], [7.0, 3.5], [7.5, 5.0], [6.5, 5.7]]
    #     legend_loc = 0
    #     filename = './plots/exercise_2_6.png'
    
    x_ticks = histories['x_labels']
    
    plt.figure(figsize=(10, 6), dpi=150)
    plt.grid(c='lightgray')
    plt.margins(0.02)
    
    fontdict = {
            'fontsize': 12,
            'fontweight': 'bold',
            }
    
    legends = ['$\epsilon$', '$\\alpha$', '$c$', '$Q_0$']
    colors = ['tomato', 'mediumseagreen', 'steelblue', 'orchid']
    
    for i, key in enumerate(algos):
        record = histories[key]
        bounds = record.bounds
        data = record.data
        
        plt.plot(
            np.arange(bounds[0], bounds[1]), data, label=legends[i], c=colors[i]
            )
    
    for i, spine in enumerate(plt.gca().spines.values()):
        if i in [0, 2]:
            spine.set_linewidth(1.5)
            continue
        spine.set_visible(False)
    
    plt.tick_params(axis='both', labelsize=10)
    plt.xticks(np.arange(10), x_ticks)
    
    # x labels
    plt.legend(loc=legend_loc, fontsize=12, title='Hyper Param.')
    plt.xlabel('Hyper parameter value', fontdict=fontdict)
    plt.ylabel(
        'Average reward over first 1000 steps',
        fontdict=fontdict
        )
    
    plt.text(*coords[0], '$\epsilon$-greedy', c=colors[0], fontsize=12)
    plt.text(
        *coords[1], 'gradient\nbandit', c=colors[1], fontsize=12,
        horizontalalignment='center'
        )
    plt.text(*coords[2], 'UCB', c=colors[2], fontsize=12)
    plt.text(
        *coords[3], 'greedy with\noptimistic\ninitializatio\n$\\alpha=0.1$',
        c=colors[3], fontsize=12, horizontalalignment='center'
        )
    
    # plt.show()
    plt.savefig(filename)

```

