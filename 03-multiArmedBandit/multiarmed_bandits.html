<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Saúl Díaz Infante Velasco">

<title>From Markov Decision Processes to Reinforcement Learning with Python - Multi-armed Bandits</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../04-finiteMDPs/mdp.html" rel="next">
<link href="../02-introductionToRL/intro.html" rel="prev">
<link href="../cover_RL.jpeg" rel="icon" type="image/jpeg">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "classic",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">From Markov Decision Processes to Reinforcement Learning with Python</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/SaulDiazInfante/RL-Course-2024-2.git" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../From-Markov-Decision-Processes-to-Reinforcement-Learning-with-Python.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../03-multiArmedBandit/multiarmed_bandits.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Multi-armed Bandits</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="sidebar-tools-main">
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-save"></i></a>
</div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-introduction/general_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Abstract</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-introductionToRL/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-multiArmedBandit/multiarmed_bandits.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Multi-armed Bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../04-finiteMDPs/mdp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Finite Markov Decision Processes (MDPs)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-dynamicProgramming/dp_rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Dynamic Programming</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../06-applications/applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../07-Project/project_proposal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Project proposal</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-Evaluation/rubric_evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Evaluation Rubric</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../homeworks/home_works_list.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">List of Home Works and due dates</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#multi-armed-bandits" id="toc-multi-armed-bandits" class="nav-link active" data-scroll-target="#multi-armed-bandits">Multi-armed Bandits</a>
  <ul class="collapse">
  <li><a href="#a-k-armed-bandit-problem" id="toc-a-k-armed-bandit-problem" class="nav-link" data-scroll-target="#a-k-armed-bandit-problem">A <span class="math inline">\(k\)</span>-armed Bandit Problem</a></li>
  <li><a href="#action-value-methods" id="toc-action-value-methods" class="nav-link" data-scroll-target="#action-value-methods">Action-value Methods</a></li>
  <li><a href="#the-10-armed-testbed" id="toc-the-10-armed-testbed" class="nav-link" data-scroll-target="#the-10-armed-testbed">The 10-armed Testbed</a>
  <ul class="collapse">
  <li><a href="#set-up" id="toc-set-up" class="nav-link" data-scroll-target="#set-up">Set up</a></li>
  </ul></li>
  <li><a href="#incremental-implementation" id="toc-incremental-implementation" class="nav-link" data-scroll-target="#incremental-implementation">Incremental Implementation</a>
  <ul class="collapse">
  <li><a href="#incremental-update-formula-for-action-value-estimation" id="toc-incremental-update-formula-for-action-value-estimation" class="nav-link" data-scroll-target="#incremental-update-formula-for-action-value-estimation">Incremental Update Formula for Action-Value Estimation</a></li>
  <li><a href="#derivation-of-the-incremental-formula" id="toc-derivation-of-the-incremental-formula" class="nav-link" data-scroll-target="#derivation-of-the-incremental-formula">Derivation of the Incremental Formula</a></li>
  <li><a href="#advantages-of-the-incremental-method" id="toc-advantages-of-the-incremental-method" class="nav-link" data-scroll-target="#advantages-of-the-incremental-method">Advantages of the Incremental Method</a></li>
  </ul></li>
  <li><a href="#tracking-a-nonstationary-problem" id="toc-tracking-a-nonstationary-problem" class="nav-link" data-scroll-target="#tracking-a-nonstationary-problem">Tracking a Nonstationary Problem</a></li>
  <li><a href="#optimistic-initial-values" id="toc-optimistic-initial-values" class="nav-link" data-scroll-target="#optimistic-initial-values">Optimistic Initial Values</a></li>
  <li><a href="#upper-confidence-bound-action-selection" id="toc-upper-confidence-bound-action-selection" class="nav-link" data-scroll-target="#upper-confidence-bound-action-selection">Upper-Confidence-Bound Action Selection</a></li>
  <li><a href="#gradient-bandit-method" id="toc-gradient-bandit-method" class="nav-link" data-scroll-target="#gradient-bandit-method">Gradient Bandit method</a></li>
  <li><a href="#scripts-for-visualization" id="toc-scripts-for-visualization" class="nav-link" data-scroll-target="#scripts-for-visualization">Scripts for visualization</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a>
  <ul class="collapse">
  <li><a href="#epsilon-greedy-method" id="toc-epsilon-greedy-method" class="nav-link" data-scroll-target="#epsilon-greedy-method">1. <span class="math inline">\(\epsilon\)</span>-greedy Method</a></li>
  <li><a href="#ucb-upper-confidence-bound-method" id="toc-ucb-upper-confidence-bound-method" class="nav-link" data-scroll-target="#ucb-upper-confidence-bound-method">2. UCB (Upper Confidence Bound) Method</a></li>
  <li><a href="#gradient-bandit-algorithm" id="toc-gradient-bandit-algorithm" class="nav-link" data-scroll-target="#gradient-bandit-algorithm">3. Gradient Bandit Algorithm</a></li>
  <li><a href="#optimistic-initialization" id="toc-optimistic-initialization" class="nav-link" data-scroll-target="#optimistic-initialization">4. Optimistic Initialization</a></li>
  <li><a href="#final-remarks" id="toc-final-remarks" class="nav-link" data-scroll-target="#final-remarks">Final Remarks:</a></li>
  </ul></li>
  <li><a href="#visualization" id="toc-visualization" class="nav-link" data-scroll-target="#visualization">Visualization</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/SaulDiazInfante/RL-Course-2024-2.git/edit/main/03-multiArmedBandit/multiarmed_bandits.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/SaulDiazInfante/RL-Course-2024-2.git/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Multi-armed Bandits</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Saúl Díaz Infante Velasco </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="multi-armed-bandits" class="level1 page-columns page-full">
<h1>Multi-armed Bandits</h1>
<p>A very important feature distinguishing reinforcement learning from other types of learning is that it uses training information to evaluate the actions taken, rather than instruct by giving correct actions.</p>
<section id="a-k-armed-bandit-problem" class="level2">
<h2 class="anchored" data-anchor-id="a-k-armed-bandit-problem">A <span class="math inline">\(k\)</span>-armed Bandit Problem</h2>
<p>We consider the following setup:</p>
<ul>
<li>You repeatedly face a choice among <span class="math inline">\(k\)</span> different options or actions.</li>
<li>After a choice, you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected</li>
<li>Your goal is to maximize the total expected reward over a specific time period, such as 1000 action selections or time steps. The problem is named by analogy to a slot machine, or <code>one-armed bandit</code>, except that it has <span class="math inline">\(k\)</span> levers instead of one.</li>
</ul>
<p>We denote the action selected on time step <span class="math inline">\(t\)</span> as <span class="math inline">\(A_t\)</span> and the corresponding reward as <span class="math inline">\(R_t\)</span>. Each of the <span class="math inline">\(k\)</span> actions has an expected or mean reward given that that action is selected; let us call this the value of that action.</p>
<p>The value then of an arbitrary action <span class="math inline">\(a\)</span>, denoted <span class="math inline">\(q_{*} (a)\)</span>, is the expected reward given that <span class="math inline">\(a\)</span> is selected:</p>
<p><span class="math display">\[
    q_{*}(a): = \mathbb{E} \left[ R_t | A_t =a\right].
\]</span></p>
<p>If you knew the value of each action, then we solve the <span class="math inline">\(k\)</span>-armed bandit problem—you would always <code>select the action with highest value</code>.</p>
<p>We assume that you may not have precise knowledge of the action values, although you may have some estimates. We denote this estimated value of action <span class="math inline">\(a\)</span> at time step <span class="math inline">\(t\)</span> as <span class="math inline">\(Q_t(a)\)</span>. Thus, we would like that <span class="math display">\[
    Q_t(a) \approx q_{*}(a).
\]</span></p>
<p>If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the <em>greedy</em> actions. When you select one of these actions, we say that you are <em>exploiting</em> your current knowledge of the values of the actions. If instead you select one of the non-greedy actions, then we say you are exploring, because this enables you to improve your estimate of the non-greedy action’s value.</p>
<p>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</p>
<p>Reward is lower in the short run, during exploration, but higher in the long run because after you have discovered the better actions, you can exploit them many times. Because it is not possible both to explore and to exploit with any single action selection, one often refers to the “conflict” between exploration and exploitation.</p>
<p>In any specific case, whether it is better to explore or exploit depends in a complex way on the precise values of the estimates, uncertainties, and the number of remaining steps. There are many sophisticated methods for balancing exploration and exploitation for particular mathematical formulations of the <span class="math inline">\(k\)</span>-armed bandit and related problems.</p>
<p>However, most of these methods make strong assumptions about stationary and prior knowledge that are either violated or impossible to verify in most applications.</p>
<p>The guarantees of optimality or bounded loss for these methods offer little comfort when the assumptions of their theory do not apply.</p>
</section>
<section id="action-value-methods" class="level2">
<h2 class="anchored" data-anchor-id="action-value-methods">Action-value Methods</h2>
<p>One natural way to estimate the value of a given action is by averaging the rewards actually received. In mathematical symbols reads</p>
<p><span id="eq-action_avering_kbandit"><span class="math display">\[
  Q_t(a):=
    \dfrac{
      \sum_{i=1}^{t-1}
        R_i \cdot \mathbb{1}_{A_{i} = a}
    }{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}} .
\tag{1.1}\]</span></span></p>
<p>Next we understand as greedy action as the action that results from <span id="eq-greedy_action"><span class="math display">\[
  A_t := \underset{a}{\mathrm{argmax}} \ Q_t(a).
\tag{1.2}\]</span></span></p>
<p>Greedy action selection always exploits current knowledge to maximize immediate reward. It also only spends time sampling apparently superior actions. A simple alternative is to behave greedily but occasionally, with a small <span class="math inline">\(\epsilon\)</span>-probability, select randomly from all the actions with equal probability, regardless of the action-value estimates. We call methods using this near-greedy action selection rule <span class="math inline">\(\epsilon\)</span>-greedy methods.</p>
</section>
<section id="the-10-armed-testbed" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-10-armed-testbed">The 10-armed Testbed</h2>
<p>To evaluate the relative effectiveness of the greedy and <span class="math inline">\(\epsilon\)</span>-greedy action-value methods, we compared them numerically on a suite of test problems.</p>
<section id="set-up" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="set-up">Set up</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The experiment runs as follows.
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Consider a <span class="math inline">\(k\)</span>-bandit problem with <span class="math inline">\(k=10\)</span></p></li>
<li><p>For each bandit problem, the action values</p></li>
</ul>
<p><span class="math display">\[
  q_{*}(a) \sim \mathcal{N}(0,1)
\]</span></p>
<ul>
<li>Then when choosing an action <span class="math inline">\(A_t\)</span> the corresponding reward <span class="math inline">\(R_t\)</span> is sampling from a Gaussian distribution <span class="math display">\[
R_t \sim \mathcal{N}(q_{*}(A_t), 1)  
\]</span></li>
</ul>
</div>
</div>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>k_armed_testbed.py</strong></pre>
</div>
<div class="sourceCode" id="cb1" data-filename="k_armed_testbed.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co">#k_armed_testbed.py</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4"></a></span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co"># Randomly sample mean reward for each action</span></span>
<span id="cb1-7"><a href="#cb1-7"></a>means <span class="op">=</span> np.random.normal(size<span class="op">=</span>(<span class="dv">10</span>, ))</span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="co"># Generate sample data based on normal distribution</span></span>
<span id="cb1-10"><a href="#cb1-10"></a>data <span class="op">=</span> [np.random.normal(mean, <span class="fl">1.0</span>, <span class="dv">2000</span>) <span class="cf">for</span> mean <span class="kw">in</span> means]</span>
<span id="cb1-11"><a href="#cb1-11"></a></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co"># Create violin plot</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>), dpi<span class="op">=</span><span class="dv">150</span>)</span>
<span id="cb1-14"><a href="#cb1-14"></a>plt.violinplot(</span>
<span id="cb1-15"><a href="#cb1-15"></a>  dataset<span class="op">=</span>data,</span>
<span id="cb1-16"><a href="#cb1-16"></a>  showextrema<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-17"><a href="#cb1-17"></a>  showmeans<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-18"><a href="#cb1-18"></a>  points<span class="op">=</span><span class="dv">2000</span></span>
<span id="cb1-19"><a href="#cb1-19"></a>)</span>
<span id="cb1-20"><a href="#cb1-20"></a></span>
<span id="cb1-21"><a href="#cb1-21"></a><span class="co"># Draw mean marks</span></span>
<span id="cb1-22"><a href="#cb1-22"></a><span class="cf">for</span> i, mean <span class="kw">in</span> <span class="bu">enumerate</span>(means):</span>
<span id="cb1-23"><a href="#cb1-23"></a>    idx <span class="op">=</span> i <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb1-24"><a href="#cb1-24"></a>    plt.plot([idx <span class="op">-</span> <span class="fl">0.3</span>, idx <span class="op">+</span> <span class="fl">0.3</span>], [mean, mean],</span>
<span id="cb1-25"><a href="#cb1-25"></a>             c<span class="op">=</span><span class="st">'black'</span>,</span>
<span id="cb1-26"><a href="#cb1-26"></a>             linewidth<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-27"><a href="#cb1-27"></a>    plt.text(idx <span class="op">+</span> <span class="fl">0.2</span>, mean <span class="op">-</span> <span class="fl">0.2</span>, </span>
<span id="cb1-28"><a href="#cb1-28"></a>             s<span class="op">=</span><span class="ss">f"$q_*(</span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">)$"</span>,</span>
<span id="cb1-29"><a href="#cb1-29"></a>             fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb1-30"><a href="#cb1-30"></a></span>
<span id="cb1-31"><a href="#cb1-31"></a><span class="co"># Draw 0-value dashed line</span></span>
<span id="cb1-32"><a href="#cb1-32"></a>plt.plot(np.arange(<span class="dv">0</span>, <span class="dv">12</span>), np.zeros(<span class="dv">12</span>), </span>
<span id="cb1-33"><a href="#cb1-33"></a>            c<span class="op">=</span><span class="st">'gray'</span>, </span>
<span id="cb1-34"><a href="#cb1-34"></a>            linewidth<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb1-35"><a href="#cb1-35"></a>            linestyle<span class="op">=</span>(<span class="dv">5</span>, (<span class="dv">20</span>, <span class="dv">10</span>)))</span>
<span id="cb1-36"><a href="#cb1-36"></a>plt.tick_params(axis<span class="op">=</span><span class="st">'both'</span>, labelsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb1-37"><a href="#cb1-37"></a>plt.xticks(np.arange(<span class="dv">1</span>, <span class="dv">11</span>))</span>
<span id="cb1-38"><a href="#cb1-38"></a></span>
<span id="cb1-39"><a href="#cb1-39"></a><span class="co"># get rid of the frame</span></span>
<span id="cb1-40"><a href="#cb1-40"></a><span class="cf">for</span> i, spine <span class="kw">in</span> <span class="bu">enumerate</span>(plt.gca().spines.values()):</span>
<span id="cb1-41"><a href="#cb1-41"></a>    <span class="cf">if</span> i <span class="op">==</span> <span class="dv">2</span>: <span class="cf">continue</span></span>
<span id="cb1-42"><a href="#cb1-42"></a>    spine.set_visible(<span class="va">False</span>)</span>
<span id="cb1-43"><a href="#cb1-43"></a>    </span>
<span id="cb1-44"><a href="#cb1-44"></a></span>
<span id="cb1-45"><a href="#cb1-45"></a><span class="co"># Draw labels</span></span>
<span id="cb1-46"><a href="#cb1-46"></a>label_font <span class="op">=</span> {</span>
<span id="cb1-47"><a href="#cb1-47"></a>    <span class="st">'fontsize'</span>: <span class="dv">12</span>,</span>
<span id="cb1-48"><a href="#cb1-48"></a>    <span class="st">'fontweight'</span>: <span class="st">'bold'</span></span>
<span id="cb1-49"><a href="#cb1-49"></a>}</span>
<span id="cb1-50"><a href="#cb1-50"></a></span>
<span id="cb1-51"><a href="#cb1-51"></a>plt.xlabel(<span class="st">'Action'</span>, fontdict<span class="op">=</span>label_font)</span>
<span id="cb1-52"><a href="#cb1-52"></a>plt.ylabel(<span class="st">'Reward distribution'</span>, fontdict<span class="op">=</span>label_font)</span>
<span id="cb1-53"><a href="#cb1-53"></a>plt.margins(<span class="dv">0</span>)</span>
<span id="cb1-54"><a href="#cb1-54"></a></span>
<span id="cb1-55"><a href="#cb1-55"></a>plt.tight_layout()</span>
<span id="cb1-56"><a href="#cb1-56"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We consider a set of 2000 randomly generated <span class="math inline">\(k\)</span>-armed bandit problems with <span class="math inline">\(k\)</span> = 10. For each bandit problem, such as the one shown in the output of the above code. The action values, <span class="math inline">\(q_{*} (a), a = 1, . . . , 10\)</span>, were selected according to a normal (Gaussian) distribution with mean 0 and variance 1. Thus when we apply a learning method to this problem, the selected action <span class="math inline">\(A_t\)</span> a time step <span class="math inline">\(t\)</span> the regarding reward <span class="math inline">\(R_t\)</span> is sampling from a normal distribution <span class="math display">\[
  R_{t} \sim \mathcal{N}(q_{*}(A_t), 1).
\]</span> Sutton and Barto <span class="citation" data-cites="Sutton2018">(<a href="../references.html#ref-Sutton2018" role="doc-biblioref">Sutton and Barto 2018, 28</a>)</span> calls this suite of test tasks the 10-armed test-bed. By using this suit of benchmarks, we can measure the performance of any learning method. In fact we also can observe its behavior while the learning improves with experience of 1000 time steps, when it is applied to a selected bandit of this bed. This makes up one run. Thus, if we <strong>iterate 2000</strong> independent runs, each with different bandit problem, we can obtain a measure of learning algorithm’s average behavior.</p>
<div class="no-row-height column-margin column-container"><div id="ref-Sutton2018" class="csl-entry" role="listitem">
Sutton, Richard S., and Andrew G. Barto. 2018. <em>Reinforcement Learning: An Introduction</em>. Second. Adaptive Computation and Machine Learning. MIT Press, Cambridge, MA.
</div></div><p>Next we code functions to deploy the above experiment with <span class="math inline">\(\epsilon\)</span>-greedy actions</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>utils.py</strong></pre>
</div>
<div class="sourceCode" id="cb2" data-filename="utils.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">from</span> typing <span class="im">import</span> Any</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="im">from</span> numpy <span class="im">import</span> dtype, ndarray, signedinteger</span>
<span id="cb2-5"><a href="#cb2-5"></a></span>
<span id="cb2-6"><a href="#cb2-6"></a></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="co"># Get the action with the max Q value</span></span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="kw">def</span> get_argmax(G:np.ndarray) <span class="op">-&gt;</span> ndarray[Any, dtype[signedinteger[Any]]]:</span>
<span id="cb2-9"><a href="#cb2-9"></a>    candidates <span class="op">=</span> np.argwhere(G <span class="op">==</span> G.<span class="bu">max</span>()).flatten()</span>
<span id="cb2-10"><a href="#cb2-10"></a>    <span class="co"># return the only index if there's only one max</span></span>
<span id="cb2-11"><a href="#cb2-11"></a>    <span class="cf">if</span> <span class="bu">len</span>(candidates) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb2-12"><a href="#cb2-12"></a>        <span class="cf">return</span> candidates[<span class="dv">0</span>]</span>
<span id="cb2-13"><a href="#cb2-13"></a>    <span class="cf">else</span>:</span>
<span id="cb2-14"><a href="#cb2-14"></a>        <span class="co"># instead break the tie randomly</span></span>
<span id="cb2-15"><a href="#cb2-15"></a>        <span class="cf">return</span> np.random.choice(candidates)</span>
<span id="cb2-16"><a href="#cb2-16"></a></span>
<span id="cb2-17"><a href="#cb2-17"></a></span>
<span id="cb2-18"><a href="#cb2-18"></a><span class="co"># Select arm and get the reward</span></span>
<span id="cb2-19"><a href="#cb2-19"></a><span class="kw">def</span> bandit(q_star:np.ndarray, </span>
<span id="cb2-20"><a href="#cb2-20"></a>           act:<span class="bu">int</span>) <span class="op">-&gt;</span> <span class="bu">tuple</span>:</span>
<span id="cb2-21"><a href="#cb2-21"></a>    real_rewards <span class="op">=</span> np.random.normal(q_star, <span class="fl">1.0</span>)</span>
<span id="cb2-22"><a href="#cb2-22"></a>    <span class="co"># optim_choice = int(real_rewards[act] == real_rewards.max())</span></span>
<span id="cb2-23"><a href="#cb2-23"></a>    optim_choice <span class="op">=</span> <span class="bu">int</span>(q_star[act] <span class="op">==</span> q_star.<span class="bu">max</span>())</span>
<span id="cb2-24"><a href="#cb2-24"></a>    <span class="cf">return</span> real_rewards[act], optim_choice</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Please save the above script as <code>utils.py</code> in the firs level of the regrding project such that we can imported by the ist name fora example by <code>from utils import bandit, plots</code></p>
</section>
</section>
<section id="incremental-implementation" class="level2">
<h2 class="anchored" data-anchor-id="incremental-implementation">Incremental Implementation</h2>
<p>Certainly! To express a more efficient method for estimating action values, we focus on using an <strong>incremental update formula</strong> rather than recalculating the average based on all past observations. The goal is to maintain a constant memory footprint and fixed computation per time step.</p>
<section id="incremental-update-formula-for-action-value-estimation" class="level3">
<h3 class="anchored" data-anchor-id="incremental-update-formula-for-action-value-estimation">Incremental Update Formula for Action-Value Estimation</h3>
<p>Let <span class="math inline">\(R_i\)</span> denote the reward received after the <span class="math inline">\(i\)</span>-th selection of the action.<span class="math inline">\(Q_n\)</span> denote the estimate of the action value after the action has been chosen <span class="math inline">\(n-1\)</span> times.</p>
<p>Instead of computing <span class="math inline">\(Q_n\)</span> as the sample average of all observed rewards (which requires storing and summing all rewards), we use the <strong>incremental formula</strong>:</p>
<p><span class="math display">\[
    Q_n = Q_{n-1} + \alpha \left(R_n - Q_{n-1}\right)
\]</span></p>
<p>Where: <span class="math inline">\(Q_{n-1}\)</span> is the previous estimate of the action value. <span class="math inline">\(R_n\)</span> is the reward received on the <span class="math inline">\(n\)</span>-th selection. <span class="math inline">\(\alpha\)</span> is a <strong>constant step size</strong>, often set as <span class="math inline">\(\dfrac{1}{n}\)</span> to mimic the behavior of sample averaging when the number of observations grows.</p>
</section>
<section id="derivation-of-the-incremental-formula" class="level3">
<h3 class="anchored" data-anchor-id="derivation-of-the-incremental-formula">Derivation of the Incremental Formula</h3>
<ol type="1">
<li><p>Start with the definition of the action value as the sample mean: <span class="math inline">\(\displaystyle Q_n =\dfrac{1}{n} \sum_{i=1}^{n} R_i\)</span></p></li>
<li><p>Express <span class="math inline">\(Q_n\)</span> in terms of <span class="math inline">\(Q_{n-1}\)</span>: <span class="math display">\[
Q_n = \frac{1}{n}
\left[ \sum_{i=1}^{n-1} R_i + R_n \right]
\]</span></p></li>
</ol>
<p>This can be rearranged as: <span class="math display">\[
        Q_n = \frac{n-1}{n} \cdot Q_{n-1} +
    \frac{1}{n} \cdot R_n.
\]</span></p>
<ol start="3" type="1">
<li>Notice that <span class="math inline">\(\displaystyle
    \frac{n-1}{n} \cdot Q_{n-1} = Q_{n-1} - \frac{1}{n}\cdot Q_{n-1}\)</span>, so: <span class="math display">\[
    Q_n = Q_{n-1} + \frac{1}{n} \left(R_n -Q_{n-1}\right)
\]</span></li>
</ol>
<p>Here, <span class="math inline">\(\alpha = \dfrac{1}{n}\)</span> adapts to the number of observations, ensuring the update balances the influence of new and past rewards.</p>
</section>
<section id="advantages-of-the-incremental-method" class="level3">
<h3 class="anchored" data-anchor-id="advantages-of-the-incremental-method">Advantages of the Incremental Method</h3>
<ul>
<li><strong>Constant Memory</strong>: The method only requires storing <span class="math inline">\(Q_{n-1}\)</span> and <span class="math inline">\(R_n\)</span>, avoiding the need to keep all past rewards.</li>
<li><strong>Fixed Computation</strong>: Each update involves a fixed, small number of operations, regardless of <span class="math inline">\(n\)</span>.</li>
</ul>
<p>This approach efficiently updates the action-value estimate with minimal resources, making it suitable for online learning algorithms and scenarios where computational efficiency is critical.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../assets/ch_03/mab_simple_algorithm.png" class="img-fluid figure-img"></p>
<figcaption>Incremental algorithm</figcaption>
</figure>
</div>
<p>Bellow a python implementation.</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>example_2_2_bandits_algo.py</strong></pre>
</div>
<div class="sourceCode" id="cb3" data-filename="example_2_2_bandits_algo.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="im">import</span> matplotlib</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-4"><a href="#cb3-4"></a>matplotlib.use(<span class="st">'qt5agg'</span>)</span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="im">import</span> pickle</span>
<span id="cb3-6"><a href="#cb3-6"></a></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="im">from</span> utils <span class="im">import</span> get_argmax, bandit</span>
<span id="cb3-8"><a href="#cb3-8"></a></span>
<span id="cb3-9"><a href="#cb3-9"></a><span class="co">#SEED = 123456</span></span>
<span id="cb3-10"><a href="#cb3-10"></a><span class="co">#np.random.seed(SEED)</span></span>
<span id="cb3-11"><a href="#cb3-11"></a></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="co"># running the k-armed bandit algorithm</span></span>
<span id="cb3-13"><a href="#cb3-13"></a><span class="kw">def</span> run_bandit(K:<span class="bu">int</span>, </span>
<span id="cb3-14"><a href="#cb3-14"></a>            q_star:np.ndarray,</span>
<span id="cb3-15"><a href="#cb3-15"></a>            rewards:np.ndarray,</span>
<span id="cb3-16"><a href="#cb3-16"></a>            optim_acts_ratio:np.ndarray,</span>
<span id="cb3-17"><a href="#cb3-17"></a>            epsilon:<span class="bu">float</span>, </span>
<span id="cb3-18"><a href="#cb3-18"></a>            num_steps:<span class="bu">int</span><span class="op">=</span><span class="dv">1000</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb3-19"><a href="#cb3-19"></a>    </span>
<span id="cb3-20"><a href="#cb3-20"></a>    Q <span class="op">=</span> np.zeros(K)     <span class="co"># Initialize Q values</span></span>
<span id="cb3-21"><a href="#cb3-21"></a>    N <span class="op">=</span> np.zeros(K)     <span class="co"># The number of times each action been selected</span></span>
<span id="cb3-22"><a href="#cb3-22"></a>    ttl_optim_acts <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-23"><a href="#cb3-23"></a></span>
<span id="cb3-24"><a href="#cb3-24"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb3-25"><a href="#cb3-25"></a>        <span class="co"># get action</span></span>
<span id="cb3-26"><a href="#cb3-26"></a>        A <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-27"><a href="#cb3-27"></a>        <span class="cf">if</span> np.random.random() <span class="op">&gt;</span> epsilon:</span>
<span id="cb3-28"><a href="#cb3-28"></a>            A <span class="op">=</span> get_argmax(Q)</span>
<span id="cb3-29"><a href="#cb3-29"></a>        <span class="cf">else</span>:</span>
<span id="cb3-30"><a href="#cb3-30"></a>            A <span class="op">=</span> np.random.randint(<span class="dv">0</span>, K)</span>
<span id="cb3-31"><a href="#cb3-31"></a>        </span>
<span id="cb3-32"><a href="#cb3-32"></a>        R, is_optim <span class="op">=</span> bandit(q_star, A)</span>
<span id="cb3-33"><a href="#cb3-33"></a>        N[A] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb3-34"><a href="#cb3-34"></a>        Q[A] <span class="op">+=</span> (R <span class="op">-</span> Q[A]) <span class="op">/</span> N[A]</span>
<span id="cb3-35"><a href="#cb3-35"></a></span>
<span id="cb3-36"><a href="#cb3-36"></a>        ttl_optim_acts <span class="op">+=</span> is_optim</span>
<span id="cb3-37"><a href="#cb3-37"></a>        rewards[i] <span class="op">=</span> R</span>
<span id="cb3-38"><a href="#cb3-38"></a>        optim_acts_ratio[i] <span class="op">=</span> ttl_optim_acts <span class="op">/</span> (i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb3-39"><a href="#cb3-39"></a></span>
<span id="cb3-40"><a href="#cb3-40"></a></span>
<span id="cb3-41"><a href="#cb3-41"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb3-42"><a href="#cb3-42"></a></span>
<span id="cb3-43"><a href="#cb3-43"></a>    <span class="co"># Initializing the hyperparameters</span></span>
<span id="cb3-44"><a href="#cb3-44"></a>    K <span class="op">=</span> <span class="dv">10</span>  <span class="co"># Number of arms</span></span>
<span id="cb3-45"><a href="#cb3-45"></a>    epsilons <span class="op">=</span> [<span class="fl">0.0</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>]</span>
<span id="cb3-46"><a href="#cb3-46"></a>    num_steps <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb3-47"><a href="#cb3-47"></a>    total_rounds <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb3-48"><a href="#cb3-48"></a></span>
<span id="cb3-49"><a href="#cb3-49"></a>    <span class="co"># Initialize the environment</span></span>
<span id="cb3-50"><a href="#cb3-50"></a>    q_star <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="fl">1.0</span>, size<span class="op">=</span>K)</span>
<span id="cb3-51"><a href="#cb3-51"></a>    rewards <span class="op">=</span> np.zeros(shape<span class="op">=</span>(<span class="bu">len</span>(epsilons), total_rounds, num_steps))</span>
<span id="cb3-52"><a href="#cb3-52"></a>    optim_acts_ratio <span class="op">=</span> np.zeros(shape<span class="op">=</span>(<span class="bu">len</span>(epsilons), total_rounds, num_steps))</span>
<span id="cb3-53"><a href="#cb3-53"></a>    </span>
<span id="cb3-54"><a href="#cb3-54"></a>    <span class="co"># Run the k-armed bandits alg.</span></span>
<span id="cb3-55"><a href="#cb3-55"></a>    <span class="cf">for</span> i, epsilon <span class="kw">in</span> <span class="bu">enumerate</span>(epsilons):</span>
<span id="cb3-56"><a href="#cb3-56"></a>        <span class="cf">for</span> curr_round <span class="kw">in</span> <span class="bu">range</span>(total_rounds):</span>
<span id="cb3-57"><a href="#cb3-57"></a>            run_bandit(K, q_star, </span>
<span id="cb3-58"><a href="#cb3-58"></a>                       rewards[i, curr_round], </span>
<span id="cb3-59"><a href="#cb3-59"></a>                       optim_acts_ratio[i, curr_round], </span>
<span id="cb3-60"><a href="#cb3-60"></a>                       epsilon, </span>
<span id="cb3-61"><a href="#cb3-61"></a>                       num_steps)</span>
<span id="cb3-62"><a href="#cb3-62"></a>    </span>
<span id="cb3-63"><a href="#cb3-63"></a>    rewards <span class="op">=</span> rewards.mean(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-64"><a href="#cb3-64"></a>    optim_acts_ratio <span class="op">=</span> optim_acts_ratio.mean(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-65"><a href="#cb3-65"></a></span>
<span id="cb3-66"><a href="#cb3-66"></a>    record <span class="op">=</span> {</span>
<span id="cb3-67"><a href="#cb3-67"></a>        <span class="st">'hyper_params'</span>: epsilons, </span>
<span id="cb3-68"><a href="#cb3-68"></a>        <span class="st">'rewards'</span>: rewards,</span>
<span id="cb3-69"><a href="#cb3-69"></a>        <span class="st">'optim_acts_ratio'</span>: optim_acts_ratio</span>
<span id="cb3-70"><a href="#cb3-70"></a>    }</span>
<span id="cb3-71"><a href="#cb3-71"></a></span>
<span id="cb3-72"><a href="#cb3-72"></a>    fig_01, ax_01 <span class="op">=</span> plt.subplots()</span>
<span id="cb3-73"><a href="#cb3-73"></a>    fig_02, ax_02 <span class="op">=</span> plt.subplots()</span>
<span id="cb3-74"><a href="#cb3-74"></a>    <span class="cf">for</span> i, ratio <span class="kw">in</span> <span class="bu">enumerate</span>(optim_acts_ratio):</span>
<span id="cb3-75"><a href="#cb3-75"></a>        ax_01.plot(</span>
<span id="cb3-76"><a href="#cb3-76"></a>                ratio,</span>
<span id="cb3-77"><a href="#cb3-77"></a>                label<span class="op">=</span><span class="vs">r'$\epsilon=</span><span class="sc">{epsilon_i}</span><span class="vs">$'</span>.<span class="bu">format</span>(epsilon_i<span class="op">=</span>epsilons[i])</span>
<span id="cb3-78"><a href="#cb3-78"></a>        )</span>
<span id="cb3-79"><a href="#cb3-79"></a>    </span>
<span id="cb3-80"><a href="#cb3-80"></a>    <span class="cf">for</span> i, reward <span class="kw">in</span> <span class="bu">enumerate</span>(rewards):</span>
<span id="cb3-81"><a href="#cb3-81"></a>        ax_02.plot(</span>
<span id="cb3-82"><a href="#cb3-82"></a>                reward,</span>
<span id="cb3-83"><a href="#cb3-83"></a>                label<span class="op">=</span><span class="vs">r'$\epsilon=</span><span class="sc">{epsilon_i}</span><span class="vs">$'</span>.<span class="bu">format</span>(epsilon_i<span class="op">=</span>epsilons[i])</span>
<span id="cb3-84"><a href="#cb3-84"></a>                )</span>
<span id="cb3-85"><a href="#cb3-85"></a>    ax_01.set_xlabel(<span class="vs">r'$t$'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-86"><a href="#cb3-86"></a>    ax_01.set_ylabel(<span class="vs">r'Optimal Action'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-87"><a href="#cb3-87"></a>    ax_01.legend(loc<span class="op">=</span><span class="st">'best'</span>)</span>
<span id="cb3-88"><a href="#cb3-88"></a>    ax_02.set_xlabel(<span class="vs">r'$t$'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-89"><a href="#cb3-89"></a>    ax_02.set_ylabel(<span class="vs">r'Reward'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb3-90"><a href="#cb3-90"></a>    ax_02.legend(loc<span class="op">=</span><span class="st">'best'</span>)</span>
<span id="cb3-91"><a href="#cb3-91"></a>    plt.show()</span>
<span id="cb3-92"><a href="#cb3-92"></a>    </span>
<span id="cb3-93"><a href="#cb3-93"></a>    <span class="co"># with open('./history/record.pkl', 'wb') as f:</span></span>
<span id="cb3-94"><a href="#cb3-94"></a>    <span class="co">#     pickle.dump(record, f)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="tracking-a-nonstationary-problem" class="level2">
<h2 class="anchored" data-anchor-id="tracking-a-nonstationary-problem">Tracking a Nonstationary Problem</h2>
<p>The averaging methods we have discussed are suitable for stationary bandit problems, where the reward probabilities remain constant over time. However, in reinforcement learning, we often encounter non-stationary problems where it makes more sense to give greater weight to recent rewards than to rewards from a long time ago. One popular approach to achieve this is by using a constant step-size parameter.</p>
<p>#TODO: Formulation with constant alpha and implications</p>
</section>
<section id="optimistic-initial-values" class="level2">
<h2 class="anchored" data-anchor-id="optimistic-initial-values">Optimistic Initial Values</h2>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>example_2_3_OIV.py</strong></pre>
</div>
<div class="sourceCode" id="cb4" data-filename="example_2_3_OIV.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3"></a><span class="im">import</span> pickle</span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a><span class="im">from</span> utils <span class="im">import</span> get_argmax, bandit</span>
<span id="cb4-6"><a href="#cb4-6"></a></span>
<span id="cb4-7"><a href="#cb4-7"></a>SEED <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb4-8"><a href="#cb4-8"></a>np.random.seed(SEED)</span>
<span id="cb4-9"><a href="#cb4-9"></a></span>
<span id="cb4-10"><a href="#cb4-10"></a></span>
<span id="cb4-11"><a href="#cb4-11"></a><span class="co"># running the k-armed bandit algorithm</span></span>
<span id="cb4-12"><a href="#cb4-12"></a><span class="kw">def</span> run_bandit(K: <span class="bu">int</span>,</span>
<span id="cb4-13"><a href="#cb4-13"></a>            q_star: np.ndarray,</span>
<span id="cb4-14"><a href="#cb4-14"></a>            rewards: np.ndarray,</span>
<span id="cb4-15"><a href="#cb4-15"></a>            optim_acts_ratio: np.ndarray,</span>
<span id="cb4-16"><a href="#cb4-16"></a>            epsilon: <span class="bu">float</span>,</span>
<span id="cb4-17"><a href="#cb4-17"></a>            num_steps: <span class="bu">int</span><span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb4-18"><a href="#cb4-18"></a>            init_val: <span class="bu">int</span><span class="op">=</span><span class="dv">0</span></span>
<span id="cb4-19"><a href="#cb4-19"></a>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb4-20"><a href="#cb4-20"></a>    Q <span class="op">=</span> np.ones(K) <span class="op">*</span> init_val   <span class="co"># Initial Q values with OIV</span></span>
<span id="cb4-21"><a href="#cb4-21"></a>    ttl_optim_acts <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-22"><a href="#cb4-22"></a>    alpha <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb4-23"><a href="#cb4-23"></a></span>
<span id="cb4-24"><a href="#cb4-24"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb4-25"><a href="#cb4-25"></a>        <span class="co"># get action</span></span>
<span id="cb4-26"><a href="#cb4-26"></a>        A <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-27"><a href="#cb4-27"></a>        <span class="cf">if</span> np.random.random() <span class="op">&gt;</span> epsilon:</span>
<span id="cb4-28"><a href="#cb4-28"></a>            A <span class="op">=</span> get_argmax(Q)</span>
<span id="cb4-29"><a href="#cb4-29"></a>        <span class="cf">else</span>:</span>
<span id="cb4-30"><a href="#cb4-30"></a>            A <span class="op">=</span> np.random.randint(<span class="dv">0</span>, K)</span>
<span id="cb4-31"><a href="#cb4-31"></a>        </span>
<span id="cb4-32"><a href="#cb4-32"></a>        R, is_optim <span class="op">=</span> bandit(q_star, A)</span>
<span id="cb4-33"><a href="#cb4-33"></a>        Q[A] <span class="op">+=</span> alpha <span class="op">*</span> (R <span class="op">-</span> Q[A])</span>
<span id="cb4-34"><a href="#cb4-34"></a></span>
<span id="cb4-35"><a href="#cb4-35"></a>        ttl_optim_acts <span class="op">+=</span> is_optim</span>
<span id="cb4-36"><a href="#cb4-36"></a>        rewards[i] <span class="op">=</span> R</span>
<span id="cb4-37"><a href="#cb4-37"></a>        optim_acts_ratio[i] <span class="op">=</span> ttl_optim_acts <span class="op">/</span> (i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-38"><a href="#cb4-38"></a></span>
<span id="cb4-39"><a href="#cb4-39"></a></span>
<span id="cb4-40"><a href="#cb4-40"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb4-41"><a href="#cb4-41"></a></span>
<span id="cb4-42"><a href="#cb4-42"></a>    <span class="co"># Initializing the hyper-parameters</span></span>
<span id="cb4-43"><a href="#cb4-43"></a>    K <span class="op">=</span> <span class="dv">10</span> <span class="co"># Number of arms</span></span>
<span id="cb4-44"><a href="#cb4-44"></a>    epsilons <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.0</span>]</span>
<span id="cb4-45"><a href="#cb4-45"></a>    init_vals <span class="op">=</span> [<span class="fl">0.0</span>, <span class="fl">5.0</span>]</span>
<span id="cb4-46"><a href="#cb4-46"></a>    num_steps <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb4-47"><a href="#cb4-47"></a>    total_rounds <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb4-48"><a href="#cb4-48"></a></span>
<span id="cb4-49"><a href="#cb4-49"></a>    <span class="co"># Initialize the environment</span></span>
<span id="cb4-50"><a href="#cb4-50"></a>    q_star <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="fl">1.0</span>, size<span class="op">=</span>K)</span>
<span id="cb4-51"><a href="#cb4-51"></a>    rewards <span class="op">=</span> np.zeros(shape<span class="op">=</span>(<span class="bu">len</span>(epsilons), total_rounds, num_steps))</span>
<span id="cb4-52"><a href="#cb4-52"></a>    optim_acts_ratio <span class="op">=</span> np.zeros(shape<span class="op">=</span>(<span class="bu">len</span>(epsilons), total_rounds, num_steps))</span>
<span id="cb4-53"><a href="#cb4-53"></a>    </span>
<span id="cb4-54"><a href="#cb4-54"></a>    <span class="co"># Run the k-armed bandits alg.</span></span>
<span id="cb4-55"><a href="#cb4-55"></a>    <span class="cf">for</span> i, (epsilon, init_val) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(epsilons, init_vals)):</span>
<span id="cb4-56"><a href="#cb4-56"></a>        <span class="cf">for</span> curr_round <span class="kw">in</span> <span class="bu">range</span>(total_rounds):</span>
<span id="cb4-57"><a href="#cb4-57"></a>            run_bandit(K, q_star, </span>
<span id="cb4-58"><a href="#cb4-58"></a>                       rewards[i, curr_round], </span>
<span id="cb4-59"><a href="#cb4-59"></a>                       optim_acts_ratio[i, curr_round], </span>
<span id="cb4-60"><a href="#cb4-60"></a>                       epsilon<span class="op">=</span>epsilon, </span>
<span id="cb4-61"><a href="#cb4-61"></a>                       num_steps<span class="op">=</span>num_steps,</span>
<span id="cb4-62"><a href="#cb4-62"></a>                       init_val<span class="op">=</span>init_val)</span>
<span id="cb4-63"><a href="#cb4-63"></a>    </span>
<span id="cb4-64"><a href="#cb4-64"></a>    rewards <span class="op">=</span> rewards.mean(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-65"><a href="#cb4-65"></a>    optim_acts_ratio <span class="op">=</span> optim_acts_ratio.mean(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-66"><a href="#cb4-66"></a></span>
<span id="cb4-67"><a href="#cb4-67"></a>    record <span class="op">=</span> {</span>
<span id="cb4-68"><a href="#cb4-68"></a>        <span class="st">'hyper_params'</span>: [epsilons, init_vals], </span>
<span id="cb4-69"><a href="#cb4-69"></a>        <span class="st">'rewards'</span>: rewards,</span>
<span id="cb4-70"><a href="#cb4-70"></a>        <span class="st">'optim_acts_ratio'</span>: optim_acts_ratio</span>
<span id="cb4-71"><a href="#cb4-71"></a>    }</span>
<span id="cb4-72"><a href="#cb4-72"></a></span>
<span id="cb4-73"><a href="#cb4-73"></a>    <span class="cf">for</span> vals <span class="kw">in</span> rewards:</span>
<span id="cb4-74"><a href="#cb4-74"></a>        plt.plot(vals)</span>
<span id="cb4-75"><a href="#cb4-75"></a>    plt.show()</span>
<span id="cb4-76"><a href="#cb4-76"></a>    <span class="co"># with open('./history/OIV_record.pkl', 'wb') as f:</span></span>
<span id="cb4-77"><a href="#cb4-77"></a>    <span class="co">#     pickle.dump(record, f)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="upper-confidence-bound-action-selection" class="level2">
<h2 class="anchored" data-anchor-id="upper-confidence-bound-action-selection">Upper-Confidence-Bound Action Selection</h2>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>example_2_4_UCB.py</strong></pre>
</div>
<div class="sourceCode" id="cb5" data-filename="example_2_4_UCB.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">from</span> typing <span class="im">import</span> Any</span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="im">import</span> pickle</span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="im">from</span> numpy <span class="im">import</span> dtype, ndarray</span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb5-7"><a href="#cb5-7"></a><span class="im">from</span> utils <span class="im">import</span> get_argmax, bandit</span>
<span id="cb5-8"><a href="#cb5-8"></a></span>
<span id="cb5-9"><a href="#cb5-9"></a>SEED <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb5-10"><a href="#cb5-10"></a>np.random.seed(SEED)</span>
<span id="cb5-11"><a href="#cb5-11"></a></span>
<span id="cb5-12"><a href="#cb5-12"></a></span>
<span id="cb5-13"><a href="#cb5-13"></a><span class="co"># running the k-armed bandit algorithm</span></span>
<span id="cb5-14"><a href="#cb5-14"></a><span class="kw">def</span> run_bandit(</span>
<span id="cb5-15"><a href="#cb5-15"></a>        K: <span class="bu">int</span>,</span>
<span id="cb5-16"><a href="#cb5-16"></a>        q_star: np.ndarray,</span>
<span id="cb5-17"><a href="#cb5-17"></a>        rewards: np.ndarray,</span>
<span id="cb5-18"><a href="#cb5-18"></a>        optim_acts_ratio: np.ndarray,</span>
<span id="cb5-19"><a href="#cb5-19"></a>        epsilon: <span class="bu">float</span>,</span>
<span id="cb5-20"><a href="#cb5-20"></a>        num_steps: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb5-21"><a href="#cb5-21"></a>        ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb5-22"><a href="#cb5-22"></a>    Q <span class="op">=</span> np.zeros(K)</span>
<span id="cb5-23"><a href="#cb5-23"></a>    N <span class="op">=</span> np.zeros(K)  <span class="co"># The number of times each action been selected</span></span>
<span id="cb5-24"><a href="#cb5-24"></a>    ttl_optim_acts <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-25"><a href="#cb5-25"></a>    </span>
<span id="cb5-26"><a href="#cb5-26"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb5-27"><a href="#cb5-27"></a>        A <span class="op">=</span> <span class="va">None</span></span>
<span id="cb5-28"><a href="#cb5-28"></a>        <span class="co"># Get action</span></span>
<span id="cb5-29"><a href="#cb5-29"></a>        <span class="cf">if</span> np.random.random() <span class="op">&gt;</span> epsilon:</span>
<span id="cb5-30"><a href="#cb5-30"></a>            A <span class="op">=</span> get_argmax(Q)</span>
<span id="cb5-31"><a href="#cb5-31"></a>        <span class="cf">else</span>:</span>
<span id="cb5-32"><a href="#cb5-32"></a>            A <span class="op">=</span> np.random.randint(<span class="dv">0</span>, K)</span>
<span id="cb5-33"><a href="#cb5-33"></a>        </span>
<span id="cb5-34"><a href="#cb5-34"></a>        R, is_optim <span class="op">=</span> bandit(q_star, A)</span>
<span id="cb5-35"><a href="#cb5-35"></a>        N[A] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb5-36"><a href="#cb5-36"></a>        Q[A] <span class="op">+=</span> (R <span class="op">-</span> Q[A]) <span class="op">/</span> N[A]</span>
<span id="cb5-37"><a href="#cb5-37"></a>        </span>
<span id="cb5-38"><a href="#cb5-38"></a>        ttl_optim_acts <span class="op">+=</span> is_optim</span>
<span id="cb5-39"><a href="#cb5-39"></a>        rewards[i] <span class="op">=</span> R</span>
<span id="cb5-40"><a href="#cb5-40"></a>        optim_acts_ratio[i] <span class="op">=</span> ttl_optim_acts <span class="op">/</span> (i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb5-41"><a href="#cb5-41"></a></span>
<span id="cb5-42"><a href="#cb5-42"></a></span>
<span id="cb5-43"><a href="#cb5-43"></a><span class="co"># running the bandit algorithm with UCB</span></span>
<span id="cb5-44"><a href="#cb5-44"></a><span class="kw">def</span> run_bandit_UCB(</span>
<span id="cb5-45"><a href="#cb5-45"></a>        K: <span class="bu">int</span>,</span>
<span id="cb5-46"><a href="#cb5-46"></a>        q_star: np.ndarray,</span>
<span id="cb5-47"><a href="#cb5-47"></a>        rewards: np.ndarray,</span>
<span id="cb5-48"><a href="#cb5-48"></a>        optim_acts_ratio: np.ndarray,</span>
<span id="cb5-49"><a href="#cb5-49"></a>        c: <span class="bu">float</span>,</span>
<span id="cb5-50"><a href="#cb5-50"></a>        num_steps: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb5-51"><a href="#cb5-51"></a>        ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb5-52"><a href="#cb5-52"></a>    Q <span class="op">=</span> np.zeros(K)</span>
<span id="cb5-53"><a href="#cb5-53"></a>    N <span class="op">=</span> np.zeros(K)  <span class="co"># The number of times each action been selected</span></span>
<span id="cb5-54"><a href="#cb5-54"></a>    ttl_optim_acts <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-55"><a href="#cb5-55"></a>    </span>
<span id="cb5-56"><a href="#cb5-56"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb5-57"><a href="#cb5-57"></a>        A <span class="op">=</span> <span class="va">None</span></span>
<span id="cb5-58"><a href="#cb5-58"></a>        </span>
<span id="cb5-59"><a href="#cb5-59"></a>        <span class="co"># Avoid 0-division:</span></span>
<span id="cb5-60"><a href="#cb5-60"></a>        <span class="co"># If there's 0 in N, then choose the action with N = 0</span></span>
<span id="cb5-61"><a href="#cb5-61"></a>        <span class="cf">if</span> <span class="dv">0</span> <span class="kw">in</span> N:</span>
<span id="cb5-62"><a href="#cb5-62"></a>            candidates <span class="op">=</span> np.argwhere(N <span class="op">==</span> <span class="dv">0</span>).flatten()</span>
<span id="cb5-63"><a href="#cb5-63"></a>            A <span class="op">=</span> np.random.choice(candidates)</span>
<span id="cb5-64"><a href="#cb5-64"></a>        <span class="cf">else</span>:</span>
<span id="cb5-65"><a href="#cb5-65"></a>            confidence <span class="op">=</span> c <span class="op">*</span> np.sqrt(np.log(i) <span class="op">/</span> N)</span>
<span id="cb5-66"><a href="#cb5-66"></a>            freqs: ndarray[Any, dtype[Any]] <span class="op">|</span> Any <span class="op">=</span> Q <span class="op">+</span> confidence</span>
<span id="cb5-67"><a href="#cb5-67"></a>            A <span class="op">=</span> np.argmax(freqs).flatten()</span>
<span id="cb5-68"><a href="#cb5-68"></a>        </span>
<span id="cb5-69"><a href="#cb5-69"></a>        R, is_optim <span class="op">=</span> bandit(q_star, A)</span>
<span id="cb5-70"><a href="#cb5-70"></a>        N[A] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb5-71"><a href="#cb5-71"></a>        Q[A] <span class="op">+=</span> (R <span class="op">-</span> Q[A]) <span class="op">/</span> N[A]</span>
<span id="cb5-72"><a href="#cb5-72"></a>        </span>
<span id="cb5-73"><a href="#cb5-73"></a>        ttl_optim_acts <span class="op">+=</span> is_optim</span>
<span id="cb5-74"><a href="#cb5-74"></a>        rewards[i] <span class="op">=</span> R</span>
<span id="cb5-75"><a href="#cb5-75"></a>        optim_acts_ratio[i] <span class="op">=</span> ttl_optim_acts <span class="op">/</span> (i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb5-76"><a href="#cb5-76"></a></span>
<span id="cb5-77"><a href="#cb5-77"></a></span>
<span id="cb5-78"><a href="#cb5-78"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb5-79"><a href="#cb5-79"></a>    </span>
<span id="cb5-80"><a href="#cb5-80"></a>    <span class="co"># Initializing the hyper-parameters</span></span>
<span id="cb5-81"><a href="#cb5-81"></a>    K <span class="op">=</span> <span class="dv">10</span>  <span class="co"># Number of arms</span></span>
<span id="cb5-82"><a href="#cb5-82"></a>    num_steps <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb5-83"><a href="#cb5-83"></a>    total_rounds <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb5-84"><a href="#cb5-84"></a>    q_star <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="fl">1.0</span>, size<span class="op">=</span>K)</span>
<span id="cb5-85"><a href="#cb5-85"></a>    hyper_params <span class="op">=</span> {<span class="st">'UCB'</span>: <span class="dv">2</span>, <span class="st">'epsilon'</span>: <span class="fl">0.1</span>}</span>
<span id="cb5-86"><a href="#cb5-86"></a>    rewards <span class="op">=</span> np.zeros(shape<span class="op">=</span>(<span class="bu">len</span>(hyper_params), total_rounds, num_steps))</span>
<span id="cb5-87"><a href="#cb5-87"></a>    optim_acts_ratio <span class="op">=</span> np.zeros(</span>
<span id="cb5-88"><a href="#cb5-88"></a>            shape<span class="op">=</span>(<span class="bu">len</span>(hyper_params), total_rounds, num_steps)</span>
<span id="cb5-89"><a href="#cb5-89"></a>            )</span>
<span id="cb5-90"><a href="#cb5-90"></a>    </span>
<span id="cb5-91"><a href="#cb5-91"></a>    <span class="co"># Run bandit alg. with e-greedy</span></span>
<span id="cb5-92"><a href="#cb5-92"></a>    <span class="cf">for</span> curr_round <span class="kw">in</span> tqdm(<span class="bu">range</span>(total_rounds)):</span>
<span id="cb5-93"><a href="#cb5-93"></a>        <span class="co"># for curr_round in range(total_rounds):</span></span>
<span id="cb5-94"><a href="#cb5-94"></a>        run_bandit(</span>
<span id="cb5-95"><a href="#cb5-95"></a>                K,</span>
<span id="cb5-96"><a href="#cb5-96"></a>                q_star,</span>
<span id="cb5-97"><a href="#cb5-97"></a>                rewards[<span class="dv">0</span>, curr_round],</span>
<span id="cb5-98"><a href="#cb5-98"></a>                optim_acts_ratio[<span class="dv">0</span>, curr_round],</span>
<span id="cb5-99"><a href="#cb5-99"></a>                epsilon<span class="op">=</span>hyper_params[<span class="st">'epsilon'</span>],</span>
<span id="cb5-100"><a href="#cb5-100"></a>                num_steps<span class="op">=</span>num_steps</span>
<span id="cb5-101"><a href="#cb5-101"></a>                )</span>
<span id="cb5-102"><a href="#cb5-102"></a>    </span>
<span id="cb5-103"><a href="#cb5-103"></a>    <span class="co"># Run UCB and get records</span></span>
<span id="cb5-104"><a href="#cb5-104"></a>    <span class="cf">for</span> curr_round <span class="kw">in</span> tqdm(<span class="bu">range</span>(total_rounds)):</span>
<span id="cb5-105"><a href="#cb5-105"></a>        <span class="co"># for curr_round in range(total_rounds):</span></span>
<span id="cb5-106"><a href="#cb5-106"></a>        run_bandit_UCB(</span>
<span id="cb5-107"><a href="#cb5-107"></a>                K,</span>
<span id="cb5-108"><a href="#cb5-108"></a>                q_star,</span>
<span id="cb5-109"><a href="#cb5-109"></a>                rewards[<span class="dv">1</span>, curr_round],</span>
<span id="cb5-110"><a href="#cb5-110"></a>                optim_acts_ratio[<span class="dv">1</span>, curr_round],</span>
<span id="cb5-111"><a href="#cb5-111"></a>                c<span class="op">=</span>hyper_params[<span class="st">'UCB'</span>],</span>
<span id="cb5-112"><a href="#cb5-112"></a>                num_steps<span class="op">=</span>num_steps</span>
<span id="cb5-113"><a href="#cb5-113"></a>                )</span>
<span id="cb5-114"><a href="#cb5-114"></a>    </span>
<span id="cb5-115"><a href="#cb5-115"></a>    rewards <span class="op">=</span> rewards.mean(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-116"><a href="#cb5-116"></a>    optim_acts_ratio <span class="op">=</span> optim_acts_ratio.mean(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-117"><a href="#cb5-117"></a>    </span>
<span id="cb5-118"><a href="#cb5-118"></a>    record <span class="op">=</span> {</span>
<span id="cb5-119"><a href="#cb5-119"></a>            <span class="st">'hyper_params'</span>: hyper_params,</span>
<span id="cb5-120"><a href="#cb5-120"></a>            <span class="st">'rewards'</span>: rewards,</span>
<span id="cb5-121"><a href="#cb5-121"></a>            <span class="st">'optim_acts_ratio'</span>: optim_acts_ratio</span>
<span id="cb5-122"><a href="#cb5-122"></a>            }</span>
<span id="cb5-123"><a href="#cb5-123"></a>    data <span class="op">=</span> rewards</span>
<span id="cb5-124"><a href="#cb5-124"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>), dpi<span class="op">=</span><span class="dv">150</span>)</span>
<span id="cb5-125"><a href="#cb5-125"></a>    plt.grid(c<span class="op">=</span><span class="st">'lightgray'</span>)</span>
<span id="cb5-126"><a href="#cb5-126"></a>    plt.margins(<span class="fl">0.02</span>)</span>
<span id="cb5-127"><a href="#cb5-127"></a>    <span class="co"># revers the loop for a better visualization</span></span>
<span id="cb5-128"><a href="#cb5-128"></a>    <span class="co"># colors = ['cornflowerblue', 'tomato', 'lightseagreen']</span></span>
<span id="cb5-129"><a href="#cb5-129"></a>    colors <span class="op">=</span> [<span class="st">'r'</span>, <span class="st">'b'</span>]</span>
<span id="cb5-130"><a href="#cb5-130"></a>    meta <span class="op">=</span> record[<span class="st">'hyper_params'</span>]</span>
<span id="cb5-131"><a href="#cb5-131"></a>    optim_ratio <span class="op">=</span> (optim_acts_ratio <span class="op">*</span> <span class="dv">100</span>)</span>
<span id="cb5-132"><a href="#cb5-132"></a>    legends <span class="op">=</span> [<span class="ss">f'$\epsilon$-greedy $\epsilon$=</span><span class="sc">{</span>meta[<span class="st">"epsilon"</span>]<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb5-133"><a href="#cb5-133"></a>               <span class="ss">f'UCB c=</span><span class="sc">{</span>meta[<span class="st">"UCB"</span>]<span class="sc">}</span><span class="ss">'</span>]</span>
<span id="cb5-134"><a href="#cb5-134"></a>    fontdict <span class="op">=</span> {</span>
<span id="cb5-135"><a href="#cb5-135"></a>            <span class="st">'fontsize'</span>: <span class="dv">12</span>,</span>
<span id="cb5-136"><a href="#cb5-136"></a>            <span class="st">'fontweight'</span>: <span class="st">'bold'</span>,</span>
<span id="cb5-137"><a href="#cb5-137"></a>            }</span>
<span id="cb5-138"><a href="#cb5-138"></a>    plt.plot(rewards[<span class="dv">0</span>, :], linestyle<span class="op">=</span><span class="st">'-'</span>, linewidth<span class="op">=</span><span class="dv">2</span> )</span>
<span id="cb5-139"><a href="#cb5-139"></a>    plt.plot(rewards[<span class="dv">1</span>, :], linestyle<span class="op">=</span><span class="st">'-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-140"><a href="#cb5-140"></a>    plt.tick_params(axis<span class="op">=</span><span class="st">'both'</span>, labelsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb5-141"><a href="#cb5-141"></a>    plt.xlabel(<span class="st">'step'</span>, fontdict<span class="op">=</span>fontdict)</span>
<span id="cb5-142"><a href="#cb5-142"></a>    plt.ylabel(<span class="st">'reward'</span>, fontdict<span class="op">=</span>fontdict)</span>
<span id="cb5-143"><a href="#cb5-143"></a>    plt.legend(loc<span class="op">=</span><span class="dv">4</span>, fontsize<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb5-144"><a href="#cb5-144"></a>plt.show()</span>
<span id="cb5-145"><a href="#cb5-145"></a></span>
<span id="cb5-146"><a href="#cb5-146"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'./history/UCB_record.pkl'</span>, <span class="st">'wb'</span>) <span class="im">as</span> f:</span>
<span id="cb5-147"><a href="#cb5-147"></a>    pickle.dump(record, f)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="gradient-bandit-method" class="level2">
<h2 class="anchored" data-anchor-id="gradient-bandit-method">Gradient Bandit method</h2>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>example_2_5_gradient.py</strong></pre>
</div>
<div class="sourceCode" id="cb6" data-filename="example_2_5_gradient.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="im">import</span> pickle</span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="im">import</span> itertools</span>
<span id="cb6-5"><a href="#cb6-5"></a></span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="im">from</span> utils <span class="im">import</span> bandit</span>
<span id="cb6-7"><a href="#cb6-7"></a></span>
<span id="cb6-8"><a href="#cb6-8"></a>SEED <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb6-9"><a href="#cb6-9"></a>np.random.seed(SEED)</span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a></span>
<span id="cb6-12"><a href="#cb6-12"></a><span class="kw">def</span> update_policy(H: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb6-13"><a href="#cb6-13"></a>    <span class="cf">return</span> np.exp(H) <span class="op">/</span> np.exp(H).<span class="bu">sum</span>()</span>
<span id="cb6-14"><a href="#cb6-14"></a></span>
<span id="cb6-15"><a href="#cb6-15"></a></span>
<span id="cb6-16"><a href="#cb6-16"></a><span class="kw">def</span> update_H(</span>
<span id="cb6-17"><a href="#cb6-17"></a>        H: np.ndarray,</span>
<span id="cb6-18"><a href="#cb6-18"></a>        policy: np.ndarray,</span>
<span id="cb6-19"><a href="#cb6-19"></a>        alpha: <span class="bu">float</span>,</span>
<span id="cb6-20"><a href="#cb6-20"></a>        A: <span class="bu">int</span>,</span>
<span id="cb6-21"><a href="#cb6-21"></a>        curr_reward: <span class="bu">float</span>,</span>
<span id="cb6-22"><a href="#cb6-22"></a>        avg_reward: <span class="bu">float</span></span>
<span id="cb6-23"><a href="#cb6-23"></a>        ) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb6-24"><a href="#cb6-24"></a>    selec <span class="op">=</span> np.zeros(<span class="bu">len</span>(H), dtype<span class="op">=</span>np.float32)</span>
<span id="cb6-25"><a href="#cb6-25"></a>    selec[A] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb6-26"><a href="#cb6-26"></a>    H <span class="op">=</span> H <span class="op">+</span> alpha <span class="op">*</span> (curr_reward <span class="op">-</span> avg_reward) <span class="op">*</span> (selec <span class="op">-</span> policy)</span>
<span id="cb6-27"><a href="#cb6-27"></a>    <span class="cf">return</span> H</span>
<span id="cb6-28"><a href="#cb6-28"></a></span>
<span id="cb6-29"><a href="#cb6-29"></a></span>
<span id="cb6-30"><a href="#cb6-30"></a><span class="co"># running the k-armed bandit algorithm</span></span>
<span id="cb6-31"><a href="#cb6-31"></a><span class="kw">def</span> run_bandit(</span>
<span id="cb6-32"><a href="#cb6-32"></a>        K: <span class="bu">int</span>,</span>
<span id="cb6-33"><a href="#cb6-33"></a>        q_star: np.ndarray,</span>
<span id="cb6-34"><a href="#cb6-34"></a>        rewards: np.ndarray,</span>
<span id="cb6-35"><a href="#cb6-35"></a>        optim_acts_ratio: np.ndarray,</span>
<span id="cb6-36"><a href="#cb6-36"></a>        alpha: <span class="bu">float</span>,</span>
<span id="cb6-37"><a href="#cb6-37"></a>        baseline: <span class="bu">bool</span>,</span>
<span id="cb6-38"><a href="#cb6-38"></a>        num_steps: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb6-39"><a href="#cb6-39"></a>        ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb6-40"><a href="#cb6-40"></a>    H <span class="op">=</span> np.zeros(K, dtype<span class="op">=</span>np.float32)  <span class="co"># initialize preference</span></span>
<span id="cb6-41"><a href="#cb6-41"></a>    policy <span class="op">=</span> np.ones(K, dtype<span class="op">=</span>np.float32) <span class="op">/</span> K</span>
<span id="cb6-42"><a href="#cb6-42"></a>    ttl_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-43"><a href="#cb6-43"></a>    ttl_optim_acts <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-44"><a href="#cb6-44"></a>    </span>
<span id="cb6-45"><a href="#cb6-45"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb6-46"><a href="#cb6-46"></a>        </span>
<span id="cb6-47"><a href="#cb6-47"></a>        A <span class="op">=</span> np.random.choice(np.arange(K), p<span class="op">=</span>policy)</span>
<span id="cb6-48"><a href="#cb6-48"></a>        reward, is_optim <span class="op">=</span> bandit(q_star, A)</span>
<span id="cb6-49"><a href="#cb6-49"></a>        avg_reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-50"><a href="#cb6-50"></a>        </span>
<span id="cb6-51"><a href="#cb6-51"></a>        <span class="cf">if</span> baseline:</span>
<span id="cb6-52"><a href="#cb6-52"></a>            <span class="co"># Get average reward unitl timestep=i</span></span>
<span id="cb6-53"><a href="#cb6-53"></a>            avg_reward <span class="op">=</span> ttl_reward <span class="op">/</span> i <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> reward</span>
<span id="cb6-54"><a href="#cb6-54"></a>        </span>
<span id="cb6-55"><a href="#cb6-55"></a>        <span class="co"># Update preference and policy</span></span>
<span id="cb6-56"><a href="#cb6-56"></a>        H <span class="op">=</span> update_H(H, policy, alpha, A, reward, avg_reward)</span>
<span id="cb6-57"><a href="#cb6-57"></a>        policy <span class="op">=</span> update_policy(H)</span>
<span id="cb6-58"><a href="#cb6-58"></a>        </span>
<span id="cb6-59"><a href="#cb6-59"></a>        ttl_reward <span class="op">+=</span> reward</span>
<span id="cb6-60"><a href="#cb6-60"></a>        ttl_optim_acts <span class="op">+=</span> is_optim</span>
<span id="cb6-61"><a href="#cb6-61"></a>        rewards[i] <span class="op">=</span> reward</span>
<span id="cb6-62"><a href="#cb6-62"></a>        optim_acts_ratio[i] <span class="op">=</span> ttl_optim_acts <span class="op">/</span> (i <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb6-63"><a href="#cb6-63"></a></span>
<span id="cb6-64"><a href="#cb6-64"></a></span>
<span id="cb6-65"><a href="#cb6-65"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb6-66"><a href="#cb6-66"></a>    </span>
<span id="cb6-67"><a href="#cb6-67"></a>    <span class="co"># Initializing the hyperparameters</span></span>
<span id="cb6-68"><a href="#cb6-68"></a>    K <span class="op">=</span> <span class="dv">10</span>  <span class="co"># Number of arms</span></span>
<span id="cb6-69"><a href="#cb6-69"></a>    alphas <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.4</span>]</span>
<span id="cb6-70"><a href="#cb6-70"></a>    baselines <span class="op">=</span> [<span class="va">False</span>, <span class="va">True</span>]</span>
<span id="cb6-71"><a href="#cb6-71"></a>    hyper_params <span class="op">=</span> <span class="bu">list</span>(itertools.product(baselines, alphas))</span>
<span id="cb6-72"><a href="#cb6-72"></a>    </span>
<span id="cb6-73"><a href="#cb6-73"></a>    num_steps <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb6-74"><a href="#cb6-74"></a>    total_rounds <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb6-75"><a href="#cb6-75"></a>    </span>
<span id="cb6-76"><a href="#cb6-76"></a>    rewards <span class="op">=</span> np.zeros(shape<span class="op">=</span>(<span class="bu">len</span>(hyper_params), total_rounds, num_steps))</span>
<span id="cb6-77"><a href="#cb6-77"></a>    optim_acts_ratio <span class="op">=</span> np.zeros(</span>
<span id="cb6-78"><a href="#cb6-78"></a>        shape<span class="op">=</span>(<span class="bu">len</span>(hyper_params), total_rounds, num_steps)</span>
<span id="cb6-79"><a href="#cb6-79"></a>        )</span>
<span id="cb6-80"><a href="#cb6-80"></a>    q_star <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="fl">4.0</span>, scale<span class="op">=</span><span class="fl">1.0</span>, size<span class="op">=</span>K)</span>
<span id="cb6-81"><a href="#cb6-81"></a>    </span>
<span id="cb6-82"><a href="#cb6-82"></a>    <span class="bu">print</span>(hyper_params)</span>
<span id="cb6-83"><a href="#cb6-83"></a>    <span class="cf">for</span> i, (is_baseline, alpha) <span class="kw">in</span> <span class="bu">enumerate</span>(hyper_params):</span>
<span id="cb6-84"><a href="#cb6-84"></a>        <span class="cf">for</span> curr_round <span class="kw">in</span> <span class="bu">range</span>(total_rounds):</span>
<span id="cb6-85"><a href="#cb6-85"></a>            run_bandit(</span>
<span id="cb6-86"><a href="#cb6-86"></a>                K,</span>
<span id="cb6-87"><a href="#cb6-87"></a>                q_star,</span>
<span id="cb6-88"><a href="#cb6-88"></a>                rewards[i, curr_round],</span>
<span id="cb6-89"><a href="#cb6-89"></a>                optim_acts_ratio[i, curr_round],</span>
<span id="cb6-90"><a href="#cb6-90"></a>                alpha,</span>
<span id="cb6-91"><a href="#cb6-91"></a>                is_baseline,</span>
<span id="cb6-92"><a href="#cb6-92"></a>                num_steps</span>
<span id="cb6-93"><a href="#cb6-93"></a>                )</span>
<span id="cb6-94"><a href="#cb6-94"></a>    </span>
<span id="cb6-95"><a href="#cb6-95"></a>    optim_acts_ratio <span class="op">=</span> optim_acts_ratio.mean(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-96"><a href="#cb6-96"></a>    </span>
<span id="cb6-97"><a href="#cb6-97"></a>    <span class="cf">for</span> val <span class="kw">in</span> optim_acts_ratio:</span>
<span id="cb6-98"><a href="#cb6-98"></a>        plt.plot(val)</span>
<span id="cb6-99"><a href="#cb6-99"></a>    plt.show()</span>
<span id="cb6-100"><a href="#cb6-100"></a>    </span>
<span id="cb6-101"><a href="#cb6-101"></a>    record <span class="op">=</span> {</span>
<span id="cb6-102"><a href="#cb6-102"></a>            <span class="st">'hyper_params'</span>: hyper_params,</span>
<span id="cb6-103"><a href="#cb6-103"></a>            <span class="st">'optim_acts_ratio'</span>: optim_acts_ratio</span>
<span id="cb6-104"><a href="#cb6-104"></a>            }</span>
<span id="cb6-105"><a href="#cb6-105"></a>    </span>
<span id="cb6-106"><a href="#cb6-106"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">'./history/sga_record.pkl'</span>, <span class="st">'wb'</span>) <span class="im">as</span> f:</span>
<span id="cb6-107"><a href="#cb6-107"></a>         pickle.dump(record, f)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="scripts-for-visualization" class="level2">
<h2 class="anchored" data-anchor-id="scripts-for-visualization">Scripts for visualization</h2>
<p>Run the following script and save the output.</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>plot_gradient.py</strong></pre>
</div>
<div class="sourceCode" id="cb7" data-filename="plot_gradient.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-3"><a href="#cb7-3"></a><span class="im">import</span> pickle</span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-5"><a href="#cb7-5"></a></span>
<span id="cb7-6"><a href="#cb7-6"></a></span>
<span id="cb7-7"><a href="#cb7-7"></a><span class="co"># Plot results</span></span>
<span id="cb7-8"><a href="#cb7-8"></a><span class="kw">def</span> plot(</span>
<span id="cb7-9"><a href="#cb7-9"></a>        data: np.ndarray,</span>
<span id="cb7-10"><a href="#cb7-10"></a>        legends: <span class="bu">list</span>,</span>
<span id="cb7-11"><a href="#cb7-11"></a>        xlabel: <span class="bu">str</span>,</span>
<span id="cb7-12"><a href="#cb7-12"></a>        ylabel: <span class="bu">str</span>,</span>
<span id="cb7-13"><a href="#cb7-13"></a>        filename: <span class="bu">str</span> <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb7-14"><a href="#cb7-14"></a>        fn<span class="op">=</span><span class="kw">lambda</span>: <span class="va">None</span>, ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb7-15"><a href="#cb7-15"></a>    fontdict <span class="op">=</span> {</span>
<span id="cb7-16"><a href="#cb7-16"></a>            <span class="st">'fontsize'</span>: <span class="dv">12</span>,</span>
<span id="cb7-17"><a href="#cb7-17"></a>            <span class="st">'fontweight'</span>: <span class="st">'bold'</span>,</span>
<span id="cb7-18"><a href="#cb7-18"></a>            }</span>
<span id="cb7-19"><a href="#cb7-19"></a>    </span>
<span id="cb7-20"><a href="#cb7-20"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>), dpi<span class="op">=</span><span class="dv">150</span>)</span>
<span id="cb7-21"><a href="#cb7-21"></a>    plt.grid(c<span class="op">=</span><span class="st">'lightgray'</span>)</span>
<span id="cb7-22"><a href="#cb7-22"></a>    plt.margins(<span class="fl">0.02</span>)</span>
<span id="cb7-23"><a href="#cb7-23"></a>    </span>
<span id="cb7-24"><a href="#cb7-24"></a>    <span class="co"># revers the loop for a better visualization</span></span>
<span id="cb7-25"><a href="#cb7-25"></a>    colors <span class="op">=</span> [<span class="st">'navy'</span>, <span class="st">'lightblue'</span>, <span class="st">'tomato'</span>, <span class="st">'pink'</span>]</span>
<span id="cb7-26"><a href="#cb7-26"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(data) <span class="op">-</span> <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb7-27"><a href="#cb7-27"></a>        <span class="co"># data[i] = uniform_filter(data[i])</span></span>
<span id="cb7-28"><a href="#cb7-28"></a>        plt.plot(data[i], label<span class="op">=</span>legends[i], linewidth<span class="op">=</span><span class="fl">1.5</span>, c<span class="op">=</span>colors[i])</span>
<span id="cb7-29"><a href="#cb7-29"></a>    </span>
<span id="cb7-30"><a href="#cb7-30"></a>    <span class="co"># get rid of the top/right frame lines</span></span>
<span id="cb7-31"><a href="#cb7-31"></a>    <span class="cf">for</span> i, spine <span class="kw">in</span> <span class="bu">enumerate</span>(plt.gca().spines.values()):</span>
<span id="cb7-32"><a href="#cb7-32"></a>        <span class="cf">if</span> i <span class="kw">in</span> [<span class="dv">0</span>, <span class="dv">2</span>]:</span>
<span id="cb7-33"><a href="#cb7-33"></a>            spine.set_linewidth(<span class="fl">1.5</span>)</span>
<span id="cb7-34"><a href="#cb7-34"></a>            <span class="cf">continue</span></span>
<span id="cb7-35"><a href="#cb7-35"></a>        spine.set_visible(<span class="va">False</span>)</span>
<span id="cb7-36"><a href="#cb7-36"></a>    </span>
<span id="cb7-37"><a href="#cb7-37"></a>    plt.tick_params(axis<span class="op">=</span><span class="st">'both'</span>, labelsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb7-38"><a href="#cb7-38"></a>    plt.xlabel(xlabel, fontdict<span class="op">=</span>fontdict)</span>
<span id="cb7-39"><a href="#cb7-39"></a>    plt.ylabel(ylabel, fontdict<span class="op">=</span>fontdict)</span>
<span id="cb7-40"><a href="#cb7-40"></a>    <span class="co"># plt.legend(loc=4, fontsize=13)</span></span>
<span id="cb7-41"><a href="#cb7-41"></a>    fn()</span>
<span id="cb7-42"><a href="#cb7-42"></a>    </span>
<span id="cb7-43"><a href="#cb7-43"></a>    plt.text(<span class="dv">500</span>, <span class="dv">57</span>, s<span class="op">=</span><span class="st">"$</span><span class="ch">\\</span><span class="st">alpha = 0.4$"</span>, c<span class="op">=</span>colors[<span class="dv">3</span>], fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb7-44"><a href="#cb7-44"></a>    plt.text(<span class="dv">500</span>, <span class="dv">28</span>, s<span class="op">=</span><span class="st">"$</span><span class="ch">\\</span><span class="st">alpha = 0.4$"</span>, c<span class="op">=</span>colors[<span class="dv">1</span>], fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb7-45"><a href="#cb7-45"></a>    plt.text(<span class="dv">900</span>, <span class="dv">72</span>, s<span class="op">=</span><span class="st">"$</span><span class="ch">\\</span><span class="st">alpha = 0.1$"</span>, c<span class="op">=</span>colors[<span class="dv">2</span>], fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb7-46"><a href="#cb7-46"></a>    plt.text(<span class="dv">900</span>, <span class="dv">52</span>, s<span class="op">=</span><span class="st">"$</span><span class="ch">\\</span><span class="st">alpha = 0.1$"</span>, c<span class="op">=</span>colors[<span class="dv">0</span>], fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb7-47"><a href="#cb7-47"></a>    </span>
<span id="cb7-48"><a href="#cb7-48"></a>    plt.text(<span class="dv">770</span>, <span class="dv">65</span>, s<span class="op">=</span><span class="st">"with baseline"</span>, c<span class="op">=</span>colors[<span class="dv">2</span>], fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb7-49"><a href="#cb7-49"></a>    plt.text(<span class="dv">770</span>, <span class="dv">42</span>, s<span class="op">=</span><span class="st">"without baseline"</span>, c<span class="op">=</span>colors[<span class="dv">0</span>], fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb7-50"><a href="#cb7-50"></a>    </span>
<span id="cb7-51"><a href="#cb7-51"></a>    <span class="cf">if</span> <span class="kw">not</span> filename:</span>
<span id="cb7-52"><a href="#cb7-52"></a>        plt.show()</span>
<span id="cb7-53"><a href="#cb7-53"></a>    <span class="cf">else</span>:</span>
<span id="cb7-54"><a href="#cb7-54"></a>        plt.savefig(<span class="ss">f'./plots/</span><span class="sc">{</span>filename<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb7-55"><a href="#cb7-55"></a></span>
<span id="cb7-56"><a href="#cb7-56"></a></span>
<span id="cb7-57"><a href="#cb7-57"></a><span class="kw">def</span> plot_result(</span>
<span id="cb7-58"><a href="#cb7-58"></a>        optim_ratio: np.ndarray,</span>
<span id="cb7-59"><a href="#cb7-59"></a>        legends: <span class="bu">list</span>,</span>
<span id="cb7-60"><a href="#cb7-60"></a>        output_name: <span class="bu">str</span> <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-61"><a href="#cb7-61"></a>        ):</span>
<span id="cb7-62"><a href="#cb7-62"></a>    <span class="co"># Set tick labels</span></span>
<span id="cb7-63"><a href="#cb7-63"></a>    fn <span class="op">=</span> <span class="kw">lambda</span>: plt.yticks(</span>
<span id="cb7-64"><a href="#cb7-64"></a>        np.arange(<span class="dv">0</span>, <span class="dv">100</span>, <span class="dv">10</span>), labels<span class="op">=</span>[<span class="ss">f'</span><span class="sc">{</span>val<span class="sc">}</span><span class="ss">%'</span> <span class="cf">for</span> val <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="dv">100</span>, <span class="dv">10</span>)]</span>
<span id="cb7-65"><a href="#cb7-65"></a>        )</span>
<span id="cb7-66"><a href="#cb7-66"></a>    plot(</span>
<span id="cb7-67"><a href="#cb7-67"></a>        optim_ratio,</span>
<span id="cb7-68"><a href="#cb7-68"></a>        legends,</span>
<span id="cb7-69"><a href="#cb7-69"></a>        xlabel<span class="op">=</span><span class="st">'Time step'</span>,</span>
<span id="cb7-70"><a href="#cb7-70"></a>        ylabel<span class="op">=</span><span class="st">'% Optimal actions'</span>,</span>
<span id="cb7-71"><a href="#cb7-71"></a>        filename<span class="op">=</span>output_name,</span>
<span id="cb7-72"><a href="#cb7-72"></a>        fn<span class="op">=</span>fn</span>
<span id="cb7-73"><a href="#cb7-73"></a>        )</span>
<span id="cb7-74"><a href="#cb7-74"></a></span>
<span id="cb7-75"><a href="#cb7-75"></a></span>
<span id="cb7-76"><a href="#cb7-76"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb7-77"><a href="#cb7-77"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">'./history/sga_record.pkl'</span>, <span class="st">'rb'</span>) <span class="im">as</span> f:</span>
<span id="cb7-78"><a href="#cb7-78"></a>        history <span class="op">=</span> pickle.load(f)</span>
<span id="cb7-79"><a href="#cb7-79"></a>    </span>
<span id="cb7-80"><a href="#cb7-80"></a>    optim_ratio <span class="op">=</span> history[<span class="st">'optim_acts_ratio'</span>] <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb7-81"><a href="#cb7-81"></a>    hyper_params <span class="op">=</span> history[<span class="st">'hyper_params'</span>]</span>
<span id="cb7-82"><a href="#cb7-82"></a>    </span>
<span id="cb7-83"><a href="#cb7-83"></a>    <span class="co"># plot_result(optim_ratio, hyper_params, output_name="example_2_5_sga.png")</span></span>
<span id="cb7-84"><a href="#cb7-84"></a>    plot_result(optim_ratio, hyper_params, output_name<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb7-85"><a href="#cb7-85"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>Outline:</p>
<ol type="1">
<li><strong><span class="math inline">\(\epsilon\)</span>-greedy Method</strong>:
<ul>
<li>Overview of the method.</li>
<li>Mathematical formulation.</li>
<li>Parameter sensitivity and tuning.</li>
</ul></li>
<li><strong>UCB (Upper Confidence Bound) Method</strong>:
<ul>
<li>Explanation of UCB and its deterministic exploration mechanism.</li>
<li>The mathematical equation governing UCB.</li>
<li>Parameter considerations and impact on performance.</li>
</ul></li>
<li><strong>Gradient Bandit Algorithm</strong>:
<ul>
<li>Introduction to action preferences and how they differ from action values.</li>
<li>Derivation of the softmax probability distribution.</li>
<li>Discussion on parameter choice and influence on outcomes.</li>
</ul></li>
<li><strong>Optimistic Initialization</strong>:
<ul>
<li>How optimistic estimates influence exploration.</li>
<li>Comparison with <span class="math inline">\(\epsilon\)</span>-greedy methods.</li>
</ul></li>
</ol>
<p>Let’s begin with the <span class="math inline">\(\epsilon\)</span> -greedy method:</p>
<section id="epsilon-greedy-method" class="level3">
<h3 class="anchored" data-anchor-id="epsilon-greedy-method">1. <span class="math inline">\(\epsilon\)</span>-greedy Method</h3>
<p>This is one of the simplest algorithms to balance exploration and exploitation. The method works by choosing a random action with probability <span class="math inline">\(\epsilon\)</span> (exploration) and the action with the highest estimated value (exploitation) with probability <span class="math inline">\(1 - \epsilon\)</span>.</p>
<section id="mathematical-formulation" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-formulation">Mathematical Formulation</h4>
<ul>
<li>Let <span class="math inline">\(Q(a)\)</span> be the estimated value of action <span class="math inline">\(a\)</span>.</li>
<li>At each time step <span class="math inline">\(t\)</span>, the agent chooses:
<ul>
<li>A random action <span class="math inline">\(a\)</span> with probability <span class="math inline">\(\epsilon\)</span>.</li>
<li>The action with the maximum <span class="math inline">\(Q(a)\)</span>,</li>
<li><span class="math inline">\(\text{argmax}_a Q(a)\)</span>, with probability <span class="math inline">\({1 - \epsilon}.\)</span></li>
</ul></li>
</ul>
</section>
<section id="update-rule" class="level4">
<h4 class="anchored" data-anchor-id="update-rule">Update Rule</h4>
<p>The estimate for the value of an action, <span class="math inline">\(Q(a)\)</span>, is updated using the following equation after observing a reward <span class="math inline">\(R_t\)</span> for taking action <span class="math inline">\(a\)</span>: <span class="math display">\[
Q_{t+1}(a) = Q_t(a) + \alpha \left( R_t - Q_t(a) \right)
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(\alpha\)</span> is the <strong>step size</strong> or learning rate, determining how much the estimate is updated based on new information.</p></li>
<li><p><span class="math inline">\(R_t\)</span> is the reward received after action <span class="math inline">\(a\)</span> at time <span class="math inline">\(t\)</span>.</p></li>
</ul>
</section>
<section id="parameter-sensitivity" class="level4">
<h4 class="anchored" data-anchor-id="parameter-sensitivity">Parameter Sensitivity</h4>
<ul>
<li><strong><span class="math inline">\(\epsilon\)</span></strong>: A small value of <span class="math inline">\(\epsilon\)</span> (e.g., 0.01) results in a mostly greedy policy with occasional exploration, while a larger <span class="math inline">\(\epsilon\)</span> (e.g., 0.1) encourages moreexploration. The optimal value balances sufficient exploration to discover rewarding actions while exploiting known high-value actions effectively.</li>
</ul>
</section>
</section>
<section id="ucb-upper-confidence-bound-method" class="level3">
<h3 class="anchored" data-anchor-id="ucb-upper-confidence-bound-method">2. UCB (Upper Confidence Bound) Method</h3>
<p>UCB addresses the exploration-exploitation dilemma by adding a confidence term to the action value. The idea is to choose actions that might not have the highest estimated value but have been less explored, thus increasing exploration in a systematic way.</p>
<section id="mathematical-equation" class="level4">
<h4 class="anchored" data-anchor-id="mathematical-equation">Mathematical Equation</h4>
<p>The action <span class="math inline">\(a_t\)</span> selected at time <span class="math inline">\(t\)</span> is:</p>
<p><span class="math display">\[a_t = \text{argmax}_a
    \left(
        Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \right)
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(Q_t(a)\)</span> is the estimated value of action <span class="math inline">\(a\)</span> at time <span class="math inline">\(t\)</span>.</li>
<li><span class="math inline">\(c\)</span> is a parameter controlling the degree of exploration. A larger <span class="math inline">\(c\)</span> increases exploration.</li>
<li><span class="math inline">\(N_t(a)\)</span> is the number of times action <span class="math inline">\(a\)</span> has been selected so far.</li>
<li><span class="math inline">\(\ln t\)</span> scales the confidence bound logarithmically with time.</li>
</ul>
<p>This approach encourages the selection of actions with high uncertainty (lower ( N_t(a) )), balancing exploration based on how frequently each action has been tried.</p>
</section>
<section id="parameter-considerations" class="level4">
<h4 class="anchored" data-anchor-id="parameter-considerations">Parameter Considerations</h4>
<ul>
<li>The parameter <span class="math inline">\(c\)</span> is crucial. If <span class="math inline">\(c\)</span> is too low, the algorithm might not explore enough; if too high, it may explore excessively. The optimal <span class="math inline">\(c\)</span> varies depending on the problem setting.</li>
</ul>
</section>
</section>
<section id="gradient-bandit-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="gradient-bandit-algorithm">3. Gradient Bandit Algorithm</h3>
<p>Unlike <span class="math inline">\(\epsilon\)</span>-greedy and UCB methods that estimate action values, gradient bandit algorithms estimate <strong>action preferences</strong>, denoted <span class="math inline">\(H(a)\)</span>. These preferences are used to determine the probability of selecting each action.</p>
<section id="softmax-distribution" class="level4">
<h4 class="anchored" data-anchor-id="softmax-distribution">Softmax Distribution</h4>
<p>The probability of selecting action <span class="math inline">\(a\)</span> is given by: <span class="math display">\[
    \pi(a) =\dfrac{e^{H(a)}}{\displaystyle \sum_{b=1} ^{k} e^{H(b)}}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(H(a)\)</span> is the preference for action <span class="math inline">\(a\)</span>.</li>
<li><span class="math inline">\(\pi(a)\)</span> represents the probability of taking action <span class="math inline">\(a\)</span>.</li>
</ul>
</section>
<section id="update-rule-1" class="level4">
<h4 class="anchored" data-anchor-id="update-rule-1">Update Rule</h4>
<p>The preferences are updated based on the received reward as follows:</p>
<p><span class="math display">\[
\begin{aligned}
    H_{t+1}(a) &amp;=
        H_t(a) + \alpha (R_t - \bar{R}_t)(1 - \pi_t(a))
        \quad \text{if action } a \text{ was chosen}
        \\  
    H_{t+1}(b) &amp;=
        H_t(b) - \alpha (R_t - \bar{R}_t)\pi_t(b)
        \quad \text{for all other actions } b
\end{aligned}
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\alpha\)</span> is the learning rate.</li>
<li><span class="math inline">\(\bar{R}_t\)</span> is the average reward received so far, acting as a baseline to stabilize learning.</li>
</ul>
<p>This update rule encourages actions that receive above-average rewards while discouraging less rewarding ones.</p>
</section>
</section>
<section id="optimistic-initialization" class="level3">
<h3 class="anchored" data-anchor-id="optimistic-initialization">4. Optimistic Initialization</h3>
<p>This is a simple method where the initial estimates <span class="math inline">\(Q_0(a)\)</span> are set to high values, encouraging the algorithm to explore different actions because all initial action values seem promising.</p>
<section id="comparison-with-epsilon-greedy" class="level4">
<h4 class="anchored" data-anchor-id="comparison-with-epsilon-greedy">Comparison with <span class="math inline">\(\epsilon\)</span>-greedy</h4>
<ul>
<li>Unlike <span class="math inline">\(\epsilon\)</span>-greedy, which uses a random chance for exploration, optimistic initialization drives exploration until the agent converges on accurate value estimates.</li>
<li>It is especially useful when the reward distribution is unknown but expected to have some higher values.</li>
</ul>
</section>
</section>
<section id="final-remarks" class="level3">
<h3 class="anchored" data-anchor-id="final-remarks">Final Remarks:</h3>
<p>To determine which algorithm is most effective in practice:</p>
<ul>
<li>A <strong>parameter study</strong> is essential, as highlighted in the passage. This involves varying parameters (like <span class="math inline">\(\epsilon\)</span>, <span class="math inline">\(c\)</span>, or <span class="math inline">\(\alpha\)</span>) to find the optimal range for each algorithm.</li>
<li><strong>Learning curves</strong> provide insight into how each algorithm performs over time. Averaging these curves over several runs ensures statistical reliability.</li>
</ul>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>example_2_6_summary.py</strong></pre>
</div>
<div class="sourceCode" id="cb8" data-filename="example_2_6_summary.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="im">from</span> collections <span class="im">import</span> namedtuple</span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="im">import</span> tqdm</span>
<span id="cb8-5"><a href="#cb8-5"></a><span class="im">import</span> pickle</span>
<span id="cb8-6"><a href="#cb8-6"></a></span>
<span id="cb8-7"><a href="#cb8-7"></a><span class="im">from</span> example_2_2_bandits_algo <span class="im">import</span> run_bandit <span class="im">as</span> e_greedy</span>
<span id="cb8-8"><a href="#cb8-8"></a><span class="im">from</span> example_2_3_OIV <span class="im">import</span> run_bandit <span class="im">as</span> OIV</span>
<span id="cb8-9"><a href="#cb8-9"></a><span class="im">from</span> example_2_4_UCB <span class="im">import</span> run_bandit_UCB <span class="im">as</span> UCB</span>
<span id="cb8-10"><a href="#cb8-10"></a><span class="im">from</span> example_2_5_gradient <span class="im">import</span> run_bandit <span class="im">as</span> gradient</span>
<span id="cb8-11"><a href="#cb8-11"></a></span>
<span id="cb8-12"><a href="#cb8-12"></a>SEED <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb8-13"><a href="#cb8-13"></a>np.random.seed(SEED)</span>
<span id="cb8-14"><a href="#cb8-14"></a></span>
<span id="cb8-15"><a href="#cb8-15"></a></span>
<span id="cb8-16"><a href="#cb8-16"></a><span class="co"># A wrapper function for running different algorithms</span></span>
<span id="cb8-17"><a href="#cb8-17"></a><span class="kw">def</span> run_algorithm(</span>
<span id="cb8-18"><a href="#cb8-18"></a>        fn_name: <span class="bu">str</span>,</span>
<span id="cb8-19"><a href="#cb8-19"></a>        fn: <span class="st">'function'</span>,</span>
<span id="cb8-20"><a href="#cb8-20"></a>        params: np.ndarray,</span>
<span id="cb8-21"><a href="#cb8-21"></a>        args: <span class="bu">dict</span>,</span>
<span id="cb8-22"><a href="#cb8-22"></a>        total_rounds: <span class="bu">int</span></span>
<span id="cb8-23"><a href="#cb8-23"></a>        ) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb8-24"><a href="#cb8-24"></a>    <span class="kw">global</span> hyper_param</span>
<span id="cb8-25"><a href="#cb8-25"></a>    <span class="cf">if</span> fn_name <span class="op">==</span> <span class="st">'e_greedy'</span>:</span>
<span id="cb8-26"><a href="#cb8-26"></a>        hyper_param <span class="op">=</span> <span class="st">'epsilon'</span></span>
<span id="cb8-27"><a href="#cb8-27"></a>    <span class="cf">elif</span> fn_name <span class="op">==</span> <span class="st">'ucb'</span>:</span>
<span id="cb8-28"><a href="#cb8-28"></a>        hyper_param <span class="op">=</span> <span class="st">'c'</span></span>
<span id="cb8-29"><a href="#cb8-29"></a>    <span class="cf">elif</span> fn_name <span class="op">==</span> <span class="st">'gradient'</span>:</span>
<span id="cb8-30"><a href="#cb8-30"></a>        hyper_param <span class="op">=</span> <span class="st">'alpha'</span></span>
<span id="cb8-31"><a href="#cb8-31"></a>    <span class="cf">elif</span> fn_name <span class="op">==</span> <span class="st">'oiv'</span>:</span>
<span id="cb8-32"><a href="#cb8-32"></a>        hyper_param <span class="op">=</span> <span class="st">'init_val'</span></span>
<span id="cb8-33"><a href="#cb8-33"></a>    </span>
<span id="cb8-34"><a href="#cb8-34"></a>    args[hyper_param] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb8-35"><a href="#cb8-35"></a>    </span>
<span id="cb8-36"><a href="#cb8-36"></a>    rewards_hist <span class="op">=</span> np.zeros(</span>
<span id="cb8-37"><a href="#cb8-37"></a>        shape<span class="op">=</span>(<span class="bu">len</span>(params), total_rounds, args[<span class="st">'num_steps'</span>])</span>
<span id="cb8-38"><a href="#cb8-38"></a>        )</span>
<span id="cb8-39"><a href="#cb8-39"></a>    optm_acts_hist <span class="op">=</span> np.zeros_like(rewards_hist)</span>
<span id="cb8-40"><a href="#cb8-40"></a>    <span class="cf">for</span> i, param <span class="kw">in</span> tqdm.tqdm(<span class="bu">enumerate</span>(params), desc<span class="op">=</span>fn_name, position<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb8-41"><a href="#cb8-41"></a>        args[hyper_param] <span class="op">=</span> param</span>
<span id="cb8-42"><a href="#cb8-42"></a>        <span class="cf">for</span> curr_round <span class="kw">in</span> tqdm.tqdm(</span>
<span id="cb8-43"><a href="#cb8-43"></a>                <span class="bu">range</span>(total_rounds),</span>
<span id="cb8-44"><a href="#cb8-44"></a>                desc<span class="op">=</span><span class="ss">f'</span><span class="sc">{</span>fn_name<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>param<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb8-45"><a href="#cb8-45"></a>                position<span class="op">=</span>i<span class="op">+</span><span class="dv">1</span>,</span>
<span id="cb8-46"><a href="#cb8-46"></a>                leave<span class="op">=</span><span class="va">True</span></span>
<span id="cb8-47"><a href="#cb8-47"></a>                ):</span>
<span id="cb8-48"><a href="#cb8-48"></a>            fn(</span>
<span id="cb8-49"><a href="#cb8-49"></a>                <span class="op">**</span>args,</span>
<span id="cb8-50"><a href="#cb8-50"></a>                rewards<span class="op">=</span>rewards_hist[i, curr_round],</span>
<span id="cb8-51"><a href="#cb8-51"></a>                optim_acts_ratio<span class="op">=</span>optm_acts_hist[i, curr_round]</span>
<span id="cb8-52"><a href="#cb8-52"></a>                )</span>
<span id="cb8-53"><a href="#cb8-53"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb8-54"><a href="#cb8-54"></a>    <span class="cf">return</span> rewards_hist.mean(axis<span class="op">=</span><span class="dv">1</span>).mean(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb8-55"><a href="#cb8-55"></a></span>
<span id="cb8-56"><a href="#cb8-56"></a></span>
<span id="cb8-57"><a href="#cb8-57"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb8-58"><a href="#cb8-58"></a>    K <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb8-59"><a href="#cb8-59"></a>    num_steps <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb8-60"><a href="#cb8-60"></a>    total_rounds <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb8-61"><a href="#cb8-61"></a>    q_star <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="fl">1.0</span>, size<span class="op">=</span>K)</span>
<span id="cb8-62"><a href="#cb8-62"></a>    </span>
<span id="cb8-63"><a href="#cb8-63"></a>    <span class="co"># Creating parameter array: [1/128, 1/64, 1/32, 1/16, ...]</span></span>
<span id="cb8-64"><a href="#cb8-64"></a>    multiplier <span class="op">=</span> np.exp2(np.arange(<span class="dv">10</span>))</span>
<span id="cb8-65"><a href="#cb8-65"></a>    params <span class="op">=</span> np.ones(<span class="dv">10</span>) <span class="op">*</span> (<span class="dv">1</span> <span class="op">/</span> <span class="dv">128</span>)</span>
<span id="cb8-66"><a href="#cb8-66"></a>    params <span class="op">*=</span> multiplier</span>
<span id="cb8-67"><a href="#cb8-67"></a>    x_labels <span class="op">=</span> [<span class="st">'1/128'</span>, <span class="st">'1/64'</span>, <span class="st">'1/32'</span>, <span class="st">'1/16'</span>, <span class="st">'1/8'</span>, <span class="st">'1/4'</span>, <span class="st">'1/2'</span>, <span class="st">'1'</span>, <span class="st">'2'</span>,</span>
<span id="cb8-68"><a href="#cb8-68"></a>                <span class="st">'4'</span>]</span>
<span id="cb8-69"><a href="#cb8-69"></a>    </span>
<span id="cb8-70"><a href="#cb8-70"></a>    <span class="co"># Creating a dict to record running histories</span></span>
<span id="cb8-71"><a href="#cb8-71"></a>    records <span class="op">=</span> {</span>
<span id="cb8-72"><a href="#cb8-72"></a>            <span class="st">'params'</span>: params,</span>
<span id="cb8-73"><a href="#cb8-73"></a>            <span class="st">'x_labels'</span>: x_labels</span>
<span id="cb8-74"><a href="#cb8-74"></a>            }</span>
<span id="cb8-75"><a href="#cb8-75"></a>    history <span class="op">=</span> namedtuple(<span class="st">'history'</span>, [<span class="st">'bounds'</span>, <span class="st">'data'</span>])</span>
<span id="cb8-76"><a href="#cb8-76"></a>    </span>
<span id="cb8-77"><a href="#cb8-77"></a>    base_args <span class="op">=</span> {</span>
<span id="cb8-78"><a href="#cb8-78"></a>            <span class="st">'K'</span>: K,</span>
<span id="cb8-79"><a href="#cb8-79"></a>            <span class="st">'q_star'</span>: q_star,</span>
<span id="cb8-80"><a href="#cb8-80"></a>            <span class="st">'num_steps'</span>: num_steps</span>
<span id="cb8-81"><a href="#cb8-81"></a>            }</span>
<span id="cb8-82"><a href="#cb8-82"></a>    </span>
<span id="cb8-83"><a href="#cb8-83"></a>    <span class="co"># ======== e_greedy ========</span></span>
<span id="cb8-84"><a href="#cb8-84"></a>    eps_bounds <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">6</span>]</span>
<span id="cb8-85"><a href="#cb8-85"></a>    fn_params <span class="op">=</span> params[eps_bounds[<span class="dv">0</span>]: eps_bounds[<span class="dv">1</span>]]</span>
<span id="cb8-86"><a href="#cb8-86"></a>    </span>
<span id="cb8-87"><a href="#cb8-87"></a>    eps_rewards <span class="op">=</span> run_algorithm(</span>
<span id="cb8-88"><a href="#cb8-88"></a>        <span class="st">'e_greedy'</span>, e_greedy, fn_params, base_args.copy(), total_rounds</span>
<span id="cb8-89"><a href="#cb8-89"></a>        )</span>
<span id="cb8-90"><a href="#cb8-90"></a>    records[<span class="st">'e_greedy'</span>] <span class="op">=</span> history(eps_bounds, eps_rewards)</span>
<span id="cb8-91"><a href="#cb8-91"></a>    </span>
<span id="cb8-92"><a href="#cb8-92"></a>    <span class="co"># ======== UCB ========</span></span>
<span id="cb8-93"><a href="#cb8-93"></a>    ucb_bounds <span class="op">=</span> [<span class="dv">3</span>, <span class="dv">10</span>]</span>
<span id="cb8-94"><a href="#cb8-94"></a>    fn_params <span class="op">=</span> params[ucb_bounds[<span class="dv">0</span>]: ucb_bounds[<span class="dv">1</span>]]</span>
<span id="cb8-95"><a href="#cb8-95"></a>    </span>
<span id="cb8-96"><a href="#cb8-96"></a>    ucb_rewards <span class="op">=</span> run_algorithm(</span>
<span id="cb8-97"><a href="#cb8-97"></a>        <span class="st">'ucb'</span>, UCB, fn_params, base_args.copy(), total_rounds</span>
<span id="cb8-98"><a href="#cb8-98"></a>        )</span>
<span id="cb8-99"><a href="#cb8-99"></a>    records[<span class="st">'ucb'</span>] <span class="op">=</span> history(ucb_bounds, ucb_rewards)</span>
<span id="cb8-100"><a href="#cb8-100"></a>    </span>
<span id="cb8-101"><a href="#cb8-101"></a>    <span class="co"># ======== Gradient ========</span></span>
<span id="cb8-102"><a href="#cb8-102"></a>    gd_bounds <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">9</span>]</span>
<span id="cb8-103"><a href="#cb8-103"></a>    fn_params <span class="op">=</span> params[gd_bounds[<span class="dv">0</span>]:gd_bounds[<span class="dv">1</span>]]</span>
<span id="cb8-104"><a href="#cb8-104"></a>    gd_args <span class="op">=</span> base_args.copy()</span>
<span id="cb8-105"><a href="#cb8-105"></a>    gd_args[<span class="st">'baseline'</span>] <span class="op">=</span> <span class="va">True</span></span>
<span id="cb8-106"><a href="#cb8-106"></a>    </span>
<span id="cb8-107"><a href="#cb8-107"></a>    gd_rewards <span class="op">=</span> run_algorithm(</span>
<span id="cb8-108"><a href="#cb8-108"></a>        <span class="st">'gradient'</span>, gradient, fn_params, gd_args, total_rounds</span>
<span id="cb8-109"><a href="#cb8-109"></a>        )</span>
<span id="cb8-110"><a href="#cb8-110"></a>    records[<span class="st">'gradient'</span>] <span class="op">=</span> history(gd_bounds, gd_rewards)</span>
<span id="cb8-111"><a href="#cb8-111"></a>    </span>
<span id="cb8-112"><a href="#cb8-112"></a>    <span class="co"># ======== OIV ========</span></span>
<span id="cb8-113"><a href="#cb8-113"></a>    oiv_bounds <span class="op">=</span> [<span class="dv">5</span>, <span class="dv">10</span>]</span>
<span id="cb8-114"><a href="#cb8-114"></a>    fn_params <span class="op">=</span> params[oiv_bounds[<span class="dv">0</span>]:oiv_bounds[<span class="dv">1</span>]]</span>
<span id="cb8-115"><a href="#cb8-115"></a>    oiv_args <span class="op">=</span> base_args.copy()</span>
<span id="cb8-116"><a href="#cb8-116"></a>    oiv_args[<span class="st">'epsilon'</span>] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb8-117"><a href="#cb8-117"></a>    oiv_rewards <span class="op">=</span> run_algorithm(<span class="st">'oiv'</span>, OIV, fn_params, oiv_args, total_rounds)</span>
<span id="cb8-118"><a href="#cb8-118"></a>    records[<span class="st">'oiv'</span>] <span class="op">=</span> history(oiv_bounds, oiv_rewards)</span>
<span id="cb8-119"><a href="#cb8-119"></a>    </span>
<span id="cb8-120"><a href="#cb8-120"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">'./history/summary.pkl'</span>, <span class="st">'wb'</span>) <span class="im">as</span> f:</span>
<span id="cb8-121"><a href="#cb8-121"></a>        pickle.dump(records, f)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="visualization" class="level2">
<h2 class="anchored" data-anchor-id="visualization">Visualization</h2>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>plot_sumary.py</strong></pre>
</div>
<div class="sourceCode" id="cb9" data-filename="plot_sumary.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="im">import</span> pickle</span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-4"><a href="#cb9-4"></a></span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="im">from</span> collections <span class="im">import</span> namedtuple</span>
<span id="cb9-6"><a href="#cb9-6"></a></span>
<span id="cb9-7"><a href="#cb9-7"></a>history <span class="op">=</span> namedtuple(<span class="st">'history'</span>, [<span class="st">'bounds'</span>, <span class="st">'data'</span>])</span>
<span id="cb9-8"><a href="#cb9-8"></a>algos <span class="op">=</span> [<span class="st">'e_greedy'</span>, <span class="st">'gradient'</span>, <span class="st">'ucb'</span>, <span class="st">'oiv'</span>]</span>
<span id="cb9-9"><a href="#cb9-9"></a></span>
<span id="cb9-10"><a href="#cb9-10"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">'__main__'</span>:</span>
<span id="cb9-11"><a href="#cb9-11"></a>    </span>
<span id="cb9-12"><a href="#cb9-12"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">'./history/summary.pkl'</span>, <span class="st">'rb'</span>) <span class="im">as</span> f:</span>
<span id="cb9-13"><a href="#cb9-13"></a>        histories <span class="op">=</span> pickle.load(f)</span>
<span id="cb9-14"><a href="#cb9-14"></a>        coords <span class="op">=</span> [[<span class="fl">0.95</span>, <span class="fl">1.55</span>], [<span class="fl">6.5</span>, <span class="fl">1.45</span>], [<span class="dv">3</span>, <span class="fl">1.82</span>], [<span class="fl">8.5</span>, <span class="fl">1.82</span>]]</span>
<span id="cb9-15"><a href="#cb9-15"></a>        legend_loc <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb9-16"><a href="#cb9-16"></a>        filename <span class="op">=</span> <span class="st">'./plots/example_2_6_summary.png'</span></span>
<span id="cb9-17"><a href="#cb9-17"></a>    </span>
<span id="cb9-18"><a href="#cb9-18"></a>    <span class="co"># with open('./history/exercise_2_6.pkl', 'rb') as f:</span></span>
<span id="cb9-19"><a href="#cb9-19"></a>    <span class="co">#     histories = pickle.load(f)</span></span>
<span id="cb9-20"><a href="#cb9-20"></a>    <span class="co">#     coords = [[2.5, 6.0], [7.0, 3.5], [7.5, 5.0], [6.5, 5.7]]</span></span>
<span id="cb9-21"><a href="#cb9-21"></a>    <span class="co">#     legend_loc = 0</span></span>
<span id="cb9-22"><a href="#cb9-22"></a>    <span class="co">#     filename = './plots/exercise_2_6.png'</span></span>
<span id="cb9-23"><a href="#cb9-23"></a>    </span>
<span id="cb9-24"><a href="#cb9-24"></a>    x_ticks <span class="op">=</span> histories[<span class="st">'x_labels'</span>]</span>
<span id="cb9-25"><a href="#cb9-25"></a>    </span>
<span id="cb9-26"><a href="#cb9-26"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>), dpi<span class="op">=</span><span class="dv">150</span>)</span>
<span id="cb9-27"><a href="#cb9-27"></a>    plt.grid(c<span class="op">=</span><span class="st">'lightgray'</span>)</span>
<span id="cb9-28"><a href="#cb9-28"></a>    plt.margins(<span class="fl">0.02</span>)</span>
<span id="cb9-29"><a href="#cb9-29"></a>    </span>
<span id="cb9-30"><a href="#cb9-30"></a>    fontdict <span class="op">=</span> {</span>
<span id="cb9-31"><a href="#cb9-31"></a>            <span class="st">'fontsize'</span>: <span class="dv">12</span>,</span>
<span id="cb9-32"><a href="#cb9-32"></a>            <span class="st">'fontweight'</span>: <span class="st">'bold'</span>,</span>
<span id="cb9-33"><a href="#cb9-33"></a>            }</span>
<span id="cb9-34"><a href="#cb9-34"></a>    </span>
<span id="cb9-35"><a href="#cb9-35"></a>    legends <span class="op">=</span> [<span class="st">'$\epsilon$'</span>, <span class="st">'$</span><span class="ch">\\</span><span class="st">alpha$'</span>, <span class="st">'$c$'</span>, <span class="st">'$Q_0$'</span>]</span>
<span id="cb9-36"><a href="#cb9-36"></a>    colors <span class="op">=</span> [<span class="st">'tomato'</span>, <span class="st">'mediumseagreen'</span>, <span class="st">'steelblue'</span>, <span class="st">'orchid'</span>]</span>
<span id="cb9-37"><a href="#cb9-37"></a>    </span>
<span id="cb9-38"><a href="#cb9-38"></a>    <span class="cf">for</span> i, key <span class="kw">in</span> <span class="bu">enumerate</span>(algos):</span>
<span id="cb9-39"><a href="#cb9-39"></a>        record <span class="op">=</span> histories[key]</span>
<span id="cb9-40"><a href="#cb9-40"></a>        bounds <span class="op">=</span> record.bounds</span>
<span id="cb9-41"><a href="#cb9-41"></a>        data <span class="op">=</span> record.data</span>
<span id="cb9-42"><a href="#cb9-42"></a>        </span>
<span id="cb9-43"><a href="#cb9-43"></a>        plt.plot(</span>
<span id="cb9-44"><a href="#cb9-44"></a>            np.arange(bounds[<span class="dv">0</span>], bounds[<span class="dv">1</span>]), data, label<span class="op">=</span>legends[i], c<span class="op">=</span>colors[i]</span>
<span id="cb9-45"><a href="#cb9-45"></a>            )</span>
<span id="cb9-46"><a href="#cb9-46"></a>    </span>
<span id="cb9-47"><a href="#cb9-47"></a>    <span class="cf">for</span> i, spine <span class="kw">in</span> <span class="bu">enumerate</span>(plt.gca().spines.values()):</span>
<span id="cb9-48"><a href="#cb9-48"></a>        <span class="cf">if</span> i <span class="kw">in</span> [<span class="dv">0</span>, <span class="dv">2</span>]:</span>
<span id="cb9-49"><a href="#cb9-49"></a>            spine.set_linewidth(<span class="fl">1.5</span>)</span>
<span id="cb9-50"><a href="#cb9-50"></a>            <span class="cf">continue</span></span>
<span id="cb9-51"><a href="#cb9-51"></a>        spine.set_visible(<span class="va">False</span>)</span>
<span id="cb9-52"><a href="#cb9-52"></a>    </span>
<span id="cb9-53"><a href="#cb9-53"></a>    plt.tick_params(axis<span class="op">=</span><span class="st">'both'</span>, labelsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb9-54"><a href="#cb9-54"></a>    plt.xticks(np.arange(<span class="dv">10</span>), x_ticks)</span>
<span id="cb9-55"><a href="#cb9-55"></a>    </span>
<span id="cb9-56"><a href="#cb9-56"></a>    <span class="co"># x labels</span></span>
<span id="cb9-57"><a href="#cb9-57"></a>    plt.legend(loc<span class="op">=</span>legend_loc, fontsize<span class="op">=</span><span class="dv">12</span>, title<span class="op">=</span><span class="st">'Hyper Param.'</span>)</span>
<span id="cb9-58"><a href="#cb9-58"></a>    plt.xlabel(<span class="st">'Hyper parameter value'</span>, fontdict<span class="op">=</span>fontdict)</span>
<span id="cb9-59"><a href="#cb9-59"></a>    plt.ylabel(</span>
<span id="cb9-60"><a href="#cb9-60"></a>        <span class="st">'Average reward over first 1000 steps'</span>,</span>
<span id="cb9-61"><a href="#cb9-61"></a>        fontdict<span class="op">=</span>fontdict</span>
<span id="cb9-62"><a href="#cb9-62"></a>        )</span>
<span id="cb9-63"><a href="#cb9-63"></a>    </span>
<span id="cb9-64"><a href="#cb9-64"></a>    plt.text(<span class="op">*</span>coords[<span class="dv">0</span>], <span class="st">'$\epsilon$-greedy'</span>, c<span class="op">=</span>colors[<span class="dv">0</span>], fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb9-65"><a href="#cb9-65"></a>    plt.text(</span>
<span id="cb9-66"><a href="#cb9-66"></a>        <span class="op">*</span>coords[<span class="dv">1</span>], <span class="st">'gradient</span><span class="ch">\n</span><span class="st">bandit'</span>, c<span class="op">=</span>colors[<span class="dv">1</span>], fontsize<span class="op">=</span><span class="dv">12</span>,</span>
<span id="cb9-67"><a href="#cb9-67"></a>        horizontalalignment<span class="op">=</span><span class="st">'center'</span></span>
<span id="cb9-68"><a href="#cb9-68"></a>        )</span>
<span id="cb9-69"><a href="#cb9-69"></a>    plt.text(<span class="op">*</span>coords[<span class="dv">2</span>], <span class="st">'UCB'</span>, c<span class="op">=</span>colors[<span class="dv">2</span>], fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb9-70"><a href="#cb9-70"></a>    plt.text(</span>
<span id="cb9-71"><a href="#cb9-71"></a>        <span class="op">*</span>coords[<span class="dv">3</span>], <span class="st">'greedy with</span><span class="ch">\n</span><span class="st">optimistic</span><span class="ch">\n</span><span class="st">initializatio</span><span class="ch">\n</span><span class="st">$</span><span class="ch">\\</span><span class="st">alpha=0.1$'</span>,</span>
<span id="cb9-72"><a href="#cb9-72"></a>        c<span class="op">=</span>colors[<span class="dv">3</span>], fontsize<span class="op">=</span><span class="dv">12</span>, horizontalalignment<span class="op">=</span><span class="st">'center'</span></span>
<span id="cb9-73"><a href="#cb9-73"></a>        )</span>
<span id="cb9-74"><a href="#cb9-74"></a>    </span>
<span id="cb9-75"><a href="#cb9-75"></a>    <span class="co"># plt.show()</span></span>
<span id="cb9-76"><a href="#cb9-76"></a>    plt.savefig(filename)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


<!-- -->


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/sauldiazinfante\.github\.io\/RL-Course-2024-2\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../02-introductionToRL/intro.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../04-finiteMDPs/mdp.html" class="pagination-link" aria-label="Finite Markov Decision Processes (MDPs)">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Finite Markov Decision Processes (MDPs)</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb10" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb10-1"><a href="#cb10-1"></a><span class="co">---</span></span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="an">title:</span><span class="co"> "Multi-armed Bandits"</span></span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="an">author:</span><span class="co"> "Saúl Díaz Infante Velasco"</span></span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="an">format:</span></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="co">  html:</span></span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="co">    grid:</span></span>
<span id="cb10-7"><a href="#cb10-7"></a><span class="co">      margin-width: 350px</span></span>
<span id="cb10-8"><a href="#cb10-8"></a><span class="co">  pdf: default</span></span>
<span id="cb10-9"><a href="#cb10-9"></a><span class="an">reference-location:</span><span class="co"> margin</span></span>
<span id="cb10-10"><a href="#cb10-10"></a><span class="an">citation-location:</span><span class="co"> margin</span></span>
<span id="cb10-11"><a href="#cb10-11"></a><span class="co">---</span></span>
<span id="cb10-12"><a href="#cb10-12"></a></span>
<span id="cb10-13"><a href="#cb10-13"></a><span class="fu"># Multi-armed Bandits</span></span>
<span id="cb10-14"><a href="#cb10-14"></a></span>
<span id="cb10-15"><a href="#cb10-15"></a>A very important feature distinguishing reinforcement learning from other types</span>
<span id="cb10-16"><a href="#cb10-16"></a>of learning is that it uses training information to evaluate the actions taken,</span>
<span id="cb10-17"><a href="#cb10-17"></a>rather than instruct by giving correct actions.</span>
<span id="cb10-18"><a href="#cb10-18"></a></span>
<span id="cb10-19"><a href="#cb10-19"></a><span class="fu">## A $k$-armed Bandit Problem</span></span>
<span id="cb10-20"><a href="#cb10-20"></a></span>
<span id="cb10-21"><a href="#cb10-21"></a>We consider the following setup:</span>
<span id="cb10-22"><a href="#cb10-22"></a></span>
<span id="cb10-23"><a href="#cb10-23"></a><span class="ss">-   </span>You repeatedly face a choice among $k$ different options or actions.</span>
<span id="cb10-24"><a href="#cb10-24"></a><span class="ss">-   </span>After a choice, you receive a numerical reward chosen from a stationary</span>
<span id="cb10-25"><a href="#cb10-25"></a>    probability distribution that depends on the action you selected</span>
<span id="cb10-26"><a href="#cb10-26"></a><span class="ss">-   </span>Your goal is to maximize the total expected reward over a specific time</span>
<span id="cb10-27"><a href="#cb10-27"></a>    period, such as 1000 action selections or time steps. The problem is named</span>
<span id="cb10-28"><a href="#cb10-28"></a>    by analogy to a slot machine, or <span class="in">`one-armed bandit`</span>, except that it has $k$</span>
<span id="cb10-29"><a href="#cb10-29"></a>    levers instead of one.</span>
<span id="cb10-30"><a href="#cb10-30"></a></span>
<span id="cb10-31"><a href="#cb10-31"></a>We denote the action selected on time step $t$ as $A_t$ and the corresponding</span>
<span id="cb10-32"><a href="#cb10-32"></a>reward as $R_t$. Each of the $k$ actions has an expected or mean reward given</span>
<span id="cb10-33"><a href="#cb10-33"></a>that that action is selected; let us call this the value of that action.</span>
<span id="cb10-34"><a href="#cb10-34"></a></span>
<span id="cb10-35"><a href="#cb10-35"></a>The value then of an arbitrary action $a$, denoted $q_{*} (a)$, is the expected</span>
<span id="cb10-36"><a href="#cb10-36"></a>reward given that $a$ is selected:</span>
<span id="cb10-37"><a href="#cb10-37"></a></span>
<span id="cb10-38"><a href="#cb10-38"></a>$$</span>
<span id="cb10-39"><a href="#cb10-39"></a>    q_{*}(a): = \mathbb{E} \left<span class="co">[</span><span class="ot"> R_t | A_t =a\right</span><span class="co">]</span>.</span>
<span id="cb10-40"><a href="#cb10-40"></a>$$</span>
<span id="cb10-41"><a href="#cb10-41"></a></span>
<span id="cb10-42"><a href="#cb10-42"></a>If you knew the value of each action, then we solve the $k$-armed bandit</span>
<span id="cb10-43"><a href="#cb10-43"></a>problem---you would always <span class="in">`select the action with highest value`</span>.</span>
<span id="cb10-44"><a href="#cb10-44"></a></span>
<span id="cb10-45"><a href="#cb10-45"></a>We assume that you may not have precise knowledge of the action values, although</span>
<span id="cb10-46"><a href="#cb10-46"></a>you may have some estimates. We denote this estimated value of action $a$ at</span>
<span id="cb10-47"><a href="#cb10-47"></a>time step $t$ as $Q_t(a)$. Thus, we would like that $$</span>
<span id="cb10-48"><a href="#cb10-48"></a>    Q_t(a) \approx q_{*}(a).</span>
<span id="cb10-49"><a href="#cb10-49"></a>$$</span>
<span id="cb10-50"><a href="#cb10-50"></a></span>
<span id="cb10-51"><a href="#cb10-51"></a>If you maintain estimates of the action values, then at any time step there is</span>
<span id="cb10-52"><a href="#cb10-52"></a>at least one action whose estimated value is greatest. We call these the</span>
<span id="cb10-53"><a href="#cb10-53"></a>*greedy* actions. When you select one of these actions, we say that you are</span>
<span id="cb10-54"><a href="#cb10-54"></a>*exploiting* your current knowledge of the values of the actions. If instead you</span>
<span id="cb10-55"><a href="#cb10-55"></a>select one of the non-greedy actions, then we say you are exploring, because</span>
<span id="cb10-56"><a href="#cb10-56"></a>this enables you to improve your estimate of the non-greedy action’s value.</span>
<span id="cb10-57"><a href="#cb10-57"></a></span>
<span id="cb10-58"><a href="#cb10-58"></a>Exploitation is the right thing to do to maximize the expected reward on the one</span>
<span id="cb10-59"><a href="#cb10-59"></a>step, but exploration may produce the greater total reward in the long run.</span>
<span id="cb10-60"><a href="#cb10-60"></a></span>
<span id="cb10-61"><a href="#cb10-61"></a>Reward is lower in the short run, during exploration, but higher in the long run</span>
<span id="cb10-62"><a href="#cb10-62"></a>because after you have discovered the better actions, you can exploit them many</span>
<span id="cb10-63"><a href="#cb10-63"></a>times. Because it is not possible both to explore and to exploit with any single</span>
<span id="cb10-64"><a href="#cb10-64"></a>action selection, one often refers to the “conflict” between exploration and</span>
<span id="cb10-65"><a href="#cb10-65"></a>exploitation.</span>
<span id="cb10-66"><a href="#cb10-66"></a></span>
<span id="cb10-67"><a href="#cb10-67"></a>In any specific case, whether it is better to explore or exploit depends in a</span>
<span id="cb10-68"><a href="#cb10-68"></a>complex way on the precise values of the estimates, uncertainties, and the</span>
<span id="cb10-69"><a href="#cb10-69"></a>number of remaining steps. There are many sophisticated methods for balancing</span>
<span id="cb10-70"><a href="#cb10-70"></a>exploration and exploitation for particular mathematical formulations of the</span>
<span id="cb10-71"><a href="#cb10-71"></a>$k$-armed bandit and related problems.</span>
<span id="cb10-72"><a href="#cb10-72"></a></span>
<span id="cb10-73"><a href="#cb10-73"></a>However, most of these methods make strong assumptions about stationary and</span>
<span id="cb10-74"><a href="#cb10-74"></a>prior knowledge that are either violated or impossible to verify in most</span>
<span id="cb10-75"><a href="#cb10-75"></a>applications.</span>
<span id="cb10-76"><a href="#cb10-76"></a></span>
<span id="cb10-77"><a href="#cb10-77"></a>The guarantees of optimality or bounded loss for these methods offer little</span>
<span id="cb10-78"><a href="#cb10-78"></a>comfort when the assumptions of their theory do not apply.</span>
<span id="cb10-79"><a href="#cb10-79"></a></span>
<span id="cb10-80"><a href="#cb10-80"></a><span class="fu">## Action-value Methods</span></span>
<span id="cb10-81"><a href="#cb10-81"></a></span>
<span id="cb10-82"><a href="#cb10-82"></a>One natural way to estimate the value of a given action is by averaging the</span>
<span id="cb10-83"><a href="#cb10-83"></a>rewards actually received. In mathematical symbols reads</span>
<span id="cb10-84"><a href="#cb10-84"></a></span>
<span id="cb10-85"><a href="#cb10-85"></a>$$</span>
<span id="cb10-86"><a href="#cb10-86"></a>  Q_t(a):=</span>
<span id="cb10-87"><a href="#cb10-87"></a>    \dfrac{</span>
<span id="cb10-88"><a href="#cb10-88"></a>      \sum_{i=1}^{t-1}</span>
<span id="cb10-89"><a href="#cb10-89"></a>        R_i \cdot \mathbb{1}_{A_{i} = a}</span>
<span id="cb10-90"><a href="#cb10-90"></a>    }{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}} .</span>
<span id="cb10-91"><a href="#cb10-91"></a>$$ {#eq-action_avering_kbandit}</span>
<span id="cb10-92"><a href="#cb10-92"></a></span>
<span id="cb10-93"><a href="#cb10-93"></a>Next we understand as greedy action as the action that results from $$</span>
<span id="cb10-94"><a href="#cb10-94"></a>  A_t := \underset{a}{\mathrm{argmax}} \ Q_t(a).</span>
<span id="cb10-95"><a href="#cb10-95"></a>$$ {#eq-greedy_action}</span>
<span id="cb10-96"><a href="#cb10-96"></a></span>
<span id="cb10-97"><a href="#cb10-97"></a>Greedy action selection always exploits current knowledge to maximize immediate</span>
<span id="cb10-98"><a href="#cb10-98"></a>reward. It also only spends time sampling apparently superior actions. A simple</span>
<span id="cb10-99"><a href="#cb10-99"></a>alternative is to behave greedily but occasionally, with a small</span>
<span id="cb10-100"><a href="#cb10-100"></a>$\epsilon$-probability, select randomly from all the actions with equal</span>
<span id="cb10-101"><a href="#cb10-101"></a>probability, regardless of the action-value estimates. We call methods using</span>
<span id="cb10-102"><a href="#cb10-102"></a>this near-greedy action selection rule $\epsilon$-greedy methods.</span>
<span id="cb10-103"><a href="#cb10-103"></a></span>
<span id="cb10-104"><a href="#cb10-104"></a><span class="fu">## The 10-armed Testbed</span></span>
<span id="cb10-105"><a href="#cb10-105"></a></span>
<span id="cb10-106"><a href="#cb10-106"></a>To evaluate the relative effectiveness of the greedy and $\epsilon$-greedy</span>
<span id="cb10-107"><a href="#cb10-107"></a>action-value methods, we compared them numerically on a suite of test problems.</span>
<span id="cb10-108"><a href="#cb10-108"></a></span>
<span id="cb10-109"><a href="#cb10-109"></a><span class="fu">### Set up</span></span>
<span id="cb10-110"><a href="#cb10-110"></a></span>
<span id="cb10-111"><a href="#cb10-111"></a>::: callout-tip</span>
<span id="cb10-112"><a href="#cb10-112"></a><span class="fu">## The experiment runs as follows.</span></span>
<span id="cb10-113"><a href="#cb10-113"></a></span>
<span id="cb10-114"><a href="#cb10-114"></a><span class="ss">-   </span>Consider a $k$-bandit problem with $k=10$</span>
<span id="cb10-115"><a href="#cb10-115"></a></span>
<span id="cb10-116"><a href="#cb10-116"></a><span class="ss">-   </span>For each bandit problem, the action values</span>
<span id="cb10-117"><a href="#cb10-117"></a></span>
<span id="cb10-118"><a href="#cb10-118"></a>$$</span>
<span id="cb10-119"><a href="#cb10-119"></a>  q_{*}(a) \sim \mathcal{N}(0,1)</span>
<span id="cb10-120"><a href="#cb10-120"></a>$$</span>
<span id="cb10-121"><a href="#cb10-121"></a></span>
<span id="cb10-122"><a href="#cb10-122"></a><span class="ss">-   </span>Then when choosing an action $A_t$ the corresponding reward $R_t$ is</span>
<span id="cb10-123"><a href="#cb10-123"></a>    sampling from a Gaussian distribution $$</span>
<span id="cb10-124"><a href="#cb10-124"></a>    R_t \sim \mathcal{N}(q_{*}(A_t), 1)  </span>
<span id="cb10-125"><a href="#cb10-125"></a>    $$</span>
<span id="cb10-126"><a href="#cb10-126"></a>:::</span>
<span id="cb10-127"><a href="#cb10-127"></a></span>
<span id="cb10-128"><a href="#cb10-128"></a><span class="in">```{.python filename="k_armed_testbed.py"}</span></span>
<span id="cb10-129"><a href="#cb10-129"></a><span class="in">#k_armed_testbed.py</span></span>
<span id="cb10-130"><a href="#cb10-130"></a><span class="in">import numpy as np</span></span>
<span id="cb10-131"><a href="#cb10-131"></a><span class="in">from matplotlib import pyplot as plt</span></span>
<span id="cb10-132"><a href="#cb10-132"></a></span>
<span id="cb10-133"><a href="#cb10-133"></a></span>
<span id="cb10-134"><a href="#cb10-134"></a><span class="in"># Randomly sample mean reward for each action</span></span>
<span id="cb10-135"><a href="#cb10-135"></a><span class="in">means = np.random.normal(size=(10, ))</span></span>
<span id="cb10-136"><a href="#cb10-136"></a></span>
<span id="cb10-137"><a href="#cb10-137"></a><span class="in"># Generate sample data based on normal distribution</span></span>
<span id="cb10-138"><a href="#cb10-138"></a><span class="in">data = [np.random.normal(mean, 1.0, 2000) for mean in means]</span></span>
<span id="cb10-139"><a href="#cb10-139"></a></span>
<span id="cb10-140"><a href="#cb10-140"></a><span class="in"># Create violin plot</span></span>
<span id="cb10-141"><a href="#cb10-141"></a><span class="in">plt.figure(figsize=(8, 6), dpi=150)</span></span>
<span id="cb10-142"><a href="#cb10-142"></a><span class="in">plt.violinplot(</span></span>
<span id="cb10-143"><a href="#cb10-143"></a><span class="in">  dataset=data,</span></span>
<span id="cb10-144"><a href="#cb10-144"></a><span class="in">  showextrema=False,</span></span>
<span id="cb10-145"><a href="#cb10-145"></a><span class="in">  showmeans=False,</span></span>
<span id="cb10-146"><a href="#cb10-146"></a><span class="in">  points=2000</span></span>
<span id="cb10-147"><a href="#cb10-147"></a><span class="in">)</span></span>
<span id="cb10-148"><a href="#cb10-148"></a></span>
<span id="cb10-149"><a href="#cb10-149"></a><span class="in"># Draw mean marks</span></span>
<span id="cb10-150"><a href="#cb10-150"></a><span class="in">for i, mean in enumerate(means):</span></span>
<span id="cb10-151"><a href="#cb10-151"></a><span class="in">    idx = i + 1</span></span>
<span id="cb10-152"><a href="#cb10-152"></a><span class="in">    plt.plot([idx - 0.3, idx + 0.3], [mean, mean],</span></span>
<span id="cb10-153"><a href="#cb10-153"></a><span class="in">             c='black',</span></span>
<span id="cb10-154"><a href="#cb10-154"></a><span class="in">             linewidth=1)</span></span>
<span id="cb10-155"><a href="#cb10-155"></a><span class="in">    plt.text(idx + 0.2, mean - 0.2, </span></span>
<span id="cb10-156"><a href="#cb10-156"></a><span class="in">             s=f"$q_*({idx})$",</span></span>
<span id="cb10-157"><a href="#cb10-157"></a><span class="in">             fontsize=8)</span></span>
<span id="cb10-158"><a href="#cb10-158"></a></span>
<span id="cb10-159"><a href="#cb10-159"></a><span class="in"># Draw 0-value dashed line</span></span>
<span id="cb10-160"><a href="#cb10-160"></a><span class="in">plt.plot(np.arange(0, 12), np.zeros(12), </span></span>
<span id="cb10-161"><a href="#cb10-161"></a><span class="in">            c='gray', </span></span>
<span id="cb10-162"><a href="#cb10-162"></a><span class="in">            linewidth=0.5,</span></span>
<span id="cb10-163"><a href="#cb10-163"></a><span class="in">            linestyle=(5, (20, 10)))</span></span>
<span id="cb10-164"><a href="#cb10-164"></a><span class="in">plt.tick_params(axis='both', labelsize=10)</span></span>
<span id="cb10-165"><a href="#cb10-165"></a><span class="in">plt.xticks(np.arange(1, 11))</span></span>
<span id="cb10-166"><a href="#cb10-166"></a></span>
<span id="cb10-167"><a href="#cb10-167"></a><span class="in"># get rid of the frame</span></span>
<span id="cb10-168"><a href="#cb10-168"></a><span class="in">for i, spine in enumerate(plt.gca().spines.values()):</span></span>
<span id="cb10-169"><a href="#cb10-169"></a><span class="in">    if i == 2: continue</span></span>
<span id="cb10-170"><a href="#cb10-170"></a><span class="in">    spine.set_visible(False)</span></span>
<span id="cb10-171"><a href="#cb10-171"></a><span class="in">    </span></span>
<span id="cb10-172"><a href="#cb10-172"></a></span>
<span id="cb10-173"><a href="#cb10-173"></a><span class="in"># Draw labels</span></span>
<span id="cb10-174"><a href="#cb10-174"></a><span class="in">label_font = {</span></span>
<span id="cb10-175"><a href="#cb10-175"></a><span class="in">    'fontsize': 12,</span></span>
<span id="cb10-176"><a href="#cb10-176"></a><span class="in">    'fontweight': 'bold'</span></span>
<span id="cb10-177"><a href="#cb10-177"></a><span class="in">}</span></span>
<span id="cb10-178"><a href="#cb10-178"></a></span>
<span id="cb10-179"><a href="#cb10-179"></a><span class="in">plt.xlabel('Action', fontdict=label_font)</span></span>
<span id="cb10-180"><a href="#cb10-180"></a><span class="in">plt.ylabel('Reward distribution', fontdict=label_font)</span></span>
<span id="cb10-181"><a href="#cb10-181"></a><span class="in">plt.margins(0)</span></span>
<span id="cb10-182"><a href="#cb10-182"></a></span>
<span id="cb10-183"><a href="#cb10-183"></a><span class="in">plt.tight_layout()</span></span>
<span id="cb10-184"><a href="#cb10-184"></a><span class="in">plt.show()</span></span>
<span id="cb10-185"><a href="#cb10-185"></a></span>
<span id="cb10-186"><a href="#cb10-186"></a><span class="in">```</span></span>
<span id="cb10-187"><a href="#cb10-187"></a></span>
<span id="cb10-188"><a href="#cb10-188"></a>We consider a set of 2000 randomly generated $k$-armed bandit problems with $k$</span>
<span id="cb10-189"><a href="#cb10-189"></a>= 10. For each bandit problem, such as the one shown in the output of the above</span>
<span id="cb10-190"><a href="#cb10-190"></a>code. The action values, $q_{*} (a), a = 1, . . . , 10$, were selected according</span>
<span id="cb10-191"><a href="#cb10-191"></a>to a normal (Gaussian) distribution with mean 0 and variance 1. Thus when we</span>
<span id="cb10-192"><a href="#cb10-192"></a>apply a learning method to this problem, the selected action $A_t$ a time step</span>
<span id="cb10-193"><a href="#cb10-193"></a>$t$ the regarding reward $R_t$ is sampling from a normal distribution $$</span>
<span id="cb10-194"><a href="#cb10-194"></a>  R_{t} \sim \mathcal{N}(q_{*}(A_t), 1).</span>
<span id="cb10-195"><a href="#cb10-195"></a>$$ Sutton and Barto <span class="co">[</span><span class="ot">@Sutton2018, p.28</span><span class="co">]</span> calls this suite of test tasks the</span>
<span id="cb10-196"><a href="#cb10-196"></a>10-armed test-bed. By using this suit of benchmarks, we can measure the</span>
<span id="cb10-197"><a href="#cb10-197"></a>performance of any learning method. In fact we also can observe its behavior</span>
<span id="cb10-198"><a href="#cb10-198"></a>while the learning improves with experience of 1000 time steps, when it is</span>
<span id="cb10-199"><a href="#cb10-199"></a>applied to a selected bandit of this bed. This makes up one run. Thus, if we</span>
<span id="cb10-200"><a href="#cb10-200"></a>**iterate 2000** independent runs, each with different bandit problem, we can</span>
<span id="cb10-201"><a href="#cb10-201"></a>obtain a measure of learning algorithm's average behavior.</span>
<span id="cb10-202"><a href="#cb10-202"></a></span>
<span id="cb10-203"><a href="#cb10-203"></a>Next we code functions to deploy the above experiment with $\epsilon$-greedy</span>
<span id="cb10-204"><a href="#cb10-204"></a>actions</span>
<span id="cb10-205"><a href="#cb10-205"></a></span>
<span id="cb10-206"><a href="#cb10-206"></a><span class="in">``` {.python filename="utils.py"}</span></span>
<span id="cb10-207"><a href="#cb10-207"></a><span class="in">from typing import Any</span></span>
<span id="cb10-208"><a href="#cb10-208"></a><span class="in">import matplotlib.pyplot as plt</span></span>
<span id="cb10-209"><a href="#cb10-209"></a><span class="in">import numpy as np</span></span>
<span id="cb10-210"><a href="#cb10-210"></a><span class="in">from numpy import dtype, ndarray, signedinteger</span></span>
<span id="cb10-211"><a href="#cb10-211"></a></span>
<span id="cb10-212"><a href="#cb10-212"></a></span>
<span id="cb10-213"><a href="#cb10-213"></a><span class="in"># Get the action with the max Q value</span></span>
<span id="cb10-214"><a href="#cb10-214"></a><span class="in">def get_argmax(G:np.ndarray) -&gt; ndarray[Any, dtype[signedinteger[Any]]]:</span></span>
<span id="cb10-215"><a href="#cb10-215"></a><span class="in">    candidates = np.argwhere(G == G.max()).flatten()</span></span>
<span id="cb10-216"><a href="#cb10-216"></a><span class="in">    # return the only index if there's only one max</span></span>
<span id="cb10-217"><a href="#cb10-217"></a><span class="in">    if len(candidates) == 1:</span></span>
<span id="cb10-218"><a href="#cb10-218"></a><span class="in">        return candidates[0]</span></span>
<span id="cb10-219"><a href="#cb10-219"></a><span class="in">    else:</span></span>
<span id="cb10-220"><a href="#cb10-220"></a><span class="in">        # instead break the tie randomly</span></span>
<span id="cb10-221"><a href="#cb10-221"></a><span class="in">        return np.random.choice(candidates)</span></span>
<span id="cb10-222"><a href="#cb10-222"></a></span>
<span id="cb10-223"><a href="#cb10-223"></a></span>
<span id="cb10-224"><a href="#cb10-224"></a><span class="in"># Select arm and get the reward</span></span>
<span id="cb10-225"><a href="#cb10-225"></a><span class="in">def bandit(q_star:np.ndarray, </span></span>
<span id="cb10-226"><a href="#cb10-226"></a><span class="in">           act:int) -&gt; tuple:</span></span>
<span id="cb10-227"><a href="#cb10-227"></a><span class="in">    real_rewards = np.random.normal(q_star, 1.0)</span></span>
<span id="cb10-228"><a href="#cb10-228"></a><span class="in">    # optim_choice = int(real_rewards[act] == real_rewards.max())</span></span>
<span id="cb10-229"><a href="#cb10-229"></a><span class="in">    optim_choice = int(q_star[act] == q_star.max())</span></span>
<span id="cb10-230"><a href="#cb10-230"></a><span class="in">    return real_rewards[act], optim_choice</span></span>
<span id="cb10-231"><a href="#cb10-231"></a><span class="in">```</span></span>
<span id="cb10-232"><a href="#cb10-232"></a></span>
<span id="cb10-233"><a href="#cb10-233"></a>Please save the above script as <span class="in">`utils.py`</span> in the firs level of the regrding</span>
<span id="cb10-234"><a href="#cb10-234"></a>project such that we can imported by the ist name fora example by</span>
<span id="cb10-235"><a href="#cb10-235"></a><span class="in">`from utils import bandit, plots`</span></span>
<span id="cb10-236"><a href="#cb10-236"></a></span>
<span id="cb10-237"><a href="#cb10-237"></a><span class="fu">## Incremental Implementation</span></span>
<span id="cb10-238"><a href="#cb10-238"></a></span>
<span id="cb10-239"><a href="#cb10-239"></a>Certainly! To express a more efficient method for estimating action values, we</span>
<span id="cb10-240"><a href="#cb10-240"></a>focus on using an **incremental update formula** rather than recalculating the</span>
<span id="cb10-241"><a href="#cb10-241"></a>average based on all past observations. The goal is to maintain a constant</span>
<span id="cb10-242"><a href="#cb10-242"></a>memory footprint and fixed computation per time step.</span>
<span id="cb10-243"><a href="#cb10-243"></a></span>
<span id="cb10-244"><a href="#cb10-244"></a><span class="fu">### Incremental Update Formula for Action-Value Estimation</span></span>
<span id="cb10-245"><a href="#cb10-245"></a></span>
<span id="cb10-246"><a href="#cb10-246"></a>Let $R_i$ denote the reward received after the $i$-th selection of the</span>
<span id="cb10-247"><a href="#cb10-247"></a>action.$Q_n$ denote the estimate of the action value after the action has been</span>
<span id="cb10-248"><a href="#cb10-248"></a>chosen $n-1$ times.</span>
<span id="cb10-249"><a href="#cb10-249"></a></span>
<span id="cb10-250"><a href="#cb10-250"></a>Instead of computing $Q_n$ as the sample average of all observed rewards (which</span>
<span id="cb10-251"><a href="#cb10-251"></a>requires storing and summing all rewards), we use the **incremental formula**:</span>
<span id="cb10-252"><a href="#cb10-252"></a></span>
<span id="cb10-253"><a href="#cb10-253"></a>$$</span>
<span id="cb10-254"><a href="#cb10-254"></a>    Q_n = Q_{n-1} + \alpha \left(R_n - Q_{n-1}\right) </span>
<span id="cb10-255"><a href="#cb10-255"></a>$$</span>
<span id="cb10-256"><a href="#cb10-256"></a></span>
<span id="cb10-257"><a href="#cb10-257"></a>Where: $Q_{n-1}$ is the previous estimate of the action value. $R_n$ is the</span>
<span id="cb10-258"><a href="#cb10-258"></a>reward received on the $n$-th selection. $\alpha$ is a **constant step size**,</span>
<span id="cb10-259"><a href="#cb10-259"></a>often set as $\dfrac{1}{n}$ to mimic the behavior of sample averaging when the</span>
<span id="cb10-260"><a href="#cb10-260"></a>number of observations grows.</span>
<span id="cb10-261"><a href="#cb10-261"></a></span>
<span id="cb10-262"><a href="#cb10-262"></a><span class="fu">### Derivation of the Incremental Formula</span></span>
<span id="cb10-263"><a href="#cb10-263"></a></span>
<span id="cb10-264"><a href="#cb10-264"></a><span class="ss">1.  </span>Start with the definition of the action value as the sample mean:</span>
<span id="cb10-265"><a href="#cb10-265"></a>    $\displaystyle Q_n =\dfrac{1}{n} \sum_{i=1}^{n} R_i$</span>
<span id="cb10-266"><a href="#cb10-266"></a></span>
<span id="cb10-267"><a href="#cb10-267"></a><span class="ss">2.  </span>Express $Q_n$ in terms of $Q_{n-1}$: $$</span>
<span id="cb10-268"><a href="#cb10-268"></a>    Q_n = \frac{1}{n}</span>
<span id="cb10-269"><a href="#cb10-269"></a>    \left<span class="co">[</span><span class="ot"> \sum_{i=1}^{n-1} R_i + R_n \right</span><span class="co">]</span></span>
<span id="cb10-270"><a href="#cb10-270"></a>    $$</span>
<span id="cb10-271"><a href="#cb10-271"></a></span>
<span id="cb10-272"><a href="#cb10-272"></a>This can be rearranged as: $$</span>
<span id="cb10-273"><a href="#cb10-273"></a>        Q_n = \frac{n-1}{n} \cdot Q_{n-1} +</span>
<span id="cb10-274"><a href="#cb10-274"></a>    \frac{1}{n} \cdot R_n. </span>
<span id="cb10-275"><a href="#cb10-275"></a>$$</span>
<span id="cb10-276"><a href="#cb10-276"></a></span>
<span id="cb10-277"><a href="#cb10-277"></a><span class="ss">3.  </span>Notice that </span>
<span id="cb10-278"><a href="#cb10-278"></a>        $\displaystyle</span>
<span id="cb10-279"><a href="#cb10-279"></a>        \frac{n-1}{n} \cdot Q_{n-1} = Q_{n-1} - \frac{1}{n}\cdot Q_{n-1}</span>
<span id="cb10-280"><a href="#cb10-280"></a>    $, so: </span>
<span id="cb10-281"><a href="#cb10-281"></a>    $$ </span>
<span id="cb10-282"><a href="#cb10-282"></a>        Q_n = Q_{n-1} + \frac{1}{n} \left(R_n -Q_{n-1}\right)</span>
<span id="cb10-283"><a href="#cb10-283"></a>    $$</span>
<span id="cb10-284"><a href="#cb10-284"></a></span>
<span id="cb10-285"><a href="#cb10-285"></a>Here, $\alpha = \dfrac{1}{n}$ adapts to the number of observations, ensuring the</span>
<span id="cb10-286"><a href="#cb10-286"></a>update balances the influence of new and past rewards.</span>
<span id="cb10-287"><a href="#cb10-287"></a></span>
<span id="cb10-288"><a href="#cb10-288"></a><span class="fu">### Advantages of the Incremental Method</span></span>
<span id="cb10-289"><a href="#cb10-289"></a></span>
<span id="cb10-290"><a href="#cb10-290"></a><span class="ss">-   </span>**Constant Memory**: The method only requires storing $Q_{n-1}$ and $R_n$,</span>
<span id="cb10-291"><a href="#cb10-291"></a>    avoiding the need to keep all past rewards.</span>
<span id="cb10-292"><a href="#cb10-292"></a><span class="ss">-   </span>**Fixed Computation**: Each update involves a fixed, small number of</span>
<span id="cb10-293"><a href="#cb10-293"></a>    operations, regardless of $n$.</span>
<span id="cb10-294"><a href="#cb10-294"></a></span>
<span id="cb10-295"><a href="#cb10-295"></a>This approach efficiently updates the action-value estimate with minimal</span>
<span id="cb10-296"><a href="#cb10-296"></a>resources, making it suitable for online learning algorithms and scenarios where</span>
<span id="cb10-297"><a href="#cb10-297"></a>computational efficiency is critical.</span>
<span id="cb10-298"><a href="#cb10-298"></a></span>
<span id="cb10-299"><a href="#cb10-299"></a>![Incremental</span>
<span id="cb10-300"><a href="#cb10-300"></a>algorithm](/assets/ch_03/mab_simple_algorithm.png){fig-align="center"</span>
<span id="cb10-301"><a href="#cb10-301"></a>fig-env="figure"}</span>
<span id="cb10-302"><a href="#cb10-302"></a></span>
<span id="cb10-303"><a href="#cb10-303"></a>Bellow a python implementation.</span>
<span id="cb10-304"><a href="#cb10-304"></a></span>
<span id="cb10-305"><a href="#cb10-305"></a><span class="in">``` {.python filename="example_2_2_bandits_algo.py"}</span></span>
<span id="cb10-306"><a href="#cb10-306"></a><span class="in">import numpy as np</span></span>
<span id="cb10-307"><a href="#cb10-307"></a><span class="in">import matplotlib</span></span>
<span id="cb10-308"><a href="#cb10-308"></a><span class="in">import matplotlib.pyplot as plt</span></span>
<span id="cb10-309"><a href="#cb10-309"></a><span class="in">matplotlib.use('qt5agg')</span></span>
<span id="cb10-310"><a href="#cb10-310"></a><span class="in">import pickle</span></span>
<span id="cb10-311"><a href="#cb10-311"></a></span>
<span id="cb10-312"><a href="#cb10-312"></a><span class="in">from utils import get_argmax, bandit</span></span>
<span id="cb10-313"><a href="#cb10-313"></a></span>
<span id="cb10-314"><a href="#cb10-314"></a><span class="in">#SEED = 123456</span></span>
<span id="cb10-315"><a href="#cb10-315"></a><span class="in">#np.random.seed(SEED)</span></span>
<span id="cb10-316"><a href="#cb10-316"></a></span>
<span id="cb10-317"><a href="#cb10-317"></a><span class="in"># running the k-armed bandit algorithm</span></span>
<span id="cb10-318"><a href="#cb10-318"></a><span class="in">def run_bandit(K:int, </span></span>
<span id="cb10-319"><a href="#cb10-319"></a><span class="in">            q_star:np.ndarray,</span></span>
<span id="cb10-320"><a href="#cb10-320"></a><span class="in">            rewards:np.ndarray,</span></span>
<span id="cb10-321"><a href="#cb10-321"></a><span class="in">            optim_acts_ratio:np.ndarray,</span></span>
<span id="cb10-322"><a href="#cb10-322"></a><span class="in">            epsilon:float, </span></span>
<span id="cb10-323"><a href="#cb10-323"></a><span class="in">            num_steps:int=1000) -&gt; None:</span></span>
<span id="cb10-324"><a href="#cb10-324"></a><span class="in">    </span></span>
<span id="cb10-325"><a href="#cb10-325"></a><span class="in">    Q = np.zeros(K)     # Initialize Q values</span></span>
<span id="cb10-326"><a href="#cb10-326"></a><span class="in">    N = np.zeros(K)     # The number of times each action been selected</span></span>
<span id="cb10-327"><a href="#cb10-327"></a><span class="in">    ttl_optim_acts = 0</span></span>
<span id="cb10-328"><a href="#cb10-328"></a></span>
<span id="cb10-329"><a href="#cb10-329"></a><span class="in">    for i in range(num_steps):</span></span>
<span id="cb10-330"><a href="#cb10-330"></a><span class="in">        # get action</span></span>
<span id="cb10-331"><a href="#cb10-331"></a><span class="in">        A = None</span></span>
<span id="cb10-332"><a href="#cb10-332"></a><span class="in">        if np.random.random() &gt; epsilon:</span></span>
<span id="cb10-333"><a href="#cb10-333"></a><span class="in">            A = get_argmax(Q)</span></span>
<span id="cb10-334"><a href="#cb10-334"></a><span class="in">        else:</span></span>
<span id="cb10-335"><a href="#cb10-335"></a><span class="in">            A = np.random.randint(0, K)</span></span>
<span id="cb10-336"><a href="#cb10-336"></a><span class="in">        </span></span>
<span id="cb10-337"><a href="#cb10-337"></a><span class="in">        R, is_optim = bandit(q_star, A)</span></span>
<span id="cb10-338"><a href="#cb10-338"></a><span class="in">        N[A] += 1</span></span>
<span id="cb10-339"><a href="#cb10-339"></a><span class="in">        Q[A] += (R - Q[A]) / N[A]</span></span>
<span id="cb10-340"><a href="#cb10-340"></a></span>
<span id="cb10-341"><a href="#cb10-341"></a><span class="in">        ttl_optim_acts += is_optim</span></span>
<span id="cb10-342"><a href="#cb10-342"></a><span class="in">        rewards[i] = R</span></span>
<span id="cb10-343"><a href="#cb10-343"></a><span class="in">        optim_acts_ratio[i] = ttl_optim_acts / (i + 1)</span></span>
<span id="cb10-344"><a href="#cb10-344"></a></span>
<span id="cb10-345"><a href="#cb10-345"></a></span>
<span id="cb10-346"><a href="#cb10-346"></a><span class="in">if __name__ == "__main__":</span></span>
<span id="cb10-347"><a href="#cb10-347"></a></span>
<span id="cb10-348"><a href="#cb10-348"></a><span class="in">    # Initializing the hyperparameters</span></span>
<span id="cb10-349"><a href="#cb10-349"></a><span class="in">    K = 10  # Number of arms</span></span>
<span id="cb10-350"><a href="#cb10-350"></a><span class="in">    epsilons = [0.0, 0.01, 0.1]</span></span>
<span id="cb10-351"><a href="#cb10-351"></a><span class="in">    num_steps = 1000</span></span>
<span id="cb10-352"><a href="#cb10-352"></a><span class="in">    total_rounds = 1000</span></span>
<span id="cb10-353"><a href="#cb10-353"></a></span>
<span id="cb10-354"><a href="#cb10-354"></a><span class="in">    # Initialize the environment</span></span>
<span id="cb10-355"><a href="#cb10-355"></a><span class="in">    q_star = np.random.normal(loc=0, scale=1.0, size=K)</span></span>
<span id="cb10-356"><a href="#cb10-356"></a><span class="in">    rewards = np.zeros(shape=(len(epsilons), total_rounds, num_steps))</span></span>
<span id="cb10-357"><a href="#cb10-357"></a><span class="in">    optim_acts_ratio = np.zeros(shape=(len(epsilons), total_rounds, num_steps))</span></span>
<span id="cb10-358"><a href="#cb10-358"></a><span class="in">    </span></span>
<span id="cb10-359"><a href="#cb10-359"></a><span class="in">    # Run the k-armed bandits alg.</span></span>
<span id="cb10-360"><a href="#cb10-360"></a><span class="in">    for i, epsilon in enumerate(epsilons):</span></span>
<span id="cb10-361"><a href="#cb10-361"></a><span class="in">        for curr_round in range(total_rounds):</span></span>
<span id="cb10-362"><a href="#cb10-362"></a><span class="in">            run_bandit(K, q_star, </span></span>
<span id="cb10-363"><a href="#cb10-363"></a><span class="in">                       rewards[i, curr_round], </span></span>
<span id="cb10-364"><a href="#cb10-364"></a><span class="in">                       optim_acts_ratio[i, curr_round], </span></span>
<span id="cb10-365"><a href="#cb10-365"></a><span class="in">                       epsilon, </span></span>
<span id="cb10-366"><a href="#cb10-366"></a><span class="in">                       num_steps)</span></span>
<span id="cb10-367"><a href="#cb10-367"></a><span class="in">    </span></span>
<span id="cb10-368"><a href="#cb10-368"></a><span class="in">    rewards = rewards.mean(axis=1)</span></span>
<span id="cb10-369"><a href="#cb10-369"></a><span class="in">    optim_acts_ratio = optim_acts_ratio.mean(axis=1)</span></span>
<span id="cb10-370"><a href="#cb10-370"></a></span>
<span id="cb10-371"><a href="#cb10-371"></a><span class="in">    record = {</span></span>
<span id="cb10-372"><a href="#cb10-372"></a><span class="in">        'hyper_params': epsilons, </span></span>
<span id="cb10-373"><a href="#cb10-373"></a><span class="in">        'rewards': rewards,</span></span>
<span id="cb10-374"><a href="#cb10-374"></a><span class="in">        'optim_acts_ratio': optim_acts_ratio</span></span>
<span id="cb10-375"><a href="#cb10-375"></a><span class="in">    }</span></span>
<span id="cb10-376"><a href="#cb10-376"></a></span>
<span id="cb10-377"><a href="#cb10-377"></a><span class="in">    fig_01, ax_01 = plt.subplots()</span></span>
<span id="cb10-378"><a href="#cb10-378"></a><span class="in">    fig_02, ax_02 = plt.subplots()</span></span>
<span id="cb10-379"><a href="#cb10-379"></a><span class="in">    for i, ratio in enumerate(optim_acts_ratio):</span></span>
<span id="cb10-380"><a href="#cb10-380"></a><span class="in">        ax_01.plot(</span></span>
<span id="cb10-381"><a href="#cb10-381"></a><span class="in">                ratio,</span></span>
<span id="cb10-382"><a href="#cb10-382"></a><span class="in">                label=r'$\epsilon={epsilon_i}$'.format(epsilon_i=epsilons[i])</span></span>
<span id="cb10-383"><a href="#cb10-383"></a><span class="in">        )</span></span>
<span id="cb10-384"><a href="#cb10-384"></a><span class="in">    </span></span>
<span id="cb10-385"><a href="#cb10-385"></a><span class="in">    for i, reward in enumerate(rewards):</span></span>
<span id="cb10-386"><a href="#cb10-386"></a><span class="in">        ax_02.plot(</span></span>
<span id="cb10-387"><a href="#cb10-387"></a><span class="in">                reward,</span></span>
<span id="cb10-388"><a href="#cb10-388"></a><span class="in">                label=r'$\epsilon={epsilon_i}$'.format(epsilon_i=epsilons[i])</span></span>
<span id="cb10-389"><a href="#cb10-389"></a><span class="in">                )</span></span>
<span id="cb10-390"><a href="#cb10-390"></a><span class="in">    ax_01.set_xlabel(r'$t$', fontsize=12)</span></span>
<span id="cb10-391"><a href="#cb10-391"></a><span class="in">    ax_01.set_ylabel(r'Optimal Action', fontsize=12)</span></span>
<span id="cb10-392"><a href="#cb10-392"></a><span class="in">    ax_01.legend(loc='best')</span></span>
<span id="cb10-393"><a href="#cb10-393"></a><span class="in">    ax_02.set_xlabel(r'$t$', fontsize=12)</span></span>
<span id="cb10-394"><a href="#cb10-394"></a><span class="in">    ax_02.set_ylabel(r'Reward', fontsize=12)</span></span>
<span id="cb10-395"><a href="#cb10-395"></a><span class="in">    ax_02.legend(loc='best')</span></span>
<span id="cb10-396"><a href="#cb10-396"></a><span class="in">    plt.show()</span></span>
<span id="cb10-397"><a href="#cb10-397"></a><span class="in">    </span></span>
<span id="cb10-398"><a href="#cb10-398"></a><span class="in">    # with open('./history/record.pkl', 'wb') as f:</span></span>
<span id="cb10-399"><a href="#cb10-399"></a><span class="in">    #     pickle.dump(record, f)</span></span>
<span id="cb10-400"><a href="#cb10-400"></a><span class="in">```</span></span>
<span id="cb10-401"><a href="#cb10-401"></a></span>
<span id="cb10-402"><a href="#cb10-402"></a><span class="fu">## Tracking a Nonstationary Problem</span></span>
<span id="cb10-403"><a href="#cb10-403"></a></span>
<span id="cb10-404"><a href="#cb10-404"></a>The averaging methods we have discussed are suitable for stationary bandit</span>
<span id="cb10-405"><a href="#cb10-405"></a>problems, where the reward probabilities remain constant over time. However, in</span>
<span id="cb10-406"><a href="#cb10-406"></a>reinforcement learning, we often encounter non-stationary problems where it</span>
<span id="cb10-407"><a href="#cb10-407"></a>makes more sense to give greater weight to recent rewards than to rewards from a</span>
<span id="cb10-408"><a href="#cb10-408"></a>long time ago. One popular approach to achieve this is by using a constant</span>
<span id="cb10-409"><a href="#cb10-409"></a>step-size parameter.</span>
<span id="cb10-410"><a href="#cb10-410"></a></span>
<span id="cb10-411"><a href="#cb10-411"></a>#TODO: Formulation with constant alpha and implications</span>
<span id="cb10-412"><a href="#cb10-412"></a></span>
<span id="cb10-413"><a href="#cb10-413"></a><span class="fu">## Optimistic Initial Values</span></span>
<span id="cb10-414"><a href="#cb10-414"></a></span>
<span id="cb10-415"><a href="#cb10-415"></a><span class="in">``` {.python filename="example_2_3_OIV.py"}</span></span>
<span id="cb10-416"><a href="#cb10-416"></a><span class="in">import numpy as np</span></span>
<span id="cb10-417"><a href="#cb10-417"></a><span class="in">import matplotlib.pyplot as plt</span></span>
<span id="cb10-418"><a href="#cb10-418"></a><span class="in">import pickle</span></span>
<span id="cb10-419"><a href="#cb10-419"></a></span>
<span id="cb10-420"><a href="#cb10-420"></a><span class="in">from utils import get_argmax, bandit</span></span>
<span id="cb10-421"><a href="#cb10-421"></a></span>
<span id="cb10-422"><a href="#cb10-422"></a><span class="in">SEED = 200</span></span>
<span id="cb10-423"><a href="#cb10-423"></a><span class="in">np.random.seed(SEED)</span></span>
<span id="cb10-424"><a href="#cb10-424"></a></span>
<span id="cb10-425"><a href="#cb10-425"></a></span>
<span id="cb10-426"><a href="#cb10-426"></a><span class="in"># running the k-armed bandit algorithm</span></span>
<span id="cb10-427"><a href="#cb10-427"></a><span class="in">def run_bandit(K: int,</span></span>
<span id="cb10-428"><a href="#cb10-428"></a><span class="in">            q_star: np.ndarray,</span></span>
<span id="cb10-429"><a href="#cb10-429"></a><span class="in">            rewards: np.ndarray,</span></span>
<span id="cb10-430"><a href="#cb10-430"></a><span class="in">            optim_acts_ratio: np.ndarray,</span></span>
<span id="cb10-431"><a href="#cb10-431"></a><span class="in">            epsilon: float,</span></span>
<span id="cb10-432"><a href="#cb10-432"></a><span class="in">            num_steps: int=1000,</span></span>
<span id="cb10-433"><a href="#cb10-433"></a><span class="in">            init_val: int=0</span></span>
<span id="cb10-434"><a href="#cb10-434"></a><span class="in">) -&gt; None:</span></span>
<span id="cb10-435"><a href="#cb10-435"></a><span class="in">    Q = np.ones(K) * init_val   # Initial Q values with OIV</span></span>
<span id="cb10-436"><a href="#cb10-436"></a><span class="in">    ttl_optim_acts = 0</span></span>
<span id="cb10-437"><a href="#cb10-437"></a><span class="in">    alpha = 0.1</span></span>
<span id="cb10-438"><a href="#cb10-438"></a></span>
<span id="cb10-439"><a href="#cb10-439"></a><span class="in">    for i in range(num_steps):</span></span>
<span id="cb10-440"><a href="#cb10-440"></a><span class="in">        # get action</span></span>
<span id="cb10-441"><a href="#cb10-441"></a><span class="in">        A = None</span></span>
<span id="cb10-442"><a href="#cb10-442"></a><span class="in">        if np.random.random() &gt; epsilon:</span></span>
<span id="cb10-443"><a href="#cb10-443"></a><span class="in">            A = get_argmax(Q)</span></span>
<span id="cb10-444"><a href="#cb10-444"></a><span class="in">        else:</span></span>
<span id="cb10-445"><a href="#cb10-445"></a><span class="in">            A = np.random.randint(0, K)</span></span>
<span id="cb10-446"><a href="#cb10-446"></a><span class="in">        </span></span>
<span id="cb10-447"><a href="#cb10-447"></a><span class="in">        R, is_optim = bandit(q_star, A)</span></span>
<span id="cb10-448"><a href="#cb10-448"></a><span class="in">        Q[A] += alpha * (R - Q[A])</span></span>
<span id="cb10-449"><a href="#cb10-449"></a></span>
<span id="cb10-450"><a href="#cb10-450"></a><span class="in">        ttl_optim_acts += is_optim</span></span>
<span id="cb10-451"><a href="#cb10-451"></a><span class="in">        rewards[i] = R</span></span>
<span id="cb10-452"><a href="#cb10-452"></a><span class="in">        optim_acts_ratio[i] = ttl_optim_acts / (i + 1)</span></span>
<span id="cb10-453"><a href="#cb10-453"></a></span>
<span id="cb10-454"><a href="#cb10-454"></a></span>
<span id="cb10-455"><a href="#cb10-455"></a><span class="in">if __name__ == "__main__":</span></span>
<span id="cb10-456"><a href="#cb10-456"></a></span>
<span id="cb10-457"><a href="#cb10-457"></a><span class="in">    # Initializing the hyper-parameters</span></span>
<span id="cb10-458"><a href="#cb10-458"></a><span class="in">    K = 10 # Number of arms</span></span>
<span id="cb10-459"><a href="#cb10-459"></a><span class="in">    epsilons = [0.1, 0.0]</span></span>
<span id="cb10-460"><a href="#cb10-460"></a><span class="in">    init_vals = [0.0, 5.0]</span></span>
<span id="cb10-461"><a href="#cb10-461"></a><span class="in">    num_steps = 1000</span></span>
<span id="cb10-462"><a href="#cb10-462"></a><span class="in">    total_rounds = 2000</span></span>
<span id="cb10-463"><a href="#cb10-463"></a></span>
<span id="cb10-464"><a href="#cb10-464"></a><span class="in">    # Initialize the environment</span></span>
<span id="cb10-465"><a href="#cb10-465"></a><span class="in">    q_star = np.random.normal(loc=0, scale=1.0, size=K)</span></span>
<span id="cb10-466"><a href="#cb10-466"></a><span class="in">    rewards = np.zeros(shape=(len(epsilons), total_rounds, num_steps))</span></span>
<span id="cb10-467"><a href="#cb10-467"></a><span class="in">    optim_acts_ratio = np.zeros(shape=(len(epsilons), total_rounds, num_steps))</span></span>
<span id="cb10-468"><a href="#cb10-468"></a><span class="in">    </span></span>
<span id="cb10-469"><a href="#cb10-469"></a><span class="in">    # Run the k-armed bandits alg.</span></span>
<span id="cb10-470"><a href="#cb10-470"></a><span class="in">    for i, (epsilon, init_val) in enumerate(zip(epsilons, init_vals)):</span></span>
<span id="cb10-471"><a href="#cb10-471"></a><span class="in">        for curr_round in range(total_rounds):</span></span>
<span id="cb10-472"><a href="#cb10-472"></a><span class="in">            run_bandit(K, q_star, </span></span>
<span id="cb10-473"><a href="#cb10-473"></a><span class="in">                       rewards[i, curr_round], </span></span>
<span id="cb10-474"><a href="#cb10-474"></a><span class="in">                       optim_acts_ratio[i, curr_round], </span></span>
<span id="cb10-475"><a href="#cb10-475"></a><span class="in">                       epsilon=epsilon, </span></span>
<span id="cb10-476"><a href="#cb10-476"></a><span class="in">                       num_steps=num_steps,</span></span>
<span id="cb10-477"><a href="#cb10-477"></a><span class="in">                       init_val=init_val)</span></span>
<span id="cb10-478"><a href="#cb10-478"></a><span class="in">    </span></span>
<span id="cb10-479"><a href="#cb10-479"></a><span class="in">    rewards = rewards.mean(axis=1)</span></span>
<span id="cb10-480"><a href="#cb10-480"></a><span class="in">    optim_acts_ratio = optim_acts_ratio.mean(axis=1)</span></span>
<span id="cb10-481"><a href="#cb10-481"></a></span>
<span id="cb10-482"><a href="#cb10-482"></a><span class="in">    record = {</span></span>
<span id="cb10-483"><a href="#cb10-483"></a><span class="in">        'hyper_params': [epsilons, init_vals], </span></span>
<span id="cb10-484"><a href="#cb10-484"></a><span class="in">        'rewards': rewards,</span></span>
<span id="cb10-485"><a href="#cb10-485"></a><span class="in">        'optim_acts_ratio': optim_acts_ratio</span></span>
<span id="cb10-486"><a href="#cb10-486"></a><span class="in">    }</span></span>
<span id="cb10-487"><a href="#cb10-487"></a></span>
<span id="cb10-488"><a href="#cb10-488"></a><span class="in">    for vals in rewards:</span></span>
<span id="cb10-489"><a href="#cb10-489"></a><span class="in">        plt.plot(vals)</span></span>
<span id="cb10-490"><a href="#cb10-490"></a><span class="in">    plt.show()</span></span>
<span id="cb10-491"><a href="#cb10-491"></a><span class="in">    # with open('./history/OIV_record.pkl', 'wb') as f:</span></span>
<span id="cb10-492"><a href="#cb10-492"></a><span class="in">    #     pickle.dump(record, f)</span></span>
<span id="cb10-493"><a href="#cb10-493"></a><span class="in">```</span></span>
<span id="cb10-494"><a href="#cb10-494"></a></span>
<span id="cb10-495"><a href="#cb10-495"></a><span class="fu">## Upper-Confidence-Bound Action Selection</span></span>
<span id="cb10-496"><a href="#cb10-496"></a></span>
<span id="cb10-497"><a href="#cb10-497"></a><span class="in">``` {.python filename="example_2_4_UCB.py"}</span></span>
<span id="cb10-498"><a href="#cb10-498"></a><span class="in">from typing import Any</span></span>
<span id="cb10-499"><a href="#cb10-499"></a><span class="in">import numpy as np</span></span>
<span id="cb10-500"><a href="#cb10-500"></a><span class="in">import matplotlib.pyplot as plt</span></span>
<span id="cb10-501"><a href="#cb10-501"></a><span class="in">import pickle</span></span>
<span id="cb10-502"><a href="#cb10-502"></a><span class="in">from numpy import dtype, ndarray</span></span>
<span id="cb10-503"><a href="#cb10-503"></a><span class="in">from tqdm import tqdm</span></span>
<span id="cb10-504"><a href="#cb10-504"></a><span class="in">from utils import get_argmax, bandit</span></span>
<span id="cb10-505"><a href="#cb10-505"></a></span>
<span id="cb10-506"><a href="#cb10-506"></a><span class="in">SEED = 200</span></span>
<span id="cb10-507"><a href="#cb10-507"></a><span class="in">np.random.seed(SEED)</span></span>
<span id="cb10-508"><a href="#cb10-508"></a></span>
<span id="cb10-509"><a href="#cb10-509"></a></span>
<span id="cb10-510"><a href="#cb10-510"></a><span class="in"># running the k-armed bandit algorithm</span></span>
<span id="cb10-511"><a href="#cb10-511"></a><span class="in">def run_bandit(</span></span>
<span id="cb10-512"><a href="#cb10-512"></a><span class="in">        K: int,</span></span>
<span id="cb10-513"><a href="#cb10-513"></a><span class="in">        q_star: np.ndarray,</span></span>
<span id="cb10-514"><a href="#cb10-514"></a><span class="in">        rewards: np.ndarray,</span></span>
<span id="cb10-515"><a href="#cb10-515"></a><span class="in">        optim_acts_ratio: np.ndarray,</span></span>
<span id="cb10-516"><a href="#cb10-516"></a><span class="in">        epsilon: float,</span></span>
<span id="cb10-517"><a href="#cb10-517"></a><span class="in">        num_steps: int = 1000</span></span>
<span id="cb10-518"><a href="#cb10-518"></a><span class="in">        ) -&gt; None:</span></span>
<span id="cb10-519"><a href="#cb10-519"></a><span class="in">    Q = np.zeros(K)</span></span>
<span id="cb10-520"><a href="#cb10-520"></a><span class="in">    N = np.zeros(K)  # The number of times each action been selected</span></span>
<span id="cb10-521"><a href="#cb10-521"></a><span class="in">    ttl_optim_acts = 0</span></span>
<span id="cb10-522"><a href="#cb10-522"></a><span class="in">    </span></span>
<span id="cb10-523"><a href="#cb10-523"></a><span class="in">    for i in range(num_steps):</span></span>
<span id="cb10-524"><a href="#cb10-524"></a><span class="in">        A = None</span></span>
<span id="cb10-525"><a href="#cb10-525"></a><span class="in">        # Get action</span></span>
<span id="cb10-526"><a href="#cb10-526"></a><span class="in">        if np.random.random() &gt; epsilon:</span></span>
<span id="cb10-527"><a href="#cb10-527"></a><span class="in">            A = get_argmax(Q)</span></span>
<span id="cb10-528"><a href="#cb10-528"></a><span class="in">        else:</span></span>
<span id="cb10-529"><a href="#cb10-529"></a><span class="in">            A = np.random.randint(0, K)</span></span>
<span id="cb10-530"><a href="#cb10-530"></a><span class="in">        </span></span>
<span id="cb10-531"><a href="#cb10-531"></a><span class="in">        R, is_optim = bandit(q_star, A)</span></span>
<span id="cb10-532"><a href="#cb10-532"></a><span class="in">        N[A] += 1</span></span>
<span id="cb10-533"><a href="#cb10-533"></a><span class="in">        Q[A] += (R - Q[A]) / N[A]</span></span>
<span id="cb10-534"><a href="#cb10-534"></a><span class="in">        </span></span>
<span id="cb10-535"><a href="#cb10-535"></a><span class="in">        ttl_optim_acts += is_optim</span></span>
<span id="cb10-536"><a href="#cb10-536"></a><span class="in">        rewards[i] = R</span></span>
<span id="cb10-537"><a href="#cb10-537"></a><span class="in">        optim_acts_ratio[i] = ttl_optim_acts / (i + 1)</span></span>
<span id="cb10-538"><a href="#cb10-538"></a></span>
<span id="cb10-539"><a href="#cb10-539"></a></span>
<span id="cb10-540"><a href="#cb10-540"></a><span class="in"># running the bandit algorithm with UCB</span></span>
<span id="cb10-541"><a href="#cb10-541"></a><span class="in">def run_bandit_UCB(</span></span>
<span id="cb10-542"><a href="#cb10-542"></a><span class="in">        K: int,</span></span>
<span id="cb10-543"><a href="#cb10-543"></a><span class="in">        q_star: np.ndarray,</span></span>
<span id="cb10-544"><a href="#cb10-544"></a><span class="in">        rewards: np.ndarray,</span></span>
<span id="cb10-545"><a href="#cb10-545"></a><span class="in">        optim_acts_ratio: np.ndarray,</span></span>
<span id="cb10-546"><a href="#cb10-546"></a><span class="in">        c: float,</span></span>
<span id="cb10-547"><a href="#cb10-547"></a><span class="in">        num_steps: int = 1000</span></span>
<span id="cb10-548"><a href="#cb10-548"></a><span class="in">        ) -&gt; None:</span></span>
<span id="cb10-549"><a href="#cb10-549"></a><span class="in">    Q = np.zeros(K)</span></span>
<span id="cb10-550"><a href="#cb10-550"></a><span class="in">    N = np.zeros(K)  # The number of times each action been selected</span></span>
<span id="cb10-551"><a href="#cb10-551"></a><span class="in">    ttl_optim_acts = 0</span></span>
<span id="cb10-552"><a href="#cb10-552"></a><span class="in">    </span></span>
<span id="cb10-553"><a href="#cb10-553"></a><span class="in">    for i in range(num_steps):</span></span>
<span id="cb10-554"><a href="#cb10-554"></a><span class="in">        A = None</span></span>
<span id="cb10-555"><a href="#cb10-555"></a><span class="in">        </span></span>
<span id="cb10-556"><a href="#cb10-556"></a><span class="in">        # Avoid 0-division:</span></span>
<span id="cb10-557"><a href="#cb10-557"></a><span class="in">        # If there's 0 in N, then choose the action with N = 0</span></span>
<span id="cb10-558"><a href="#cb10-558"></a><span class="in">        if 0 in N:</span></span>
<span id="cb10-559"><a href="#cb10-559"></a><span class="in">            candidates = np.argwhere(N == 0).flatten()</span></span>
<span id="cb10-560"><a href="#cb10-560"></a><span class="in">            A = np.random.choice(candidates)</span></span>
<span id="cb10-561"><a href="#cb10-561"></a><span class="in">        else:</span></span>
<span id="cb10-562"><a href="#cb10-562"></a><span class="in">            confidence = c * np.sqrt(np.log(i) / N)</span></span>
<span id="cb10-563"><a href="#cb10-563"></a><span class="in">            freqs: ndarray[Any, dtype[Any]] | Any = Q + confidence</span></span>
<span id="cb10-564"><a href="#cb10-564"></a><span class="in">            A = np.argmax(freqs).flatten()</span></span>
<span id="cb10-565"><a href="#cb10-565"></a><span class="in">        </span></span>
<span id="cb10-566"><a href="#cb10-566"></a><span class="in">        R, is_optim = bandit(q_star, A)</span></span>
<span id="cb10-567"><a href="#cb10-567"></a><span class="in">        N[A] += 1</span></span>
<span id="cb10-568"><a href="#cb10-568"></a><span class="in">        Q[A] += (R - Q[A]) / N[A]</span></span>
<span id="cb10-569"><a href="#cb10-569"></a><span class="in">        </span></span>
<span id="cb10-570"><a href="#cb10-570"></a><span class="in">        ttl_optim_acts += is_optim</span></span>
<span id="cb10-571"><a href="#cb10-571"></a><span class="in">        rewards[i] = R</span></span>
<span id="cb10-572"><a href="#cb10-572"></a><span class="in">        optim_acts_ratio[i] = ttl_optim_acts / (i + 1)</span></span>
<span id="cb10-573"><a href="#cb10-573"></a></span>
<span id="cb10-574"><a href="#cb10-574"></a></span>
<span id="cb10-575"><a href="#cb10-575"></a><span class="in">if __name__ == "__main__":</span></span>
<span id="cb10-576"><a href="#cb10-576"></a><span class="in">    </span></span>
<span id="cb10-577"><a href="#cb10-577"></a><span class="in">    # Initializing the hyper-parameters</span></span>
<span id="cb10-578"><a href="#cb10-578"></a><span class="in">    K = 10  # Number of arms</span></span>
<span id="cb10-579"><a href="#cb10-579"></a><span class="in">    num_steps = 1000</span></span>
<span id="cb10-580"><a href="#cb10-580"></a><span class="in">    total_rounds = 100</span></span>
<span id="cb10-581"><a href="#cb10-581"></a><span class="in">    q_star = np.random.normal(loc=0, scale=1.0, size=K)</span></span>
<span id="cb10-582"><a href="#cb10-582"></a><span class="in">    hyper_params = {'UCB': 2, 'epsilon': 0.1}</span></span>
<span id="cb10-583"><a href="#cb10-583"></a><span class="in">    rewards = np.zeros(shape=(len(hyper_params), total_rounds, num_steps))</span></span>
<span id="cb10-584"><a href="#cb10-584"></a><span class="in">    optim_acts_ratio = np.zeros(</span></span>
<span id="cb10-585"><a href="#cb10-585"></a><span class="in">            shape=(len(hyper_params), total_rounds, num_steps)</span></span>
<span id="cb10-586"><a href="#cb10-586"></a><span class="in">            )</span></span>
<span id="cb10-587"><a href="#cb10-587"></a><span class="in">    </span></span>
<span id="cb10-588"><a href="#cb10-588"></a><span class="in">    # Run bandit alg. with e-greedy</span></span>
<span id="cb10-589"><a href="#cb10-589"></a><span class="in">    for curr_round in tqdm(range(total_rounds)):</span></span>
<span id="cb10-590"><a href="#cb10-590"></a><span class="in">        # for curr_round in range(total_rounds):</span></span>
<span id="cb10-591"><a href="#cb10-591"></a><span class="in">        run_bandit(</span></span>
<span id="cb10-592"><a href="#cb10-592"></a><span class="in">                K,</span></span>
<span id="cb10-593"><a href="#cb10-593"></a><span class="in">                q_star,</span></span>
<span id="cb10-594"><a href="#cb10-594"></a><span class="in">                rewards[0, curr_round],</span></span>
<span id="cb10-595"><a href="#cb10-595"></a><span class="in">                optim_acts_ratio[0, curr_round],</span></span>
<span id="cb10-596"><a href="#cb10-596"></a><span class="in">                epsilon=hyper_params['epsilon'],</span></span>
<span id="cb10-597"><a href="#cb10-597"></a><span class="in">                num_steps=num_steps</span></span>
<span id="cb10-598"><a href="#cb10-598"></a><span class="in">                )</span></span>
<span id="cb10-599"><a href="#cb10-599"></a><span class="in">    </span></span>
<span id="cb10-600"><a href="#cb10-600"></a><span class="in">    # Run UCB and get records</span></span>
<span id="cb10-601"><a href="#cb10-601"></a><span class="in">    for curr_round in tqdm(range(total_rounds)):</span></span>
<span id="cb10-602"><a href="#cb10-602"></a><span class="in">        # for curr_round in range(total_rounds):</span></span>
<span id="cb10-603"><a href="#cb10-603"></a><span class="in">        run_bandit_UCB(</span></span>
<span id="cb10-604"><a href="#cb10-604"></a><span class="in">                K,</span></span>
<span id="cb10-605"><a href="#cb10-605"></a><span class="in">                q_star,</span></span>
<span id="cb10-606"><a href="#cb10-606"></a><span class="in">                rewards[1, curr_round],</span></span>
<span id="cb10-607"><a href="#cb10-607"></a><span class="in">                optim_acts_ratio[1, curr_round],</span></span>
<span id="cb10-608"><a href="#cb10-608"></a><span class="in">                c=hyper_params['UCB'],</span></span>
<span id="cb10-609"><a href="#cb10-609"></a><span class="in">                num_steps=num_steps</span></span>
<span id="cb10-610"><a href="#cb10-610"></a><span class="in">                )</span></span>
<span id="cb10-611"><a href="#cb10-611"></a><span class="in">    </span></span>
<span id="cb10-612"><a href="#cb10-612"></a><span class="in">    rewards = rewards.mean(axis=1)</span></span>
<span id="cb10-613"><a href="#cb10-613"></a><span class="in">    optim_acts_ratio = optim_acts_ratio.mean(axis=1)</span></span>
<span id="cb10-614"><a href="#cb10-614"></a><span class="in">    </span></span>
<span id="cb10-615"><a href="#cb10-615"></a><span class="in">    record = {</span></span>
<span id="cb10-616"><a href="#cb10-616"></a><span class="in">            'hyper_params': hyper_params,</span></span>
<span id="cb10-617"><a href="#cb10-617"></a><span class="in">            'rewards': rewards,</span></span>
<span id="cb10-618"><a href="#cb10-618"></a><span class="in">            'optim_acts_ratio': optim_acts_ratio</span></span>
<span id="cb10-619"><a href="#cb10-619"></a><span class="in">            }</span></span>
<span id="cb10-620"><a href="#cb10-620"></a><span class="in">    data = rewards</span></span>
<span id="cb10-621"><a href="#cb10-621"></a><span class="in">    plt.figure(figsize=(10, 6), dpi=150)</span></span>
<span id="cb10-622"><a href="#cb10-622"></a><span class="in">    plt.grid(c='lightgray')</span></span>
<span id="cb10-623"><a href="#cb10-623"></a><span class="in">    plt.margins(0.02)</span></span>
<span id="cb10-624"><a href="#cb10-624"></a><span class="in">    # revers the loop for a better visualization</span></span>
<span id="cb10-625"><a href="#cb10-625"></a><span class="in">    # colors = ['cornflowerblue', 'tomato', 'lightseagreen']</span></span>
<span id="cb10-626"><a href="#cb10-626"></a><span class="in">    colors = ['r', 'b']</span></span>
<span id="cb10-627"><a href="#cb10-627"></a><span class="in">    meta = record['hyper_params']</span></span>
<span id="cb10-628"><a href="#cb10-628"></a><span class="in">    optim_ratio = (optim_acts_ratio * 100)</span></span>
<span id="cb10-629"><a href="#cb10-629"></a><span class="in">    legends = [f'$\epsilon$-greedy $\epsilon$={meta["epsilon"]}',</span></span>
<span id="cb10-630"><a href="#cb10-630"></a><span class="in">               f'UCB c={meta["UCB"]}']</span></span>
<span id="cb10-631"><a href="#cb10-631"></a><span class="in">    fontdict = {</span></span>
<span id="cb10-632"><a href="#cb10-632"></a><span class="in">            'fontsize': 12,</span></span>
<span id="cb10-633"><a href="#cb10-633"></a><span class="in">            'fontweight': 'bold',</span></span>
<span id="cb10-634"><a href="#cb10-634"></a><span class="in">            }</span></span>
<span id="cb10-635"><a href="#cb10-635"></a><span class="in">    plt.plot(rewards[0, :], linestyle='-', linewidth=2 )</span></span>
<span id="cb10-636"><a href="#cb10-636"></a><span class="in">    plt.plot(rewards[1, :], linestyle='-', linewidth=2)</span></span>
<span id="cb10-637"><a href="#cb10-637"></a><span class="in">    plt.tick_params(axis='both', labelsize=10)</span></span>
<span id="cb10-638"><a href="#cb10-638"></a><span class="in">    plt.xlabel('step', fontdict=fontdict)</span></span>
<span id="cb10-639"><a href="#cb10-639"></a><span class="in">    plt.ylabel('reward', fontdict=fontdict)</span></span>
<span id="cb10-640"><a href="#cb10-640"></a><span class="in">    plt.legend(loc=4, fontsize=13)</span></span>
<span id="cb10-641"><a href="#cb10-641"></a><span class="in">plt.show()</span></span>
<span id="cb10-642"><a href="#cb10-642"></a></span>
<span id="cb10-643"><a href="#cb10-643"></a><span class="in">with open('./history/UCB_record.pkl', 'wb') as f:</span></span>
<span id="cb10-644"><a href="#cb10-644"></a><span class="in">    pickle.dump(record, f)</span></span>
<span id="cb10-645"><a href="#cb10-645"></a><span class="in">```</span></span>
<span id="cb10-646"><a href="#cb10-646"></a></span>
<span id="cb10-647"><a href="#cb10-647"></a><span class="fu">## Gradient Bandit method</span></span>
<span id="cb10-648"><a href="#cb10-648"></a></span>
<span id="cb10-649"><a href="#cb10-649"></a><span class="in">``` {.python filename="example_2_5_gradient.py"}</span></span>
<span id="cb10-650"><a href="#cb10-650"></a><span class="in">import numpy as np</span></span>
<span id="cb10-651"><a href="#cb10-651"></a><span class="in">import matplotlib.pyplot as plt</span></span>
<span id="cb10-652"><a href="#cb10-652"></a><span class="in">import pickle</span></span>
<span id="cb10-653"><a href="#cb10-653"></a><span class="in">import itertools</span></span>
<span id="cb10-654"><a href="#cb10-654"></a></span>
<span id="cb10-655"><a href="#cb10-655"></a><span class="in">from utils import bandit</span></span>
<span id="cb10-656"><a href="#cb10-656"></a></span>
<span id="cb10-657"><a href="#cb10-657"></a><span class="in">SEED = 50</span></span>
<span id="cb10-658"><a href="#cb10-658"></a><span class="in">np.random.seed(SEED)</span></span>
<span id="cb10-659"><a href="#cb10-659"></a></span>
<span id="cb10-660"><a href="#cb10-660"></a></span>
<span id="cb10-661"><a href="#cb10-661"></a><span class="in">def update_policy(H: np.ndarray) -&gt; np.ndarray:</span></span>
<span id="cb10-662"><a href="#cb10-662"></a><span class="in">    return np.exp(H) / np.exp(H).sum()</span></span>
<span id="cb10-663"><a href="#cb10-663"></a></span>
<span id="cb10-664"><a href="#cb10-664"></a></span>
<span id="cb10-665"><a href="#cb10-665"></a><span class="in">def update_H(</span></span>
<span id="cb10-666"><a href="#cb10-666"></a><span class="in">        H: np.ndarray,</span></span>
<span id="cb10-667"><a href="#cb10-667"></a><span class="in">        policy: np.ndarray,</span></span>
<span id="cb10-668"><a href="#cb10-668"></a><span class="in">        alpha: float,</span></span>
<span id="cb10-669"><a href="#cb10-669"></a><span class="in">        A: int,</span></span>
<span id="cb10-670"><a href="#cb10-670"></a><span class="in">        curr_reward: float,</span></span>
<span id="cb10-671"><a href="#cb10-671"></a><span class="in">        avg_reward: float</span></span>
<span id="cb10-672"><a href="#cb10-672"></a><span class="in">        ) -&gt; np.ndarray:</span></span>
<span id="cb10-673"><a href="#cb10-673"></a><span class="in">    selec = np.zeros(len(H), dtype=np.float32)</span></span>
<span id="cb10-674"><a href="#cb10-674"></a><span class="in">    selec[A] = 1.0</span></span>
<span id="cb10-675"><a href="#cb10-675"></a><span class="in">    H = H + alpha * (curr_reward - avg_reward) * (selec - policy)</span></span>
<span id="cb10-676"><a href="#cb10-676"></a><span class="in">    return H</span></span>
<span id="cb10-677"><a href="#cb10-677"></a></span>
<span id="cb10-678"><a href="#cb10-678"></a></span>
<span id="cb10-679"><a href="#cb10-679"></a><span class="in"># running the k-armed bandit algorithm</span></span>
<span id="cb10-680"><a href="#cb10-680"></a><span class="in">def run_bandit(</span></span>
<span id="cb10-681"><a href="#cb10-681"></a><span class="in">        K: int,</span></span>
<span id="cb10-682"><a href="#cb10-682"></a><span class="in">        q_star: np.ndarray,</span></span>
<span id="cb10-683"><a href="#cb10-683"></a><span class="in">        rewards: np.ndarray,</span></span>
<span id="cb10-684"><a href="#cb10-684"></a><span class="in">        optim_acts_ratio: np.ndarray,</span></span>
<span id="cb10-685"><a href="#cb10-685"></a><span class="in">        alpha: float,</span></span>
<span id="cb10-686"><a href="#cb10-686"></a><span class="in">        baseline: bool,</span></span>
<span id="cb10-687"><a href="#cb10-687"></a><span class="in">        num_steps: int = 1000</span></span>
<span id="cb10-688"><a href="#cb10-688"></a><span class="in">        ) -&gt; None:</span></span>
<span id="cb10-689"><a href="#cb10-689"></a><span class="in">    H = np.zeros(K, dtype=np.float32)  # initialize preference</span></span>
<span id="cb10-690"><a href="#cb10-690"></a><span class="in">    policy = np.ones(K, dtype=np.float32) / K</span></span>
<span id="cb10-691"><a href="#cb10-691"></a><span class="in">    ttl_reward = 0</span></span>
<span id="cb10-692"><a href="#cb10-692"></a><span class="in">    ttl_optim_acts = 0</span></span>
<span id="cb10-693"><a href="#cb10-693"></a><span class="in">    </span></span>
<span id="cb10-694"><a href="#cb10-694"></a><span class="in">    for i in range(num_steps):</span></span>
<span id="cb10-695"><a href="#cb10-695"></a><span class="in">        </span></span>
<span id="cb10-696"><a href="#cb10-696"></a><span class="in">        A = np.random.choice(np.arange(K), p=policy)</span></span>
<span id="cb10-697"><a href="#cb10-697"></a><span class="in">        reward, is_optim = bandit(q_star, A)</span></span>
<span id="cb10-698"><a href="#cb10-698"></a><span class="in">        avg_reward = 0</span></span>
<span id="cb10-699"><a href="#cb10-699"></a><span class="in">        </span></span>
<span id="cb10-700"><a href="#cb10-700"></a><span class="in">        if baseline:</span></span>
<span id="cb10-701"><a href="#cb10-701"></a><span class="in">            # Get average reward unitl timestep=i</span></span>
<span id="cb10-702"><a href="#cb10-702"></a><span class="in">            avg_reward = ttl_reward / i if i &gt; 0 else reward</span></span>
<span id="cb10-703"><a href="#cb10-703"></a><span class="in">        </span></span>
<span id="cb10-704"><a href="#cb10-704"></a><span class="in">        # Update preference and policy</span></span>
<span id="cb10-705"><a href="#cb10-705"></a><span class="in">        H = update_H(H, policy, alpha, A, reward, avg_reward)</span></span>
<span id="cb10-706"><a href="#cb10-706"></a><span class="in">        policy = update_policy(H)</span></span>
<span id="cb10-707"><a href="#cb10-707"></a><span class="in">        </span></span>
<span id="cb10-708"><a href="#cb10-708"></a><span class="in">        ttl_reward += reward</span></span>
<span id="cb10-709"><a href="#cb10-709"></a><span class="in">        ttl_optim_acts += is_optim</span></span>
<span id="cb10-710"><a href="#cb10-710"></a><span class="in">        rewards[i] = reward</span></span>
<span id="cb10-711"><a href="#cb10-711"></a><span class="in">        optim_acts_ratio[i] = ttl_optim_acts / (i + 1)</span></span>
<span id="cb10-712"><a href="#cb10-712"></a></span>
<span id="cb10-713"><a href="#cb10-713"></a></span>
<span id="cb10-714"><a href="#cb10-714"></a><span class="in">if __name__ == "__main__":</span></span>
<span id="cb10-715"><a href="#cb10-715"></a><span class="in">    </span></span>
<span id="cb10-716"><a href="#cb10-716"></a><span class="in">    # Initializing the hyperparameters</span></span>
<span id="cb10-717"><a href="#cb10-717"></a><span class="in">    K = 10  # Number of arms</span></span>
<span id="cb10-718"><a href="#cb10-718"></a><span class="in">    alphas = [0.1, 0.4]</span></span>
<span id="cb10-719"><a href="#cb10-719"></a><span class="in">    baselines = [False, True]</span></span>
<span id="cb10-720"><a href="#cb10-720"></a><span class="in">    hyper_params = list(itertools.product(baselines, alphas))</span></span>
<span id="cb10-721"><a href="#cb10-721"></a><span class="in">    </span></span>
<span id="cb10-722"><a href="#cb10-722"></a><span class="in">    num_steps = 1000</span></span>
<span id="cb10-723"><a href="#cb10-723"></a><span class="in">    total_rounds = 2000</span></span>
<span id="cb10-724"><a href="#cb10-724"></a><span class="in">    </span></span>
<span id="cb10-725"><a href="#cb10-725"></a><span class="in">    rewards = np.zeros(shape=(len(hyper_params), total_rounds, num_steps))</span></span>
<span id="cb10-726"><a href="#cb10-726"></a><span class="in">    optim_acts_ratio = np.zeros(</span></span>
<span id="cb10-727"><a href="#cb10-727"></a><span class="in">        shape=(len(hyper_params), total_rounds, num_steps)</span></span>
<span id="cb10-728"><a href="#cb10-728"></a><span class="in">        )</span></span>
<span id="cb10-729"><a href="#cb10-729"></a><span class="in">    q_star = np.random.normal(loc=4.0, scale=1.0, size=K)</span></span>
<span id="cb10-730"><a href="#cb10-730"></a><span class="in">    </span></span>
<span id="cb10-731"><a href="#cb10-731"></a><span class="in">    print(hyper_params)</span></span>
<span id="cb10-732"><a href="#cb10-732"></a><span class="in">    for i, (is_baseline, alpha) in enumerate(hyper_params):</span></span>
<span id="cb10-733"><a href="#cb10-733"></a><span class="in">        for curr_round in range(total_rounds):</span></span>
<span id="cb10-734"><a href="#cb10-734"></a><span class="in">            run_bandit(</span></span>
<span id="cb10-735"><a href="#cb10-735"></a><span class="in">                K,</span></span>
<span id="cb10-736"><a href="#cb10-736"></a><span class="in">                q_star,</span></span>
<span id="cb10-737"><a href="#cb10-737"></a><span class="in">                rewards[i, curr_round],</span></span>
<span id="cb10-738"><a href="#cb10-738"></a><span class="in">                optim_acts_ratio[i, curr_round],</span></span>
<span id="cb10-739"><a href="#cb10-739"></a><span class="in">                alpha,</span></span>
<span id="cb10-740"><a href="#cb10-740"></a><span class="in">                is_baseline,</span></span>
<span id="cb10-741"><a href="#cb10-741"></a><span class="in">                num_steps</span></span>
<span id="cb10-742"><a href="#cb10-742"></a><span class="in">                )</span></span>
<span id="cb10-743"><a href="#cb10-743"></a><span class="in">    </span></span>
<span id="cb10-744"><a href="#cb10-744"></a><span class="in">    optim_acts_ratio = optim_acts_ratio.mean(axis=1)</span></span>
<span id="cb10-745"><a href="#cb10-745"></a><span class="in">    </span></span>
<span id="cb10-746"><a href="#cb10-746"></a><span class="in">    for val in optim_acts_ratio:</span></span>
<span id="cb10-747"><a href="#cb10-747"></a><span class="in">        plt.plot(val)</span></span>
<span id="cb10-748"><a href="#cb10-748"></a><span class="in">    plt.show()</span></span>
<span id="cb10-749"><a href="#cb10-749"></a><span class="in">    </span></span>
<span id="cb10-750"><a href="#cb10-750"></a><span class="in">    record = {</span></span>
<span id="cb10-751"><a href="#cb10-751"></a><span class="in">            'hyper_params': hyper_params,</span></span>
<span id="cb10-752"><a href="#cb10-752"></a><span class="in">            'optim_acts_ratio': optim_acts_ratio</span></span>
<span id="cb10-753"><a href="#cb10-753"></a><span class="in">            }</span></span>
<span id="cb10-754"><a href="#cb10-754"></a><span class="in">    </span></span>
<span id="cb10-755"><a href="#cb10-755"></a><span class="in">    with open('./history/sga_record.pkl', 'wb') as f:</span></span>
<span id="cb10-756"><a href="#cb10-756"></a><span class="in">         pickle.dump(record, f)</span></span>
<span id="cb10-757"><a href="#cb10-757"></a><span class="in">```</span></span>
<span id="cb10-758"><a href="#cb10-758"></a></span>
<span id="cb10-759"><a href="#cb10-759"></a><span class="fu">## Scripts for visualization</span></span>
<span id="cb10-760"><a href="#cb10-760"></a></span>
<span id="cb10-761"><a href="#cb10-761"></a>Run the following script and save the output.</span>
<span id="cb10-762"><a href="#cb10-762"></a></span>
<span id="cb10-763"><a href="#cb10-763"></a><span class="in">``` {.python filename="plot_gradient.py"}</span></span>
<span id="cb10-764"><a href="#cb10-764"></a></span>
<span id="cb10-765"><a href="#cb10-765"></a><span class="in">import matplotlib.pyplot as plt</span></span>
<span id="cb10-766"><a href="#cb10-766"></a><span class="in">import pickle</span></span>
<span id="cb10-767"><a href="#cb10-767"></a><span class="in">import numpy as np</span></span>
<span id="cb10-768"><a href="#cb10-768"></a></span>
<span id="cb10-769"><a href="#cb10-769"></a></span>
<span id="cb10-770"><a href="#cb10-770"></a><span class="in"># Plot results</span></span>
<span id="cb10-771"><a href="#cb10-771"></a><span class="in">def plot(</span></span>
<span id="cb10-772"><a href="#cb10-772"></a><span class="in">        data: np.ndarray,</span></span>
<span id="cb10-773"><a href="#cb10-773"></a><span class="in">        legends: list,</span></span>
<span id="cb10-774"><a href="#cb10-774"></a><span class="in">        xlabel: str,</span></span>
<span id="cb10-775"><a href="#cb10-775"></a><span class="in">        ylabel: str,</span></span>
<span id="cb10-776"><a href="#cb10-776"></a><span class="in">        filename: str = None,</span></span>
<span id="cb10-777"><a href="#cb10-777"></a><span class="in">        fn=lambda: None, ) -&gt; None:</span></span>
<span id="cb10-778"><a href="#cb10-778"></a><span class="in">    fontdict = {</span></span>
<span id="cb10-779"><a href="#cb10-779"></a><span class="in">            'fontsize': 12,</span></span>
<span id="cb10-780"><a href="#cb10-780"></a><span class="in">            'fontweight': 'bold',</span></span>
<span id="cb10-781"><a href="#cb10-781"></a><span class="in">            }</span></span>
<span id="cb10-782"><a href="#cb10-782"></a><span class="in">    </span></span>
<span id="cb10-783"><a href="#cb10-783"></a><span class="in">    plt.figure(figsize=(10, 6), dpi=150)</span></span>
<span id="cb10-784"><a href="#cb10-784"></a><span class="in">    plt.grid(c='lightgray')</span></span>
<span id="cb10-785"><a href="#cb10-785"></a><span class="in">    plt.margins(0.02)</span></span>
<span id="cb10-786"><a href="#cb10-786"></a><span class="in">    </span></span>
<span id="cb10-787"><a href="#cb10-787"></a><span class="in">    # revers the loop for a better visualization</span></span>
<span id="cb10-788"><a href="#cb10-788"></a><span class="in">    colors = ['navy', 'lightblue', 'tomato', 'pink']</span></span>
<span id="cb10-789"><a href="#cb10-789"></a><span class="in">    for i in range(len(data) - 1, -1, -1):</span></span>
<span id="cb10-790"><a href="#cb10-790"></a><span class="in">        # data[i] = uniform_filter(data[i])</span></span>
<span id="cb10-791"><a href="#cb10-791"></a><span class="in">        plt.plot(data[i], label=legends[i], linewidth=1.5, c=colors[i])</span></span>
<span id="cb10-792"><a href="#cb10-792"></a><span class="in">    </span></span>
<span id="cb10-793"><a href="#cb10-793"></a><span class="in">    # get rid of the top/right frame lines</span></span>
<span id="cb10-794"><a href="#cb10-794"></a><span class="in">    for i, spine in enumerate(plt.gca().spines.values()):</span></span>
<span id="cb10-795"><a href="#cb10-795"></a><span class="in">        if i in [0, 2]:</span></span>
<span id="cb10-796"><a href="#cb10-796"></a><span class="in">            spine.set_linewidth(1.5)</span></span>
<span id="cb10-797"><a href="#cb10-797"></a><span class="in">            continue</span></span>
<span id="cb10-798"><a href="#cb10-798"></a><span class="in">        spine.set_visible(False)</span></span>
<span id="cb10-799"><a href="#cb10-799"></a><span class="in">    </span></span>
<span id="cb10-800"><a href="#cb10-800"></a><span class="in">    plt.tick_params(axis='both', labelsize=10)</span></span>
<span id="cb10-801"><a href="#cb10-801"></a><span class="in">    plt.xlabel(xlabel, fontdict=fontdict)</span></span>
<span id="cb10-802"><a href="#cb10-802"></a><span class="in">    plt.ylabel(ylabel, fontdict=fontdict)</span></span>
<span id="cb10-803"><a href="#cb10-803"></a><span class="in">    # plt.legend(loc=4, fontsize=13)</span></span>
<span id="cb10-804"><a href="#cb10-804"></a><span class="in">    fn()</span></span>
<span id="cb10-805"><a href="#cb10-805"></a><span class="in">    </span></span>
<span id="cb10-806"><a href="#cb10-806"></a><span class="in">    plt.text(500, 57, s="$\\alpha = 0.4$", c=colors[3], fontsize=14)</span></span>
<span id="cb10-807"><a href="#cb10-807"></a><span class="in">    plt.text(500, 28, s="$\\alpha = 0.4$", c=colors[1], fontsize=14)</span></span>
<span id="cb10-808"><a href="#cb10-808"></a><span class="in">    plt.text(900, 72, s="$\\alpha = 0.1$", c=colors[2], fontsize=14)</span></span>
<span id="cb10-809"><a href="#cb10-809"></a><span class="in">    plt.text(900, 52, s="$\\alpha = 0.1$", c=colors[0], fontsize=14)</span></span>
<span id="cb10-810"><a href="#cb10-810"></a><span class="in">    </span></span>
<span id="cb10-811"><a href="#cb10-811"></a><span class="in">    plt.text(770, 65, s="with baseline", c=colors[2], fontsize=12)</span></span>
<span id="cb10-812"><a href="#cb10-812"></a><span class="in">    plt.text(770, 42, s="without baseline", c=colors[0], fontsize=12)</span></span>
<span id="cb10-813"><a href="#cb10-813"></a><span class="in">    </span></span>
<span id="cb10-814"><a href="#cb10-814"></a><span class="in">    if not filename:</span></span>
<span id="cb10-815"><a href="#cb10-815"></a><span class="in">        plt.show()</span></span>
<span id="cb10-816"><a href="#cb10-816"></a><span class="in">    else:</span></span>
<span id="cb10-817"><a href="#cb10-817"></a><span class="in">        plt.savefig(f'./plots/{filename}')</span></span>
<span id="cb10-818"><a href="#cb10-818"></a></span>
<span id="cb10-819"><a href="#cb10-819"></a></span>
<span id="cb10-820"><a href="#cb10-820"></a><span class="in">def plot_result(</span></span>
<span id="cb10-821"><a href="#cb10-821"></a><span class="in">        optim_ratio: np.ndarray,</span></span>
<span id="cb10-822"><a href="#cb10-822"></a><span class="in">        legends: list,</span></span>
<span id="cb10-823"><a href="#cb10-823"></a><span class="in">        output_name: str = None</span></span>
<span id="cb10-824"><a href="#cb10-824"></a><span class="in">        ):</span></span>
<span id="cb10-825"><a href="#cb10-825"></a><span class="in">    # Set tick labels</span></span>
<span id="cb10-826"><a href="#cb10-826"></a><span class="in">    fn = lambda: plt.yticks(</span></span>
<span id="cb10-827"><a href="#cb10-827"></a><span class="in">        np.arange(0, 100, 10), labels=[f'{val}%' for val in range(0, 100, 10)]</span></span>
<span id="cb10-828"><a href="#cb10-828"></a><span class="in">        )</span></span>
<span id="cb10-829"><a href="#cb10-829"></a><span class="in">    plot(</span></span>
<span id="cb10-830"><a href="#cb10-830"></a><span class="in">        optim_ratio,</span></span>
<span id="cb10-831"><a href="#cb10-831"></a><span class="in">        legends,</span></span>
<span id="cb10-832"><a href="#cb10-832"></a><span class="in">        xlabel='Time step',</span></span>
<span id="cb10-833"><a href="#cb10-833"></a><span class="in">        ylabel='% Optimal actions',</span></span>
<span id="cb10-834"><a href="#cb10-834"></a><span class="in">        filename=output_name,</span></span>
<span id="cb10-835"><a href="#cb10-835"></a><span class="in">        fn=fn</span></span>
<span id="cb10-836"><a href="#cb10-836"></a><span class="in">        )</span></span>
<span id="cb10-837"><a href="#cb10-837"></a></span>
<span id="cb10-838"><a href="#cb10-838"></a></span>
<span id="cb10-839"><a href="#cb10-839"></a><span class="in">if __name__ == "__main__":</span></span>
<span id="cb10-840"><a href="#cb10-840"></a><span class="in">    with open('./history/sga_record.pkl', 'rb') as f:</span></span>
<span id="cb10-841"><a href="#cb10-841"></a><span class="in">        history = pickle.load(f)</span></span>
<span id="cb10-842"><a href="#cb10-842"></a><span class="in">    </span></span>
<span id="cb10-843"><a href="#cb10-843"></a><span class="in">    optim_ratio = history['optim_acts_ratio'] * 100</span></span>
<span id="cb10-844"><a href="#cb10-844"></a><span class="in">    hyper_params = history['hyper_params']</span></span>
<span id="cb10-845"><a href="#cb10-845"></a><span class="in">    </span></span>
<span id="cb10-846"><a href="#cb10-846"></a><span class="in">    # plot_result(optim_ratio, hyper_params, output_name="example_2_5_sga.png")</span></span>
<span id="cb10-847"><a href="#cb10-847"></a><span class="in">    plot_result(optim_ratio, hyper_params, output_name=None)</span></span>
<span id="cb10-848"><a href="#cb10-848"></a><span class="in">    </span></span>
<span id="cb10-849"><a href="#cb10-849"></a><span class="in">```</span></span>
<span id="cb10-850"><a href="#cb10-850"></a></span>
<span id="cb10-851"><a href="#cb10-851"></a>{{&lt; include summary_mab.qmd &gt;}}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>This notes for RL, are the first draft of for the course: From Markov Decision Processes to Reinforcement Learning</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/SaulDiazInfante/RL-Course-2024-2.git/edit/main/03-multiArmedBandit/multiarmed_bandits.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/SaulDiazInfante/RL-Course-2024-2.git/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>