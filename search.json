[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "From Markov Decision Processes to Reinforcement Learning with Python",
    "section": "",
    "text": "Preface\nThis notes are based in the course from Berstekas for the MIT see all lectures and other resources for complete the understanding.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "From Markov Decision Processes to Reinforcement Learning with Python",
    "section": "Abstract",
    "text": "Abstract\nWe present an introduction to the solution of multi-stage optimization problems. Starting from the dynamic programming algorithm, we consider theoretical and computational aspects, mainly of deterministic problems, and discuss how to generalize some of the results to Markovian decision processes.\nThe classic problem with which the essential ideas of dynamic programming can be introduced is the optimal route problem (shortest route considering distances, or cheapest route considering costs) (Bertsekas 2005; Brandimarte 2013). A simple version of this problem is shown schematically in Fig 1. By exhaustion, we deploy the cost of path in Table 1.\n\n\n\n\n\n\nFigure 1: The shortest path\n\n\n\nIn this problem, the optimal route to go from the node labeled \\(0\\) to the node labeled \\(7\\) is to be determined. The costs between each pair of nodes connected by an arrow are represented by a number next to it. For example, the cost to go from node 0 to node 2 is 6. The optimal route will be the one for which the sum of their costs is minimum. This optimal route can be obtained by an exhaustive enumeration; generating all possible routes from node \\(0\\) to node \\(7\\) and choosing the minimum cost route.\n\n\n\nTable 1: The shortest path by exhaustion see (Brandimarte 2013).\n\n\n\n\n\nPath\nCost\n\n\n\n\n\\(0 \\to 1 \\to 4 \\to 7\\)\n18\n\n\n\\(0 \\to 1 \\to 3 \\to 4 \\to 7\\)\n22\n\n\n\\(0 \\to 1 \\to 3 \\to 6 \\to 7\\)\n18\n\n\n\\(0 \\to 1 \\to 3 \\to 5 \\to 6 \\to 7\\)\n20\n\n\n\\(\\mathbf{0 \\to 1 \\to 3 \\to 5 \\to 7}\\)\n16\n\n\n\\(0 \\to 2 \\to 3 \\to 4 \\to 7\\)\n23\n\n\n\\(0 \\to 2 \\to 3 \\to 6 \\to 7\\)\n19\n\n\n\\(0 \\to 2 \\to 3 \\to 5 \\to 6 \\to 7\\)\n21\n\n\n\\(0 \\to 2 \\to 3 \\to 5 \\to 7\\)\n17\n\n\n\\(0 \\to 2 \\to 5 \\to 6 \\to 7\\)\n22\n\n\n\\(0 \\to 2 \\to 5 \\to 7\\)\n18\n\n\n\n\n\n\nThe optimal path is \\[\n  0 \\to   1 \\to 3 \\to 5 \\to  7,\n\\] with cost 16.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-introduction/general_intro.html",
    "href": "01-introduction/general_intro.html",
    "title": "Abstract",
    "section": "",
    "text": "We present an introduction to the solution of multi-stage optimization problems. Starting from the dynamic programming algorithm, we consider theoretical and computational aspects, mainly of deterministic problems, and discuss how to generalize some of the results to Markovian decision processes.\nThe classic problem with which the essential ideas of dynamic programming can be introduced is the optimal route problem (shortest route considering distances, or cheapest route considering costs) (Bertsekas 2005; Brandimarte 2013). A simple version of this problem is shown schematically in Fig fig-brandimarte_net. By exhaustion, we deploy the cost of path in Table tbl-brandimarte_paths.\n\n\n\n\n\n\nFigure 1: The shortest path\n\n\n\nIn this problem, the optimal route to go from the node labeled \\(0\\) to the node labeled \\(7\\) is to be determined. The costs between each pair of nodes connected by an arrow are represented by a number next to it. For example, the cost to go from node 0 to node 2 is 6. The optimal route will be the one for which the sum of their costs is minimum. This optimal route can be obtained by an exhaustive enumeration; generating all possible routes from node \\(0\\) to node \\(7\\) and choosing the minimum cost route.\n\n\n\nTable 1: The shortest path by exhaustion see (Brandimarte 2013).\n\n\n\n\n\nPath\nCost\n\n\n\n\n\\(0 \\to 1 \\to 4 \\to 7\\)\n18\n\n\n\\(0 \\to 1 \\to 3 \\to 4 \\to 7\\)\n22\n\n\n\\(0 \\to 1 \\to 3 \\to 6 \\to 7\\)\n18\n\n\n\\(0 \\to 1 \\to 3 \\to 5 \\to 6 \\to 7\\)\n20\n\n\n\\(\\mathbf{0 \\to 1 \\to 3 \\to 5 \\to 7}\\)\n16\n\n\n\\(0 \\to 2 \\to 3 \\to 4 \\to 7\\)\n23\n\n\n\\(0 \\to 2 \\to 3 \\to 6 \\to 7\\)\n19\n\n\n\\(0 \\to 2 \\to 3 \\to 5 \\to 6 \\to 7\\)\n21\n\n\n\\(0 \\to 2 \\to 3 \\to 5 \\to 7\\)\n17\n\n\n\\(0 \\to 2 \\to 5 \\to 6 \\to 7\\)\n22\n\n\n\\(0 \\to 2 \\to 5 \\to 7\\)\n18\n\n\n\n\n\n\nThe optimal path is \\[\n  0 \\to   1 \\to 3 \\to 5 \\to  7,\n\\] with cost 16.\n\n\n\n\n\nFigure 1: The shortest path\n\n\n\nBertsekas, Dimitri P. 2005. Dynamic Programming and Optimal Control. Vol. I. Third. Athena Scientific, Belmont, MA.\n\n\nBrandimarte, Paolo. 2013. Numerical Methods in Finance and Economics: A MATLAB-Based Introduction. 2nd ed. Hoboken, New Jersey: John Wiley & Sons.",
    "crumbs": [
      "Abstract"
    ]
  },
  {
    "objectID": "02-introductionToRL/intro.html",
    "href": "02-introductionToRL/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Trial-Error\nAccording to Richard S. Sutton and Andrew G. Barto Sutton and Barto (2018)–the first authors to use the term–Reinforced Learning Reinforcement learning is about what to do, that is, how to map situations to action so that we optimize a reward. The learner must discover which action yield the best reward by trying them. In the most general sense, action may not only affect immediate reward but also the next situation and, through that, all subsequent rewards.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-introductionToRL/intro.html#trial-error",
    "href": "02-introductionToRL/intro.html#trial-error",
    "title": "Introduction",
    "section": "",
    "text": "Sensation action and goal\nAt the same time, Reinforcement Learning encloses a problem, a class of solution methods, and the field that studies this problem and its solutions. Its formalism is based on the theory of controlled dynamical systems, with a strong focus on the optimal control of partially known Markov decision processes. Then, the core idea consists of capturing the essence of the problem when an agent learns through experience and interaction to reach a goal. This agent can sense the state of its environment to some extent and must be able to take action that affects the state. The agent also must have a goal or goals related to the state of the environment.\nMDPs are designed to incorporate three essential elements: sensation, action, and goal. Therefore, any approach suitable for solving such problems should be considered a potential method for Reinforcement Learning.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-introductionToRL/intro.html#exploration-exploration-dilemma-and-uncertainty",
    "href": "02-introductionToRL/intro.html#exploration-exploration-dilemma-and-uncertainty",
    "title": "Introduction",
    "section": "Exploration-exploration dilemma and uncertainty",
    "text": "Exploration-exploration dilemma and uncertainty\nTo obtain the best reward, the agent must prefer actions used in the past and perceived as effective to produce a reward. However, to discover such actions, the agent must try actions never used before. So, there is a delicate trade-off between exploiting and exploring. The agent has to exploit its knowledge to produce a reward but simultaneously has to explore to improve its reward in the future. Here, our dilemma is that neither exploration nor exploitation can be pursued exclusively without failing the task.\nAnother essential aspect of reinforcement learning is that it specifically deals with the entire process of a goal-directed agent interacting with an uncertain environment. This aspect differs from many approaches that only focus on subproblems rather than considering how they might contribute to the bigger picture. For example, many machine learning researchers have studied supervised Learning without specifying how such an ability would ultimately be helpful. Other researchers have developed planning theories with general goals without considering planning’s role in real-time decision-making or whether the predictive models necessary for planning are well suited. Although these approaches have produced valuable results, their focus on isolated subproblems leads to significant limitations.\nReinforcement learning takes the opposite approach, beginning with a fully interactive, goal-seeking agent. In reinforcement learning, the agent has explicit goals, can sense aspects of their environment, and can choose actions to influence its environment.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-introductionToRL/intro.html#examples",
    "href": "02-introductionToRL/intro.html#examples",
    "title": "Introduction",
    "section": "Examples",
    "text": "Examples\n\nA master chess player makes a move.\nAn adaptive controller adjusts parameters of a petroleum refinery’s operation in real time.\nA gazelle calf struggles to its feet minutes after being born.\nA mobile robot decides whether it should enter a new room in search of more trash to collect or start trying to find its way back to its battery recharging station.\nPhil prepares his breakfast",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-introductionToRL/intro.html#the-possible-4-elements-of-reinforcement-learning",
    "href": "02-introductionToRL/intro.html#the-possible-4-elements-of-reinforcement-learning",
    "title": "Introduction",
    "section": "The (possible) 4 elements of Reinforcement Learning",
    "text": "The (possible) 4 elements of Reinforcement Learning\nGiven an agent, we identify four main element in a reinforcement learning model:\n\na policy, a reward, a value function and (optionally) a model of the environment.\n\n\nPolicy\n\nA policy is as a set of actions that guide the agent to respond according to its perception of the environment. It’s like a set of instructions that tell the agent what to do when it encounters a certain situation. In general, policies may be stochastic, specifying probabilities for each action.\n\nReward\n\nThe reward signal thus defines what are the good and bad events for the agent. In a biological system, we might think of rewards as analogous to the experiences of pleasure or pain. They are the immediate and defining features of the problem faced by the agent. The reward signal is the primary basis for altering the policy; if an action selected by the policy is followed by low reward, then the policy may be changed to select some other action in that situation in the future. In general, reward signals may be stochastic functions of the state of the environment and the actions taken.\n\nValue function\n\nWhereas the reward signal indicates what is good in the immediate sense, a value function specifies what is good in the long run. In simple terms, the value of a state represents the total reward an agent can anticipate to receive in the future, beginning from that state. While rewards reflect the immediate appeal of environmental states, values signify the long-term appeal of states, considering the potential future states and the rewards they offer. For example, a state might consistently yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards. Alternatively, the opposite could also be true. Rewards can be compared to pleasure (when high) and pain (when low). At the same time, values represent a more precise and long-term assessment of how satisfied or dissatisfied we are with the state of our environment. In a sense, rewards are primary, whereas values, as predictions of rewards, are secondary. Without rewards, there could be no values, and the only purpose of estimating values is to achieve more rewards.\nAction choices are made based on value judgments. We seek actions that bring about states of highest value, not highest reward, because these actions obtain the greatest amount of reward for us over the long run. In fact, the most important component of almost all reinforcement learning algorithms we consider is a method for efficiently estimating values.\n\nEnvironment model\n\nThe environment model is something that mimics the behavior of the environment or, more generally, that allows inferences to be made about how the environment will behave. For example, given a state and action, the model might predict the resultant next state and next reward. Models are used for planning. This means making decisions by considering potential future situations before they occur. For our purposes, the environment can be represented as a dynamic system through an ordinary differential equation or a discrete finite difference equation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-introductionToRL/intro.html#a-toy-rl-exmaple-tic-tac-toe",
    "href": "02-introductionToRL/intro.html#a-toy-rl-exmaple-tic-tac-toe",
    "title": "Introduction",
    "section": "A toy RL-exmaple: Tic-Tac-Toe",
    "text": "A toy RL-exmaple: Tic-Tac-Toe\nTo illustrate the general idea of reinforcement learning and contrast it with other ap- proaches, we next consider a single example in more detail.\nConsider the familiar child’s game of tic-tac-toe.\n\nAlthough the tic-tac-toe game is a simple problem, it cannot be satisfactorily solved using classical techniques.\nFor instance, the classical “minimax” solution from game theory is not applicable here because it assumes the opponent’s specific way of playing. A minimax player would never reach a game state from which it could lose. Even if, in reality, it always won from that state due to incorrect play by the opponent. The classical optimization methods for sequential decision problems, like dynamic programming, can find the best solution for any opponent. However, these methods need a detailed description of the opponent as input, including the probabilities of the opponent’s moves in each board state.\nAlternatively, this information can be estimated through experience, such as playing numerous games against the opponent. The best approach to this problem is to first learn a model of the opponent’s behavior with a certain level of confidence, and then use dynamic programming to calculate an optimal solution based on the approximate opponent model.\nSutton and Barto (see pp. 9-12 Sutton and Barto 2018) propose the following way to approach tic tac toe with Reinforcement Learning:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-introductionToRL/intro.html#setup",
    "href": "02-introductionToRL/intro.html#setup",
    "title": "Introduction",
    "section": "Setup:",
    "text": "Setup:\n\nFirst we would set up a table of numbers (or labels), one for each possible state of the game.\nEach number will be the latest estimate of the probability of our winning from that state.\nWe treat this estimate as the state’s value, and the whole table is the learned value function.\nState \\(A\\) has higher value than state \\(B\\), or is considered ‘better’ than state \\(B\\), if the current estimate of the probability of our winning from \\(A\\) is higher than it is from \\(B\\).\nIf we always play \\(Xs\\), then for all states with three \\(Xs\\) in a row the probability of winning is 1, because we have already won.\nSimilarly, for all states with three \\(Os\\) in a row, or that are filled up, the correct probability is 0–we cannot win from them.\nWe set the initial values of all the other states to \\(0.5\\), representing a guess that we have a 50% chance of winning.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-introductionToRL/intro.html#training",
    "href": "02-introductionToRL/intro.html#training",
    "title": "Introduction",
    "section": "Training:",
    "text": "Training:\nWe then play many games against the opponent.\nTo select our moves we examine the states that would result from each of our possible moves (one for each blank space on the board) and look up their current values in the table. Most of the time we move greedily,selecting the move that leads to the state with greatest value, that is, with the highest estimated probability of winning.\nOccasionally, however, we select randomly from among the other moves instead. These are called exploratory moves because they cause us to experience states that we might otherwise never see.\nA sequence of moves made and considered during a game can be diagrammed as the following figure:\n\n\n\n\n\n\nFigure 1: Figure taken from (Sutton and Barto 2018). A sequence of tic-tac-toe moves. Solid black lines represents moves taken during a play. Dashed lines represent plausible but not taken moves. The \\(*\\) symbol indicates the move currently estimated to be the best. Thus the move \\(e\\) denotes an exploratory move\n\n\n\n\n\n\n\n\nFigure 1: Figure taken from (Sutton and Barto 2018). A sequence of tic-tac-toe moves. Solid black lines represents moves taken during a play. Dashed lines represent plausible but not taken moves. The \\(*\\) symbol indicates the move currently estimated to be the best. Thus the move \\(e\\) denotes an exploratory move\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. Adaptive Computation and Machine Learning. MIT Press, Cambridge, MA.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html",
    "href": "03-multiArmedBandit/multiarmed_bandits.html",
    "title": "Multi-armed Bandits",
    "section": "",
    "text": "Multi-armed Bandits\nA very important feature distinguishing reinforcement learning from other types of learning is that it uses training information to evaluate the actions taken, rather than instruct by giving correct actions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#a-k-armed-bandit-problem",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#a-k-armed-bandit-problem",
    "title": "Multi-armed Bandits",
    "section": "A \\(k\\)-armed Bandit Problem",
    "text": "A \\(k\\)-armed Bandit Problem\nWe consider the following setup:\n\nYou repeatedly face a choice among \\(k\\) different options or actions.\nAfter a choice, you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected\nYour goal is to maximize the total expected reward over a specific time period, such as 1000 action selections or time steps. The problem is named by analogy to a slot machine, or one-armed bandit, except that it has \\(k\\) levers instead of one.\n\nWe denote the action selected on time step \\(t\\) as \\(A_t\\) and the corresponding reward as \\(R_t\\). Each of the \\(k\\) actions has an expected or mean reward given that that action is selected; let us call this the value of that action.\nThe value then of an arbitrary action \\(a\\), denoted \\(q_{*} (a)\\), is the expected reward given that \\(a\\) is selected:\n\\[\n    q_{*}(a): = \\mathbb{E} \\left[ R_t | A_t =a\\right].\n\\]\nIf you knew the value of each action, then we solve the \\(k\\)-armed bandit problem—you would always select the action with highest value.\nWe assume that you may not have precise knowledge of the action values, although you may have some estimates. We denote this estimated value of action \\(a\\) at time step \\(t\\) as \\(Q_t(a)\\). Thus, we would like that \\[\n    Q_t(a) \\approx q_{*}(a).\n\\]\nIf you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are exploiting your current knowledge of the values of the actions. If instead you select one of the non-greedy actions, then we say you are exploring, because this enables you to improve your estimate of the non-greedy action’s value.\nExploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.\nReward is lower in the short run, during exploration, but higher in the long run because after you have discovered the better actions, you can exploit them many times. Because it is not possible both to explore and to exploit with any single action selection, one often refers to the “conflict” between exploration and exploitation.\nIn any specific case, whether it is better to explore or exploit depends in a complex way on the precise values of the estimates, uncertainties, and the number of remaining steps. There are many sophisticated methods for balancing exploration and exploitation for particular mathematical formulations of the \\(k\\)-armed bandit and related problems.\nHowever, most of these methods make strong assumptions about stationary and prior knowledge that are either violated or impossible to verify in most applications.\nThe guarantees of optimality or bounded loss for these methods offer little comfort when the assumptions of their theory do not apply.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#action-value-methods",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#action-value-methods",
    "title": "Multi-armed Bandits",
    "section": "Action-value Methods",
    "text": "Action-value Methods\nOne natural way to estimate the value of a given action is by averaging the rewards actually received. In mathematical symbols reads\n\\[\n  Q_t(a):=\n    \\dfrac{\n      \\sum_{i=1}^{t-1}\n        R_i \\cdot \\mathbb{1}_{A_{i} = a}\n    }{\\sum_{i=1}^{t-1} \\mathbb{1}_{A_i=a}} .\n\\tag{1.1}\\]\nNext we understand as greedy action as the action that results from \\[\n  A_t := \\underset{a}{\\mathrm{argmax}} \\ Q_t(a).\n\\tag{1.2}\\]\nGreedy action selection always exploits current knowledge to maximize immediate reward. It also only spends time sampling apparently superior actions. A simple alternative is to behave greedily but occasionally, with a small \\(\\epsilon\\)-probability, select randomly from all the actions with equal probability, regardless of the action-value estimates. We call methods using this near-greedy action selection rule \\(\\epsilon\\)-greedy methods.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#the-10-armed-testbed",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#the-10-armed-testbed",
    "title": "Multi-armed Bandits",
    "section": "The 10-armed Testbed",
    "text": "The 10-armed Testbed\nTo evaluate the relative effectiveness of the greedy and \\(\\epsilon\\)-greedy action-value methods, we compared them numerically on a suite of test problems.\n\nSet up\n\n\n\n\n\n\nThe experiment runs as follows.\n\n\n\n\nConsider a \\(k\\)-bandit problem with \\(k=10\\)\nFor each bandit problem, the action values\n\n\\[\n  q_{*}(a) \\sim \\mathcal{N}(0,1)\n\\]\n\nThen when choosing an action \\(A_t\\) the corresponding reward \\(R_t\\) is sampling from a Gaussian distribution \\[\nR_t \\sim \\mathcal{N}(q_{*}(A_t), 1)  \n\\]\n\n\n\n\n\nk_armed_testbed.py\n\n#k_armed_testbed.py\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\n# Randomly sample mean reward for each action\nmeans = np.random.normal(size=(10, ))\n\n# Generate sample data based on normal distribution\ndata = [np.random.normal(mean, 1.0, 2000) for mean in means]\n\n# Create violin plot\nplt.figure(figsize=(8, 6), dpi=150)\nplt.violinplot(\n  dataset=data,\n  showextrema=False,\n  showmeans=False,\n  points=2000\n)\n\n# Draw mean marks\nfor i, mean in enumerate(means):\n    idx = i + 1\n    plt.plot([idx - 0.3, idx + 0.3], [mean, mean],\n             c='black',\n             linewidth=1)\n    plt.text(idx + 0.2, mean - 0.2, \n             s=f\"$q_*({idx})$\",\n             fontsize=8)\n\n# Draw 0-value dashed line\nplt.plot(np.arange(0, 12), np.zeros(12), \n            c='gray', \n            linewidth=0.5,\n            linestyle=(5, (20, 10)))\nplt.tick_params(axis='both', labelsize=10)\nplt.xticks(np.arange(1, 11))\n\n# get rid of the frame\nfor i, spine in enumerate(plt.gca().spines.values()):\n    if i == 2: continue\n    spine.set_visible(False)\n    \n\n# Draw labels\nlabel_font = {\n    'fontsize': 12,\n    'fontweight': 'bold'\n}\n\nplt.xlabel('Action', fontdict=label_font)\nplt.ylabel('Reward distribution', fontdict=label_font)\nplt.margins(0)\n\nplt.tight_layout()\nplt.show()\n\nWe consider a set of 2000 randomly generated \\(k\\)-armed bandit problems with \\(k\\) = 10. For each bandit problem, such as the one shown in the output of the above code. The action values, \\(q_{*} (a), a = 1, . . . , 10\\), were selected according to a normal (Gaussian) distribution with mean 0 and variance 1. Thus when we apply a learning method to this problem, the selected action \\(A_t\\) a time step \\(t\\) the regarding reward \\(R_t\\) is sampling from a normal distribution \\[\n  R_{t} \\sim \\mathcal{N}(q_{*}(A_t), 1).\n\\] Sutton and Barto (Sutton and Barto 2018, 28) calls this suite of test tasks the 10-armed test-bed. By using this suit of benchmarks, we can measure the performance of any learning method. In fact we also can observe its behavior while the learning improves with experience of 1000 time steps, when it is applied to a selected bandit of this bed. This makes up one run. Thus, if we iterate 2000 independent runs, each with different bandit problem, we can obtain a measure of learning algorithm’s average behavior.\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. Adaptive Computation and Machine Learning. MIT Press, Cambridge, MA.\nNext we code functions to deploy the above experiment with \\(\\epsilon\\)-greedy actions\n\n\nutils.py\n\nfrom typing import Any\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy import dtype, ndarray, signedinteger\n\n\n# Get the action with the max Q value\ndef get_argmax(G:np.ndarray) -&gt; ndarray[Any, dtype[signedinteger[Any]]]:\n    candidates = np.argwhere(G == G.max()).flatten()\n    # return the only index if there's only one max\n    if len(candidates) == 1:\n        return candidates[0]\n    else:\n        # instead break the tie randomly\n        return np.random.choice(candidates)\n\n\n# Select arm and get the reward\ndef bandit(q_star:np.ndarray, \n           act:int) -&gt; tuple:\n    real_rewards = np.random.normal(q_star, 1.0)\n    # optim_choice = int(real_rewards[act] == real_rewards.max())\n    optim_choice = int(q_star[act] == q_star.max())\n    return real_rewards[act], optim_choice\n\nPlease save the above script as utils.py in the firs level of the regrding project such that we can imported by the ist name fora example by from utils import bandit, plots",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#incremental-implementation",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#incremental-implementation",
    "title": "Multi-armed Bandits",
    "section": "Incremental Implementation",
    "text": "Incremental Implementation\nCertainly! To express a more efficient method for estimating action values, we focus on using an incremental update formula rather than recalculating the average based on all past observations. The goal is to maintain a constant memory footprint and fixed computation per time step.\n\nIncremental Update Formula for Action-Value Estimation\nLet \\(R_i\\) denote the reward received after the \\(i\\)-th selection of the action.\\(Q_n\\) denote the estimate of the action value after the action has been chosen \\(n-1\\) times.\nInstead of computing \\(Q_n\\) as the sample average of all observed rewards (which requires storing and summing all rewards), we use the incremental formula:\n\\[\n    Q_n = Q_{n-1} + \\alpha \\left(R_n - Q_{n-1}\\right)\n\\]\nWhere: \\(Q_{n-1}\\) is the previous estimate of the action value. \\(R_n\\) is the reward received on the \\(n\\)-th selection. \\(\\alpha\\) is a constant step size, often set as \\(\\dfrac{1}{n}\\) to mimic the behavior of sample averaging when the number of observations grows.\n\n\nDerivation of the Incremental Formula\n\nStart with the definition of the action value as the sample mean: \\(\\displaystyle Q_n =\\dfrac{1}{n} \\sum_{i=1}^{n} R_i\\)\nExpress \\(Q_n\\) in terms of \\(Q_{n-1}\\): \\[\nQ_n = \\frac{1}{n}\n\\left[ \\sum_{i=1}^{n-1} R_i + R_n \\right]\n\\]\n\nThis can be rearranged as: \\[\n        Q_n = \\frac{n-1}{n} \\cdot Q_{n-1} +\n    \\frac{1}{n} \\cdot R_n.\n\\]\n\nNotice that \\(\\displaystyle\n    \\frac{n-1}{n} \\cdot Q_{n-1} = Q_{n-1} - \\frac{1}{n}\\cdot Q_{n-1}\\), so: \\[\n    Q_n = Q_{n-1} + \\frac{1}{n} \\left(R_n -Q_{n-1}\\right)\n\\]\n\nHere, \\(\\alpha = \\dfrac{1}{n}\\) adapts to the number of observations, ensuring the update balances the influence of new and past rewards.\n\n\nAdvantages of the Incremental Method\n\nConstant Memory: The method only requires storing \\(Q_{n-1}\\) and \\(R_n\\), avoiding the need to keep all past rewards.\nFixed Computation: Each update involves a fixed, small number of operations, regardless of \\(n\\).\n\nThis approach efficiently updates the action-value estimate with minimal resources, making it suitable for online learning algorithms and scenarios where computational efficiency is critical.\n\n\n\nIncremental algorithm\n\n\nBellow a python implementation.\n\n\nexample_2_2_bandits_algo.py\n\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.use('qt5agg')\nimport pickle\n\nfrom utils import get_argmax, bandit\n\n#SEED = 123456\n#np.random.seed(SEED)\n\n# running the k-armed bandit algorithm\ndef run_bandit(K:int, \n            q_star:np.ndarray,\n            rewards:np.ndarray,\n            optim_acts_ratio:np.ndarray,\n            epsilon:float, \n            num_steps:int=1000) -&gt; None:\n    \n    Q = np.zeros(K)     # Initialize Q values\n    N = np.zeros(K)     # The number of times each action been selected\n    ttl_optim_acts = 0\n\n    for i in range(num_steps):\n        # get action\n        A = None\n        if np.random.random() &gt; epsilon:\n            A = get_argmax(Q)\n        else:\n            A = np.random.randint(0, K)\n        \n        R, is_optim = bandit(q_star, A)\n        N[A] += 1\n        Q[A] += (R - Q[A]) / N[A]\n\n        ttl_optim_acts += is_optim\n        rewards[i] = R\n        optim_acts_ratio[i] = ttl_optim_acts / (i + 1)\n\n\nif __name__ == \"__main__\":\n\n    # Initializing the hyperparameters\n    K = 10  # Number of arms\n    epsilons = [0.0, 0.01, 0.1]\n    num_steps = 1000\n    total_rounds = 1000\n\n    # Initialize the environment\n    q_star = np.random.normal(loc=0, scale=1.0, size=K)\n    rewards = np.zeros(shape=(len(epsilons), total_rounds, num_steps))\n    optim_acts_ratio = np.zeros(shape=(len(epsilons), total_rounds, num_steps))\n    \n    # Run the k-armed bandits alg.\n    for i, epsilon in enumerate(epsilons):\n        for curr_round in range(total_rounds):\n            run_bandit(K, q_star, \n                       rewards[i, curr_round], \n                       optim_acts_ratio[i, curr_round], \n                       epsilon, \n                       num_steps)\n    \n    rewards = rewards.mean(axis=1)\n    optim_acts_ratio = optim_acts_ratio.mean(axis=1)\n\n    record = {\n        'hyper_params': epsilons, \n        'rewards': rewards,\n        'optim_acts_ratio': optim_acts_ratio\n    }\n\n    fig_01, ax_01 = plt.subplots()\n    fig_02, ax_02 = plt.subplots()\n    for i, ratio in enumerate(optim_acts_ratio):\n        ax_01.plot(\n                ratio,\n                label=r'$\\epsilon={epsilon_i}$'.format(epsilon_i=epsilons[i])\n        )\n    \n    for i, reward in enumerate(rewards):\n        ax_02.plot(\n                reward,\n                label=r'$\\epsilon={epsilon_i}$'.format(epsilon_i=epsilons[i])\n                )\n    ax_01.set_xlabel(r'$t$', fontsize=12)\n    ax_01.set_ylabel(r'Optimal Action', fontsize=12)\n    ax_01.legend(loc='best')\n    ax_02.set_xlabel(r'$t$', fontsize=12)\n    ax_02.set_ylabel(r'Reward', fontsize=12)\n    ax_02.legend(loc='best')\n    plt.show()\n    \n    # with open('./history/record.pkl', 'wb') as f:\n    #     pickle.dump(record, f)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#tracking-a-nonstationary-problem",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#tracking-a-nonstationary-problem",
    "title": "Multi-armed Bandits",
    "section": "Tracking a Nonstationary Problem",
    "text": "Tracking a Nonstationary Problem\nThe averaging methods we have discussed are suitable for stationary bandit problems, where the reward probabilities remain constant over time. However, in reinforcement learning, we often encounter non-stationary problems where it makes more sense to give greater weight to recent rewards than to rewards from a long time ago. One popular approach to achieve this is by using a constant step-size parameter.\n#TODO: Formulation with constant alpha and implications",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#optimistic-initial-values",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#optimistic-initial-values",
    "title": "Multi-armed Bandits",
    "section": "Optimistic Initial Values",
    "text": "Optimistic Initial Values\n\n\nexample_2_3_OIV.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\n\nfrom utils import get_argmax, bandit\n\nSEED = 200\nnp.random.seed(SEED)\n\n\n# running the k-armed bandit algorithm\ndef run_bandit(K: int,\n            q_star: np.ndarray,\n            rewards: np.ndarray,\n            optim_acts_ratio: np.ndarray,\n            epsilon: float,\n            num_steps: int=1000,\n            init_val: int=0\n) -&gt; None:\n    Q = np.ones(K) * init_val   # Initial Q values with OIV\n    ttl_optim_acts = 0\n    alpha = 0.1\n\n    for i in range(num_steps):\n        # get action\n        A = None\n        if np.random.random() &gt; epsilon:\n            A = get_argmax(Q)\n        else:\n            A = np.random.randint(0, K)\n        \n        R, is_optim = bandit(q_star, A)\n        Q[A] += alpha * (R - Q[A])\n\n        ttl_optim_acts += is_optim\n        rewards[i] = R\n        optim_acts_ratio[i] = ttl_optim_acts / (i + 1)\n\n\nif __name__ == \"__main__\":\n\n    # Initializing the hyper-parameters\n    K = 10 # Number of arms\n    epsilons = [0.1, 0.0]\n    init_vals = [0.0, 5.0]\n    num_steps = 1000\n    total_rounds = 2000\n\n    # Initialize the environment\n    q_star = np.random.normal(loc=0, scale=1.0, size=K)\n    rewards = np.zeros(shape=(len(epsilons), total_rounds, num_steps))\n    optim_acts_ratio = np.zeros(shape=(len(epsilons), total_rounds, num_steps))\n    \n    # Run the k-armed bandits alg.\n    for i, (epsilon, init_val) in enumerate(zip(epsilons, init_vals)):\n        for curr_round in range(total_rounds):\n            run_bandit(K, q_star, \n                       rewards[i, curr_round], \n                       optim_acts_ratio[i, curr_round], \n                       epsilon=epsilon, \n                       num_steps=num_steps,\n                       init_val=init_val)\n    \n    rewards = rewards.mean(axis=1)\n    optim_acts_ratio = optim_acts_ratio.mean(axis=1)\n\n    record = {\n        'hyper_params': [epsilons, init_vals], \n        'rewards': rewards,\n        'optim_acts_ratio': optim_acts_ratio\n    }\n\n    for vals in rewards:\n        plt.plot(vals)\n    plt.show()\n    # with open('./history/OIV_record.pkl', 'wb') as f:\n    #     pickle.dump(record, f)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#upper-confidence-bound-action-selection",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#upper-confidence-bound-action-selection",
    "title": "Multi-armed Bandits",
    "section": "Upper-Confidence-Bound Action Selection",
    "text": "Upper-Confidence-Bound Action Selection\n\n\nexample_2_4_UCB.py\n\nfrom typing import Any\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nfrom numpy import dtype, ndarray\nfrom tqdm import tqdm\nfrom utils import get_argmax, bandit\n\nSEED = 200\nnp.random.seed(SEED)\n\n\n# running the k-armed bandit algorithm\ndef run_bandit(\n        K: int,\n        q_star: np.ndarray,\n        rewards: np.ndarray,\n        optim_acts_ratio: np.ndarray,\n        epsilon: float,\n        num_steps: int = 1000\n        ) -&gt; None:\n    Q = np.zeros(K)\n    N = np.zeros(K)  # The number of times each action been selected\n    ttl_optim_acts = 0\n    \n    for i in range(num_steps):\n        A = None\n        # Get action\n        if np.random.random() &gt; epsilon:\n            A = get_argmax(Q)\n        else:\n            A = np.random.randint(0, K)\n        \n        R, is_optim = bandit(q_star, A)\n        N[A] += 1\n        Q[A] += (R - Q[A]) / N[A]\n        \n        ttl_optim_acts += is_optim\n        rewards[i] = R\n        optim_acts_ratio[i] = ttl_optim_acts / (i + 1)\n\n\n# running the bandit algorithm with UCB\ndef run_bandit_UCB(\n        K: int,\n        q_star: np.ndarray,\n        rewards: np.ndarray,\n        optim_acts_ratio: np.ndarray,\n        c: float,\n        num_steps: int = 1000\n        ) -&gt; None:\n    Q = np.zeros(K)\n    N = np.zeros(K)  # The number of times each action been selected\n    ttl_optim_acts = 0\n    \n    for i in range(num_steps):\n        A = None\n        \n        # Avoid 0-division:\n        # If there's 0 in N, then choose the action with N = 0\n        if 0 in N:\n            candidates = np.argwhere(N == 0).flatten()\n            A = np.random.choice(candidates)\n        else:\n            confidence = c * np.sqrt(np.log(i) / N)\n            freqs: ndarray[Any, dtype[Any]] | Any = Q + confidence\n            A = np.argmax(freqs).flatten()\n        \n        R, is_optim = bandit(q_star, A)\n        N[A] += 1\n        Q[A] += (R - Q[A]) / N[A]\n        \n        ttl_optim_acts += is_optim\n        rewards[i] = R\n        optim_acts_ratio[i] = ttl_optim_acts / (i + 1)\n\n\nif __name__ == \"__main__\":\n    \n    # Initializing the hyper-parameters\n    K = 10  # Number of arms\n    num_steps = 1000\n    total_rounds = 100\n    q_star = np.random.normal(loc=0, scale=1.0, size=K)\n    hyper_params = {'UCB': 2, 'epsilon': 0.1}\n    rewards = np.zeros(shape=(len(hyper_params), total_rounds, num_steps))\n    optim_acts_ratio = np.zeros(\n            shape=(len(hyper_params), total_rounds, num_steps)\n            )\n    \n    # Run bandit alg. with e-greedy\n    for curr_round in tqdm(range(total_rounds)):\n        # for curr_round in range(total_rounds):\n        run_bandit(\n                K,\n                q_star,\n                rewards[0, curr_round],\n                optim_acts_ratio[0, curr_round],\n                epsilon=hyper_params['epsilon'],\n                num_steps=num_steps\n                )\n    \n    # Run UCB and get records\n    for curr_round in tqdm(range(total_rounds)):\n        # for curr_round in range(total_rounds):\n        run_bandit_UCB(\n                K,\n                q_star,\n                rewards[1, curr_round],\n                optim_acts_ratio[1, curr_round],\n                c=hyper_params['UCB'],\n                num_steps=num_steps\n                )\n    \n    rewards = rewards.mean(axis=1)\n    optim_acts_ratio = optim_acts_ratio.mean(axis=1)\n    \n    record = {\n            'hyper_params': hyper_params,\n            'rewards': rewards,\n            'optim_acts_ratio': optim_acts_ratio\n            }\n    data = rewards\n    plt.figure(figsize=(10, 6), dpi=150)\n    plt.grid(c='lightgray')\n    plt.margins(0.02)\n    # revers the loop for a better visualization\n    # colors = ['cornflowerblue', 'tomato', 'lightseagreen']\n    colors = ['r', 'b']\n    meta = record['hyper_params']\n    optim_ratio = (optim_acts_ratio * 100)\n    legends = [f'$\\epsilon$-greedy $\\epsilon$={meta[\"epsilon\"]}',\n               f'UCB c={meta[\"UCB\"]}']\n    fontdict = {\n            'fontsize': 12,\n            'fontweight': 'bold',\n            }\n    plt.plot(rewards[0, :], linestyle='-', linewidth=2 )\n    plt.plot(rewards[1, :], linestyle='-', linewidth=2)\n    plt.tick_params(axis='both', labelsize=10)\n    plt.xlabel('step', fontdict=fontdict)\n    plt.ylabel('reward', fontdict=fontdict)\n    plt.legend(loc=4, fontsize=13)\nplt.show()\n\nwith open('./history/UCB_record.pkl', 'wb') as f:\n    pickle.dump(record, f)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#gradient-bandit-method",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#gradient-bandit-method",
    "title": "Multi-armed Bandits",
    "section": "Gradient Bandit method",
    "text": "Gradient Bandit method\n\n\nexample_2_5_gradient.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\nimport itertools\n\nfrom utils import bandit\n\nSEED = 50\nnp.random.seed(SEED)\n\n\ndef update_policy(H: np.ndarray) -&gt; np.ndarray:\n    return np.exp(H) / np.exp(H).sum()\n\n\ndef update_H(\n        H: np.ndarray,\n        policy: np.ndarray,\n        alpha: float,\n        A: int,\n        curr_reward: float,\n        avg_reward: float\n        ) -&gt; np.ndarray:\n    selec = np.zeros(len(H), dtype=np.float32)\n    selec[A] = 1.0\n    H = H + alpha * (curr_reward - avg_reward) * (selec - policy)\n    return H\n\n\n# running the k-armed bandit algorithm\ndef run_bandit(\n        K: int,\n        q_star: np.ndarray,\n        rewards: np.ndarray,\n        optim_acts_ratio: np.ndarray,\n        alpha: float,\n        baseline: bool,\n        num_steps: int = 1000\n        ) -&gt; None:\n    H = np.zeros(K, dtype=np.float32)  # initialize preference\n    policy = np.ones(K, dtype=np.float32) / K\n    ttl_reward = 0\n    ttl_optim_acts = 0\n    \n    for i in range(num_steps):\n        \n        A = np.random.choice(np.arange(K), p=policy)\n        reward, is_optim = bandit(q_star, A)\n        avg_reward = 0\n        \n        if baseline:\n            # Get average reward unitl timestep=i\n            avg_reward = ttl_reward / i if i &gt; 0 else reward\n        \n        # Update preference and policy\n        H = update_H(H, policy, alpha, A, reward, avg_reward)\n        policy = update_policy(H)\n        \n        ttl_reward += reward\n        ttl_optim_acts += is_optim\n        rewards[i] = reward\n        optim_acts_ratio[i] = ttl_optim_acts / (i + 1)\n\n\nif __name__ == \"__main__\":\n    \n    # Initializing the hyperparameters\n    K = 10  # Number of arms\n    alphas = [0.1, 0.4]\n    baselines = [False, True]\n    hyper_params = list(itertools.product(baselines, alphas))\n    \n    num_steps = 1000\n    total_rounds = 2000\n    \n    rewards = np.zeros(shape=(len(hyper_params), total_rounds, num_steps))\n    optim_acts_ratio = np.zeros(\n        shape=(len(hyper_params), total_rounds, num_steps)\n        )\n    q_star = np.random.normal(loc=4.0, scale=1.0, size=K)\n    \n    print(hyper_params)\n    for i, (is_baseline, alpha) in enumerate(hyper_params):\n        for curr_round in range(total_rounds):\n            run_bandit(\n                K,\n                q_star,\n                rewards[i, curr_round],\n                optim_acts_ratio[i, curr_round],\n                alpha,\n                is_baseline,\n                num_steps\n                )\n    \n    optim_acts_ratio = optim_acts_ratio.mean(axis=1)\n    \n    for val in optim_acts_ratio:\n        plt.plot(val)\n    plt.show()\n    \n    record = {\n            'hyper_params': hyper_params,\n            'optim_acts_ratio': optim_acts_ratio\n            }\n    \n    with open('./history/sga_record.pkl', 'wb') as f:\n         pickle.dump(record, f)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#scripts-for-visualization",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#scripts-for-visualization",
    "title": "Multi-armed Bandits",
    "section": "Scripts for visualization",
    "text": "Scripts for visualization\nRun the following script and save the output.\n\n\nplot_gradient.py\n\n\nimport matplotlib.pyplot as plt\nimport pickle\nimport numpy as np\n\n\n# Plot results\ndef plot(\n        data: np.ndarray,\n        legends: list,\n        xlabel: str,\n        ylabel: str,\n        filename: str = None,\n        fn=lambda: None, ) -&gt; None:\n    fontdict = {\n            'fontsize': 12,\n            'fontweight': 'bold',\n            }\n    \n    plt.figure(figsize=(10, 6), dpi=150)\n    plt.grid(c='lightgray')\n    plt.margins(0.02)\n    \n    # revers the loop for a better visualization\n    colors = ['navy', 'lightblue', 'tomato', 'pink']\n    for i in range(len(data) - 1, -1, -1):\n        # data[i] = uniform_filter(data[i])\n        plt.plot(data[i], label=legends[i], linewidth=1.5, c=colors[i])\n    \n    # get rid of the top/right frame lines\n    for i, spine in enumerate(plt.gca().spines.values()):\n        if i in [0, 2]:\n            spine.set_linewidth(1.5)\n            continue\n        spine.set_visible(False)\n    \n    plt.tick_params(axis='both', labelsize=10)\n    plt.xlabel(xlabel, fontdict=fontdict)\n    plt.ylabel(ylabel, fontdict=fontdict)\n    # plt.legend(loc=4, fontsize=13)\n    fn()\n    \n    plt.text(500, 57, s=\"$\\\\alpha = 0.4$\", c=colors[3], fontsize=14)\n    plt.text(500, 28, s=\"$\\\\alpha = 0.4$\", c=colors[1], fontsize=14)\n    plt.text(900, 72, s=\"$\\\\alpha = 0.1$\", c=colors[2], fontsize=14)\n    plt.text(900, 52, s=\"$\\\\alpha = 0.1$\", c=colors[0], fontsize=14)\n    \n    plt.text(770, 65, s=\"with baseline\", c=colors[2], fontsize=12)\n    plt.text(770, 42, s=\"without baseline\", c=colors[0], fontsize=12)\n    \n    if not filename:\n        plt.show()\n    else:\n        plt.savefig(f'./plots/{filename}')\n\n\ndef plot_result(\n        optim_ratio: np.ndarray,\n        legends: list,\n        output_name: str = None\n        ):\n    # Set tick labels\n    fn = lambda: plt.yticks(\n        np.arange(0, 100, 10), labels=[f'{val}%' for val in range(0, 100, 10)]\n        )\n    plot(\n        optim_ratio,\n        legends,\n        xlabel='Time step',\n        ylabel='% Optimal actions',\n        filename=output_name,\n        fn=fn\n        )\n\n\nif __name__ == \"__main__\":\n    with open('./history/sga_record.pkl', 'rb') as f:\n        history = pickle.load(f)\n    \n    optim_ratio = history['optim_acts_ratio'] * 100\n    hyper_params = history['hyper_params']\n    \n    # plot_result(optim_ratio, hyper_params, output_name=\"example_2_5_sga.png\")\n    plot_result(optim_ratio, hyper_params, output_name=None)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#summary",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#summary",
    "title": "Multi-armed Bandits",
    "section": "Summary",
    "text": "Summary\nOutline:\n\n\\(\\epsilon\\)-greedy Method:\n\nOverview of the method.\nMathematical formulation.\nParameter sensitivity and tuning.\n\nUCB (Upper Confidence Bound) Method:\n\nExplanation of UCB and its deterministic exploration mechanism.\nThe mathematical equation governing UCB.\nParameter considerations and impact on performance.\n\nGradient Bandit Algorithm:\n\nIntroduction to action preferences and how they differ from action values.\nDerivation of the softmax probability distribution.\nDiscussion on parameter choice and influence on outcomes.\n\nOptimistic Initialization:\n\nHow optimistic estimates influence exploration.\nComparison with \\(\\epsilon\\)-greedy methods.\n\n\nLet’s begin with the \\(\\epsilon\\) -greedy method:\n\n1. \\(\\epsilon\\)-greedy Method\nThis is one of the simplest algorithms to balance exploration and exploitation. The method works by choosing a random action with probability \\(\\epsilon\\) (exploration) and the action with the highest estimated value (exploitation) with probability \\(1 - \\epsilon\\).\n\nMathematical Formulation\n\nLet \\(Q(a)\\) be the estimated value of action \\(a\\).\nAt each time step \\(t\\), the agent chooses:\n\nA random action \\(a\\) with probability \\(\\epsilon\\).\nThe action with the maximum \\(Q(a)\\),\n\\(\\text{argmax}_a Q(a)\\), with probability \\({1 - \\epsilon}.\\)\n\n\n\n\nUpdate Rule\nThe estimate for the value of an action, \\(Q(a)\\), is updated using the following equation after observing a reward \\(R_t\\) for taking action \\(a\\): \\[\nQ_{t+1}(a) = Q_t(a) + \\alpha \\left( R_t - Q_t(a) \\right)\n\\]\nWhere:\n\n\\(\\alpha\\) is the step size or learning rate, determining how much the estimate is updated based on new information.\n\\(R_t\\) is the reward received after action \\(a\\) at time \\(t\\).\n\n\n\nParameter Sensitivity\n\n\\(\\epsilon\\): A small value of \\(\\epsilon\\) (e.g., 0.01) results in a mostly greedy policy with occasional exploration, while a larger \\(\\epsilon\\) (e.g., 0.1) encourages moreexploration. The optimal value balances sufficient exploration to discover rewarding actions while exploiting known high-value actions effectively.\n\n\n\n\n2. UCB (Upper Confidence Bound) Method\nUCB addresses the exploration-exploitation dilemma by adding a confidence term to the action value. The idea is to choose actions that might not have the highest estimated value but have been less explored, thus increasing exploration in a systematic way.\n\nMathematical Equation\nThe action \\(a_t\\) selected at time \\(t\\) is:\n\\[a_t = \\text{argmax}_a\n    \\left(\n        Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right)\n\\]\nWhere:\n\n\\(Q_t(a)\\) is the estimated value of action \\(a\\) at time \\(t\\).\n\\(c\\) is a parameter controlling the degree of exploration. A larger \\(c\\) increases exploration.\n\\(N_t(a)\\) is the number of times action \\(a\\) has been selected so far.\n\\(\\ln t\\) scales the confidence bound logarithmically with time.\n\nThis approach encourages the selection of actions with high uncertainty (lower ( N_t(a) )), balancing exploration based on how frequently each action has been tried.\n\n\nParameter Considerations\n\nThe parameter \\(c\\) is crucial. If \\(c\\) is too low, the algorithm might not explore enough; if too high, it may explore excessively. The optimal \\(c\\) varies depending on the problem setting.\n\n\n\n\n3. Gradient Bandit Algorithm\nUnlike \\(\\epsilon\\)-greedy and UCB methods that estimate action values, gradient bandit algorithms estimate action preferences, denoted \\(H(a)\\). These preferences are used to determine the probability of selecting each action.\n\nSoftmax Distribution\nThe probability of selecting action \\(a\\) is given by: \\[\n    \\pi(a) =\\dfrac{e^{H(a)}}{\\displaystyle \\sum_{b=1} ^{k} e^{H(b)}}\n\\]\nWhere:\n\n\\(H(a)\\) is the preference for action \\(a\\).\n\\(\\pi(a)\\) represents the probability of taking action \\(a\\).\n\n\n\nUpdate Rule\nThe preferences are updated based on the received reward as follows:\n\\[\n\\begin{aligned}\n    H_{t+1}(a) &=\n        H_t(a) + \\alpha (R_t - \\bar{R}_t)(1 - \\pi_t(a))\n        \\quad \\text{if action } a \\text{ was chosen}\n        \\\\  \n    H_{t+1}(b) &=\n        H_t(b) - \\alpha (R_t - \\bar{R}_t)\\pi_t(b)\n        \\quad \\text{for all other actions } b\n\\end{aligned}\n\\]\nWhere:\n\n\\(\\alpha\\) is the learning rate.\n\\(\\bar{R}_t\\) is the average reward received so far, acting as a baseline to stabilize learning.\n\nThis update rule encourages actions that receive above-average rewards while discouraging less rewarding ones.\n\n\n\n4. Optimistic Initialization\nThis is a simple method where the initial estimates \\(Q_0(a)\\) are set to high values, encouraging the algorithm to explore different actions because all initial action values seem promising.\n\nComparison with \\(\\epsilon\\)-greedy\n\nUnlike \\(\\epsilon\\)-greedy, which uses a random chance for exploration, optimistic initialization drives exploration until the agent converges on accurate value estimates.\nIt is especially useful when the reward distribution is unknown but expected to have some higher values.\n\n\n\n\nFinal Remarks:\nTo determine which algorithm is most effective in practice:\n\nA parameter study is essential, as highlighted in the passage. This involves varying parameters (like \\(\\epsilon\\), \\(c\\), or \\(\\alpha\\)) to find the optimal range for each algorithm.\nLearning curves provide insight into how each algorithm performs over time. Averaging these curves over several runs ensures statistical reliability.\n\n\n\nexample_2_6_summary.py\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom collections import namedtuple\nimport tqdm\nimport pickle\n\nfrom example_2_2_bandits_algo import run_bandit as e_greedy\nfrom example_2_3_OIV import run_bandit as OIV\nfrom example_2_4_UCB import run_bandit_UCB as UCB\nfrom example_2_5_gradient import run_bandit as gradient\n\nSEED = 200\nnp.random.seed(SEED)\n\n\n# A wrapper function for running different algorithms\ndef run_algorithm(\n        fn_name: str,\n        fn: 'function',\n        params: np.ndarray,\n        args: dict,\n        total_rounds: int\n        ) -&gt; np.ndarray:\n    global hyper_param\n    if fn_name == 'e_greedy':\n        hyper_param = 'epsilon'\n    elif fn_name == 'ucb':\n        hyper_param = 'c'\n    elif fn_name == 'gradient':\n        hyper_param = 'alpha'\n    elif fn_name == 'oiv':\n        hyper_param = 'init_val'\n    \n    args[hyper_param] = None\n    \n    rewards_hist = np.zeros(\n        shape=(len(params), total_rounds, args['num_steps'])\n        )\n    optm_acts_hist = np.zeros_like(rewards_hist)\n    for i, param in tqdm.tqdm(enumerate(params), desc=fn_name, position=0):\n        args[hyper_param] = param\n        for curr_round in tqdm.tqdm(\n                range(total_rounds),\n                desc=f'{fn_name}: {param}',\n                position=i+1,\n                leave=True\n                ):\n            fn(\n                **args,\n                rewards=rewards_hist[i, curr_round],\n                optim_acts_ratio=optm_acts_hist[i, curr_round]\n                )\n    print('\\n')\n    return rewards_hist.mean(axis=1).mean(axis=1)\n\n\nif __name__ == \"__main__\":\n    K = 10\n    num_steps = 1000\n    total_rounds = 2000\n    q_star = np.random.normal(loc=0, scale=1.0, size=K)\n    \n    # Creating parameter array: [1/128, 1/64, 1/32, 1/16, ...]\n    multiplier = np.exp2(np.arange(10))\n    params = np.ones(10) * (1 / 128)\n    params *= multiplier\n    x_labels = ['1/128', '1/64', '1/32', '1/16', '1/8', '1/4', '1/2', '1', '2',\n                '4']\n    \n    # Creating a dict to record running histories\n    records = {\n            'params': params,\n            'x_labels': x_labels\n            }\n    history = namedtuple('history', ['bounds', 'data'])\n    \n    base_args = {\n            'K': K,\n            'q_star': q_star,\n            'num_steps': num_steps\n            }\n    \n    # ======== e_greedy ========\n    eps_bounds = [0, 6]\n    fn_params = params[eps_bounds[0]: eps_bounds[1]]\n    \n    eps_rewards = run_algorithm(\n        'e_greedy', e_greedy, fn_params, base_args.copy(), total_rounds\n        )\n    records['e_greedy'] = history(eps_bounds, eps_rewards)\n    \n    # ======== UCB ========\n    ucb_bounds = [3, 10]\n    fn_params = params[ucb_bounds[0]: ucb_bounds[1]]\n    \n    ucb_rewards = run_algorithm(\n        'ucb', UCB, fn_params, base_args.copy(), total_rounds\n        )\n    records['ucb'] = history(ucb_bounds, ucb_rewards)\n    \n    # ======== Gradient ========\n    gd_bounds = [2, 9]\n    fn_params = params[gd_bounds[0]:gd_bounds[1]]\n    gd_args = base_args.copy()\n    gd_args['baseline'] = True\n    \n    gd_rewards = run_algorithm(\n        'gradient', gradient, fn_params, gd_args, total_rounds\n        )\n    records['gradient'] = history(gd_bounds, gd_rewards)\n    \n    # ======== OIV ========\n    oiv_bounds = [5, 10]\n    fn_params = params[oiv_bounds[0]:oiv_bounds[1]]\n    oiv_args = base_args.copy()\n    oiv_args['epsilon'] = 0.0\n    oiv_rewards = run_algorithm('oiv', OIV, fn_params, oiv_args, total_rounds)\n    records['oiv'] = history(oiv_bounds, oiv_rewards)\n    \n    with open('./history/summary.pkl', 'wb') as f:\n        pickle.dump(records, f)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#visualization",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#visualization",
    "title": "Multi-armed Bandits",
    "section": "Visualization",
    "text": "Visualization\n\n\nplot_sumary.py\n\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom collections import namedtuple\n\nhistory = namedtuple('history', ['bounds', 'data'])\nalgos = ['e_greedy', 'gradient', 'ucb', 'oiv']\n\nif __name__ == '__main__':\n    \n    with open('./history/summary.pkl', 'rb') as f:\n        histories = pickle.load(f)\n        coords = [[0.95, 1.55], [6.5, 1.45], [3, 1.82], [8.5, 1.82]]\n        legend_loc = 2\n        filename = './plots/example_2_6_summary.png'\n    \n    # with open('./history/exercise_2_6.pkl', 'rb') as f:\n    #     histories = pickle.load(f)\n    #     coords = [[2.5, 6.0], [7.0, 3.5], [7.5, 5.0], [6.5, 5.7]]\n    #     legend_loc = 0\n    #     filename = './plots/exercise_2_6.png'\n    \n    x_ticks = histories['x_labels']\n    \n    plt.figure(figsize=(10, 6), dpi=150)\n    plt.grid(c='lightgray')\n    plt.margins(0.02)\n    \n    fontdict = {\n            'fontsize': 12,\n            'fontweight': 'bold',\n            }\n    \n    legends = ['$\\epsilon$', '$\\\\alpha$', '$c$', '$Q_0$']\n    colors = ['tomato', 'mediumseagreen', 'steelblue', 'orchid']\n    \n    for i, key in enumerate(algos):\n        record = histories[key]\n        bounds = record.bounds\n        data = record.data\n        \n        plt.plot(\n            np.arange(bounds[0], bounds[1]), data, label=legends[i], c=colors[i]\n            )\n    \n    for i, spine in enumerate(plt.gca().spines.values()):\n        if i in [0, 2]:\n            spine.set_linewidth(1.5)\n            continue\n        spine.set_visible(False)\n    \n    plt.tick_params(axis='both', labelsize=10)\n    plt.xticks(np.arange(10), x_ticks)\n    \n    # x labels\n    plt.legend(loc=legend_loc, fontsize=12, title='Hyper Param.')\n    plt.xlabel('Hyper parameter value', fontdict=fontdict)\n    plt.ylabel(\n        'Average reward over first 1000 steps',\n        fontdict=fontdict\n        )\n    \n    plt.text(*coords[0], '$\\epsilon$-greedy', c=colors[0], fontsize=12)\n    plt.text(\n        *coords[1], 'gradient\\nbandit', c=colors[1], fontsize=12,\n        horizontalalignment='center'\n        )\n    plt.text(*coords[2], 'UCB', c=colors[2], fontsize=12)\n    plt.text(\n        *coords[3], 'greedy with\\noptimistic\\ninitializatio\\n$\\\\alpha=0.1$',\n        c=colors[3], fontsize=12, horizontalalignment='center'\n        )\n    \n    # plt.show()\n    plt.savefig(filename)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html",
    "href": "04-finiteMDPs/mdp.html",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "",
    "text": "The Agent–Environment Interface",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#the-agentenvironment-interface",
    "href": "04-finiteMDPs/mdp.html#the-agentenvironment-interface",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "Goals and Rewards",
    "text": "Example 3 A mobile robot has the job of collecting empty soda cans in an office environment. It has sensors for detecting cans, and an arm and gripper that can pick them up and place them in an onboard bin; it runs on a rechargeable battery. The robot’s control system has components for interpreting sensory information, for navigating, and for controlling the arm and gripper. High-level decisions about how to search for cans are made by a reinforcement learning agent based on the current charge level of the battery. To make a simple example, we assume that only two charge levels can be distinguished, comprising a small state set \\(\\mathcal{S} = \\{\\texttt{high}, \\texttt{low} \\}\\). In each state, the agent can decide whether to\n\nactively search for a can for a certain period of time,\nremain stationary and wait for someone to bring it a can, or\nhead back to its home base to recharge its battery.\n\nWhen the energy level is high, recharging would always be foolish h, so we do not include it in the action set for this state. The action sets are then \\(\\mathcal{A}(\\texttt{high}) = \\{\\texttt{search}, \\texttt{wait}\\}\\) and \\(\\mathcal{A}(\\texttt{low}) = \\{\\texttt{search}, \\texttt{wait}, \\texttt{recharge}\\}\\).\nThe rewards are zero most of the time, but become positive when the robot secures an empty can, or large and negative if the battery runs all the way down. The best way to find cans is to actively search for them, but this runs down the robot’s battery, whereas waiting does not. Whenever the robot is searching, the possibility exists that its battery will become depleted. In this case the robot must shut down and wait to be rescued (producing a low reward).\nIf the energy level is high, then a period of active search can always be completed without risk of depleting the battery. A period of searching that begins with a high energy level leaves the energy level high with probability \\(\\alpha\\) and reduces it to low with probability \\(1 - \\alpha\\). On the other hand, a period of searching undertaken when the energy level is low leaves it low with probability \\(\\beta\\) and depletes the battery with probability \\(1 - \\beta\\). In the latter case, the robot must be rescued, and the battery is then recharged back to high. Each can collected by the robot counts as a unit reward, whereas a reward of \\(-3\\) results whenever the robot has to be rescued. Let \\(r_{\\texttt{search}}\\) and \\(r_{\\texttt{wait}}\\), with \\(r_{\\texttt{search}} &gt; r_{\\texttt{wait}}\\), denote the expected numbers of cans the robot will collect (and hence the expected reward) while searching and while waiting respectively. Finally, suppose that no cans can be collected during a run home for recharging, and that no cans can be collected on a step in which the battery is depleted. This system is then a finite MDP, and we can write down the transition probabilities and the expected rewards, with dynamics as indicated in the table on the left:\n\n\n\n\n\n\nFigure 1: Recycling-robot’s graph. Taken from (Sutton and Barto 2018)\n\n\n\n\n\n\n\n\n\nFigure 2\n\n\n\n\nGoals and Rewards\nIn reinforcement learning, the reward hypothesis serves as the foundation for defining the objectives of an agent operating within an environment. According to this hypothesis, the agent’s goal can be represented by the maximization of the expected cumulative reward over time, based on scalar feedback signals received from the environment.\n\nKey Points of the Reward Hypothesis:\n\nReward as a Scalar Signal: At every time step, the environment provides the agent with a simple numerical signal, \\((R_t \\in \\mathbb{R}\\), which represents the immediate reward based on the agent’s actions and the current state of the environment. This reward acts as feedback for the agent, helping it learn and adjust its strategy to achieve its ultimate goal.\nMaximizing Cumulative Reward: The agent’s goal is not just to maximize the immediate reward, but to focus on the long-term sum of rewards, known as the cumulative reward. This ensures that the agent does not become short-sighted by only pursuing short-term benefits, but rather seeks strategies that maximize its total reward across time.\nExpected Value of the Reward: Since reinforcement learning involves interaction in environments that can be stochastic (involving randomness or uncertainty), the agent aims to maximize the expected value of the cumulative reward, accounting for different possible future states and outcomes. This means the agent is interested in the average cumulative reward it would obtain over many possible sequences of interactions, rather than specific individual outcomes.\nFormalizing Goals and Purposes: According to the reward hypothesis, all goals, objectives, or purposes of the agent can be quantified by maximizing this cumulative scalar signal (reward). In other words, the “purpose” of the agent is simply to optimize the feedback it receives in the form of rewards, and this concept encapsulates everything the agent is designed to achieve.\n\n\n\nImportance in Reinforcement Learning:\nThis hypothesis is central to how reinforcement learning problems are structured. It reduces complex goals and objectives into a single, scalar value (the reward) that the agent can track and optimize over time. This abstraction makes it possible to design agents that can handle a wide variety of tasks, as long as those tasks can be expressed in terms of rewards provided by the environment.\n\n\n\nReturns and Episodes\nTo formalize the objective of learning in reinforcement learning, we introduce the concept of return. The return, denoted \\(G_t\\), is a function of the future rewards that the agent will receive after time step \\(t\\). The agent aims to maximize the expected return to achieve its goal. Let’s break down this formalization step by step:\n\n1. The Reward Sequence:\nAt each time step \\(t\\), the agent receives a reward \\(R_{t+1}, R_{t+2},\\dots,\\) as a result of interacting with the environment. This sequence of rewards represents the feedback the agent receives over time based on its actions.\n\n\n2. Defining the Return:\nThe return at time step \\(t\\), denoted \\(G_t\\), is the total accumulated reward from time step \\(t\\) onward. In reinforcement learning, the return can be defined in different ways, but it generally involves summing the future rewards, often discounted to account for the uncertainty or diminishing value of rewards received further in the future.\nThe undiscounted return would simply be the sum of all future rewards: \\[\n   G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} R_{t+k+1}\n\\] This sum may be infinite if the task never ends, which can be problematic. Thus, in many cases, a discount factor is applied to weight future rewards less than immediate rewards.\nLikewise if the MDP has finite horizont \\(T\\) then \\[G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\cdots + R_{T}\\]\n\n\n3. Discounted Return:\nTo address this issue, we introduce a discount factor \\(\\gamma\\), where \\(0\\leq \\gamma \\leq 1\\). The discount factor controls how much emphasis is placed on future rewards. When \\(\\gamma\\) is close to 1, future rewards are considered nearly as valuable as immediate rewards. When \\(\\gamma\\) is closer to 0, the agent focuses more on immediate rewards.\nThe discounted return is defined as: \\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots\n    = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\] Here, each future reward is multiplied by ( ^k ), where ( k ) is the number of time steps into the future. This ensures that the agent values immediate rewards more highly than rewards far into the future, which is often desirable in practical applications.\n\n\n4. Maximizing the Expected Return:\nSince the environment in reinforcement learning is often stochastic, the agent cannot guarantee a specific sequence of rewards, but it can aim to maximize the expected return. The expected return is the average return the agent would obtain by following a specific policy \\(\\pi\\), which defines the agent’s behavior.\nFormally, the agent’s objective is to maximize the expected return: \\[\n    \\mathbb{E}[G_t | \\pi]\n            = \\mathbb{E}\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\Big| \\pi\\right]\n\\] where \\(\\pi\\) is the policy being followed. This equation tells us that the agent should choose actions in a way that maximizes the long-term expected reward, considering both immediate and future rewards.\n\n\n5. Two Types of Tasks:\n\nFinite-Horizon Tasks: These tasks have a fixed time limit, and the agent’s goal is to maximize the sum of rewards within that time limit. In such cases, the discount factor \\(\\gamma\\) may not be necessary, and the return is just the sum of the finite rewards received before the task ends.\nInfinite-Horizon Tasks: In tasks that continue indefinitely, the discount factor \\(\\gamma\\) ensures that the return remains finite by reducing the impact of rewards far into the future.\n\n\n\nConclusion:\nThus, in reinforcement learning, the agent’s formal objective is to maximize the expected return, \\(\\mathbb{E}[G_t]\\), where the return \\(G_t\\) is the discounted sum of future rewards. The use of the discount factor \\(\\gamma\\) helps the agent focus more on immediate rewards, while still considering future rewards to some degree. This formalization ensures that the agent’s behavior is guided not just by immediate rewards but by a balanced approach to long-term success.\n\n\n\nUnified Notation for Episodic and Continuing Tasks\n\n\nPolicies and Value Functions\nIn reinforcement learning (RL), value functions play a central role in determining the effectiveness of an agent’s behavior by estimating “how good” it is for the agent to be in a particular state or perform a specific action. Here, “how good” is quantified by future rewards the agent expects to accumulate, which is also known as the expected return. The expected return typically refers to the sum of future rewards, discounted over time, that an agent can expect to obtain from a particular state or state-action pair.\n\nKey Concepts:\n\n1. Value Functions:\nA value function is a mathematical function that estimates the future rewards expected when starting in a specific state or taking a particular action in a state. Two types of value functions commonly appear in RL:\n\nState value function \\((V)\\)\nThis estimates the value of a state, which is the expected return starting from that state and following a certain policy thereafter. The state value function \\(V^\\pi(s)\\) under a policy \\(\\pi\\) represents the expected return starting from state \\(s\\) and following the policy \\(\\pi\\) thereafter. Mathematically, it is defined as:\n\\[\n    V^\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s \\right]\n\\]\nwhere:\n\n\\(G_t\\) is the return from time \\(t\\), typically defined as the sum of discounted rewards: \\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\]\n\\(S_t\\) is the state at time \\(t\\), and \\(R_{t+1}\\) is the reward received after transitioning from state \\(S_t\\) to \\(S_{t+1}\\).\n\\(\\gamma\\) is the discount factor that determines the present value of future rewards, where \\(0 \\leq \\gamma \\leq 1\\).\n\nThe goal is to calculate \\(V^\\pi(s)\\), which gives the expected return if the agent starts in state \\(s\\) and follows policy \\(\\pi\\).\n\n\nAction Value Function \\(q_{\\pi}(s, a)\\)\nThe action value function \\(q_{\\pi}(s, a)\\) gives the expected return for taking action \\(a\\) in state \\(s\\) and then following the policy \\(\\pi\\). It is defined as:\n\\[\nq_{\\pi}(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s, A_t = a \\right]\n\\] This function tells us how good it is to take a particular action \\(a\\) in a state \\(s\\), assuming we follow policy \\(\\pi\\) afterwards.\nThis estimates the value of taking a specific action in a given state, which represents the expected return from that state-action pair, following a particular policy from that point onwards.\n\n\n\n2.Expected Return:\n\nThe expected return is the total amount of reward an agent can anticipate from a particular point in time, considering both immediate and future rewards, usually discounted by a factor \\(\\gamma\\) (the discount factor). This discount factor weights the importance of future rewards relative to immediate rewards.\n\n\n\n3. Policies:\n\nA policy \\(\\pi\\) is the strategy or decision-making rule that defines the actions an agent will take in any given state. The value functions are always associated with a particular policy, meaning the future rewards depend on the actions dictated by the policy.\n\nValue functions are thus dependent on the agent’s policy, which determines the sequence of actions that the agent will take as it interacts with the environment. Policies can be deterministic (where each state leads to a fixed action) or stochastic (where actions are selected according to a probability distribution).\n\n\n4. Bellman Equations for \\(V^\\pi(s)\\) and \\(q_{\\pi}(s, a)\\):\nTo compute the value functions, we use the Bellman equation, which expresses the value of a state or state-action pair in terms of the immediate reward plus the discounted value of the next state.\nFor the state value function, the Bellman equation is:\n\\[\nV^\\pi(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s'} P(s' \\mid s, a) \\left[ R(s, a, s') + \\gamma V^\\pi(s') \\right]\n\\]\nwhere:\n\n\\(\\pi(a \\mid s)\\) is the probability of taking action \\(a\\) in state \\(s\\) under policy \\(\\pi\\),\n\\(P(s' \\mid s, a)\\) is the probability of transitioning to state \\(s'\\) after taking action \\(a\\) in state \\(s\\),\n\\(R(s, a, s')\\) is the reward received when transitioning from \\(s\\) to \\(s'\\) after taking action \\(a\\).\n\nFor the action value function, the Bellman equation is:\n\\[\nq_{\\pi}(s, a) = \\sum_{s'} P(s' \\mid s, a) \\left[ R(s, a, s') + \\gamma \\sum_{a'} \\pi(a' \\mid s') Q^\\pi(s', a') \\right]\n\\] These equations give a recursive way of expressing value functions, which are central to many RL algorithms.\n\n\n\n\\(Q\\)-Learning (Off-policy learning algorithm)\nQ-learning is a model-free, off-policy algorithm that seeks to find the optimal action-value function \\(Q^*(s, a)\\), which represents the maximum expected return for taking action \\(a\\) in state \\(s\\) and following the optimal policy from that point onward.\nThe Q-learning update rule is:\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \\right]\n\\]\nWhere:\n\n\\(\\alpha\\) is the learning rate,\n\\(R_{t+1}\\) is the immediate reward,\n\\(\\gamma\\) is the discount factor,\n\\(\\max_{a'} Q(s_{t+1}, a')\\) is the maximum estimated value of the next state \\(s_{t+1}\\) over all possible actions \\(a'\\).\n\nThe key feature of Q-learning is that it is off-policy, meaning that the learning happens regardless of the policy the agent is currently following. The agent can learn the optimal policy while exploring the environment using a different policy.\n\n\nPolicy Iteration (On-policy learning algorithm)\nPolicy iteration is a classic on-policy algorithm that alternates between policy evaluation and policy improvement until the optimal policy is found.\n\nPolicy Evaluation: Given a policy \\(\\pi\\), calculate the value function \\(V^\\pi(s)\\) for all states using the Bellman equation for the state-value function.\nThis involves solving the system of equations for \\(V^\\pi(s)\\) either by iterating until convergence (infinite horizon) or using a matrix form if the state space is small.\nPolicy Improvement: Using the current value function \\(V^\\pi(s)\\), improve the policy by choosing actions that maximize the expected return:\n\n\\[\n\\pi'(s) = \\arg\\max_a \\sum_{s'} P(s' \\mid s, a) \\left[ R(s, a, s') + \\gamma V^\\pi(s') \\right]\n\\]\nThese two steps repeat: after improving the policy, we re-evaluate the value function and then improve the policy again. The algorithm converges when the policy no longer changes, indicating that the optimal policy \\(\\pi^*\\) has been found.\n\n\nValue Iteration\nValue iteration combines policy evaluation and policy improvement into a single step. Instead of fully evaluating the current policy, value iteration updates the value function directly using the Bellman optimality equation:\n\\[\nV(s) \\leftarrow \\max_a \\sum_{s'} P(s' \\mid s, a) \\left[ R(s, a, s') + \\gamma V(s') \\right]\n\\]\nOnce the value function has converged, the optimal policy \\(\\pi^*\\) is derived by choosing actions that maximize the value function.\n\n\nSummary:\nIn summary, value functions \\(V^\\pi(s)\\) and \\(q_{\\pi}(s, a)\\) estimate future rewards based on an agent’s policy. These functions are fundamental to algorithms like Q-learning (off-policy) and policy iteration (on-policy). Q-learning directly updates the Q-values, aiming to discover the optimal action-value function, while policy iteration alternates between evaluating a policy and improving it.\n\n\nPick and drop example\n\n\n\n\n\n\nPick and Drop game\n\n\n\n\nFigure 3: Pick and Drop game. See python implementation below.\n\n\n\n\n\n\n\n\n\nRewards rules for the drop game\n\n\n\n\nFigure 4: Rewards rules for the drop game. See the above scheme for reference.\n\n\n\n\n\n\n\n\n\nWe use the following class to simulate the pick and drop game accordingly the\n\n\n\n\n\nabove figure and rules.\n\n\npick_and_drop_game.py\n\nclass Field:\n    def __init__(self, size, item_pickup, item_dropout, start_position):\n        self.size = size\n        self.item_pickup = item_pickup\n        self.item_dropout = item_dropout\n        self.position = start_position\n        self.item_in_car = False\n    \n    def get_number_of_states(self):\n        return self.size * self.size * self.size * self.size * 2\n    \n    def get_state(self):\n        state = self.position[0] * self.size * self.size * self.size * 2\n        state = state + self.position[1] * self.size * self.size * 2\n        state = state + self.item_pickup[0] * self.size * 2\n        state = state + self.item_pickup[1] * 2\n        \n        if self.item_in_car:\n            state = state + 1\n        return state\n    \n    def make_action(self, action):\n        (x, y) = self.position\n        if action == 0:  # down\n            if y == self.size - 1:\n                return -10, False\n            else:\n                self.position = (x, y + 1)\n                return -1, False\n        \n        elif action == 1:  # up\n            if y == 0:\n                return -10, False\n            else:\n                self.position = (x, y - 1)\n                return -1, False\n        \n        elif action == 2:  # left\n            if x == 0:\n                return -10, False\n            else:\n                self.position = (x - 1, y)\n                return -1, False\n        \n        elif action == 3:  # right\n            if x == self.size - 1:\n                return -10, False\n            else:\n                self.position = (x + 1, y)\n                return -1, False\n        \n        elif action == 4:  # pickup\n            if self.item_in_car:\n                return -10, False\n            elif self.item_pickup != (x, y):\n                return -10, False\n            else:\n                self.item_in_car = True\n                return 20, False\n        \n        elif action == 5:  # dropout\n            if not self.item_in_car:\n                return -10, False\n            elif self.item_dropout != (x, y):\n                self.item_pickup = (x, y)\n                self.item_in_car = False\n                return -10, False\n            else:\n                self.item_in_car = False\n                return 20, True\n\n\n\n\n\n\n\n\n\n\nTo illustrate how works this class\n\n\n\n\n\n\n\ntest_pick_and_drop_game.py\n\nfrom pick_and_drop_game import Field\nsize = 10\nitem_pickup = (0, 0)\nitem_dropout = (9, 9)\nstart_position = (9, 0)\n\n\n\nif __name__ == '__main__':\n    field = Field(size, item_pickup, item_dropout, start_position)\n    print(field.position)\n    \n# manual solution\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\n# pick\nfield.make_action(4)\n\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\n\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\n\nfield.make_action(5)\n\n\n\n\n\n\n\n\n\n\nNow we implement a random but naive solution\n\n\n\n\n\n\n\npick_and_drop_naive_random_solution.py\n\nfrom matplotlib import pyplot as plt\n\nfrom pick_and_drop_game import Field\nimport random\nimport numpy as np\n\n\ndef random_solution():\n    size = 10\n    item_pickup = (0, 0)\n    item_dropout = (9, 9)\n    start_position = (9, 0)\n    \n    field = Field(size, item_pickup, item_dropout, start_position)\n    \n    done = False\n    steps = 0\n    \n    while not done:\n        action = random.randint(0, 5)\n        reward, done = field.make_action(action)\n        steps = steps + 1\n    \n    return steps\n\n\nif __name__ == '__main__':\n    steps = random_solution()\n    print(steps)\n    sampling_size = 100\n    sample = [random_solution() for _ in range(sampling_size)]\n    sample = np.array(sample)\n    no_steps_mean = sample.mean()\n    print('Mean of # steps for reach goal {:n}'.format(no_steps_mean))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\nNext we apply the \\(Q-\\)learning algorithm for impove the above solution.\n\n\n\n\n\n\n\npick_and_drop_q_learning_solution.py\n\n    def q_learning_solution():\n    epsilon = 0.1\n    alpha = 0.1\n    gamma = 0.6\n    \n    field = Field(size, item_pickup, item_drop_out, start_position)\n    done = False\n    steps = 0\n    \n    while not done:\n        state = field.get_state()\n        if random.uniform(0, 1) &lt; epsilon:\n            action = random.randint(0, 5)  # Explore\n        else:\n            action = np.argmax(q_table[state])  # Exploit\n        \n        reward, done = field.make_action(action)\n        \n        new_state = field.get_state()\n        new_state_max = np.max(q_table[new_state])\n        \n        q_table[state, action] = \\\n            (1 - alpha) * q_table[state, action] \\\n            + alpha * (\n                    reward + gamma * new_state_max\n                    - q_table[state, action]\n            )\n        steps = steps + 1\n    return steps\n\n\nsize = 10\nitem_pickup = (0, 0)\nitem_drop_out = (9, 9)\nstart_position = (9, 0)\n\nfield = Field(size, item_pickup, item_drop_out, start_position)\n\nnumber_of_states = field.get_number_of_states()\nnumber_of_actions = 6\nq_table = np.zeros((number_of_states, number_of_actions))\nepsilon = 0.1\nalpha = 0.1\ngamma = 0.6\nn_training = 100000\n# Training phase\n\nfor _ in range(n_training):\n    field = Field(size, item_pickup, item_drop_out, start_position)\n    done = False\n    while not done:\n        state = field.get_state()\n        if random.uniform(0, 1) &lt; epsilon:\n            action = random.randint(0, 5)  # Explore\n        else:\n            action = np.argmax(q_table[state])  # Exploit\n        reward, done = field.make_action(action)\n        new_state = field.get_state()\n        new_state_max = np.max(q_table[new_state])\n        # q_learning iteration as ascendant grad\n        q_table[state, action] = \\\n            (1.0 - alpha) * q_table[state, action] \\\n            + alpha * (\n                reward + gamma * new_state_max - q_table[state, action]\n            )\n\nq_learning_sampling = [q_learning_solution() for _ in range(10000)]\nfig, ax = plt.subplots()\nax.hist(q_learning_sampling, bins=100)\nax.set_title('distribution of the No. of steps with the Q-Learning sol')\nax.set_xlabel('No. of steps')\nax.set_ylabel('Count')\nfig.savefig(\"histogram_q_learning_solution.png\")\nplt.show()\n\n\n\n\n\nExample 1 (Gridworld)  \n\nActions: north, south, east, west\nRewards:\n\nActions would take the agent off the grid leave its location unchanged, but also results in a reward of-1;\nOther actions result in a reward of 0, except those that in states A and B;\nFrom state A , all four actions yield a reward of +10 and take the agent toA’ ;\nFrom state B , all four actions yield a reward of +5 and take the agentto B’ ;\nThe learning rate (\\(\\gamma\\)) for this example is 0.9\n\n\n\n\n\n\n\n\nGrid World example. Problem scheme , possible actions and sampling reward\n\n\n\n\nFigure 5: Grid World example. Problem scheme , possible actions and sampling reward\n\n\n\n\nThe Bellman equation must hold for each state for the value function \\(v_{\\pi}\\) shown in Figure fig-grid-world-exm (right). Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, 0.4, and +0.7. (These numbers are accurate only to one decimal place.)\n\n\n\n\n\n\n\npython implementation for the grid-world example\n\n\n\n\n\n\n\ngridworld.py\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.table import Table\nmatplotlib.use('Agg')\n\nWORLD_SIZE = 5\nA_POS = [0, 1]\nA_PRIME_POS = [4, 1]\nB_POS = [0, 3]\nB_PRIME_POS = [2, 3]\nDISCOUNT = 0.9\n\n# left, up, right, down\nACTIONS = [np.array([0, -1]),\n           np.array([-1, 0]),\n           np.array([0, 1]),\n           np.array([1, 0])]\nACTIONS_FIGS = ['←', '↑', '→', '↓']\nACTION_PROB = 0.25\n\ndef step(state, action):\n    if state == A_POS:\n        return A_PRIME_POS, 10\n    if state == B_POS:\n        return B_PRIME_POS, 5\n\n    next_state = (np.array(state) + action).tolist()\n    x, y = next_state\n    if x &lt; 0 or x &gt;= WORLD_SIZE or y &lt; 0 or y &gt;= WORLD_SIZE:\n        reward = -1.0\n        next_state = state\n    else:\n        reward = 0\n    return next_state, reward\n\n\ndef draw_image(image):\n    fig, ax = plt.subplots()\n    ax.set_axis_off()\n    tb = Table(ax, bbox=[0, 0, 1, 1])\n\n    nrows, ncols = image.shape\n    width, height = 1.0 / ncols, 1.0 / nrows\n\n    # Add cells\n    for (i, j), val in np.ndenumerate(image):\n\n        # add state labels\n        if [i, j] == A_POS:\n            val = str(val) + \" (A)\"\n        if [i, j] == A_PRIME_POS:\n            val = str(val) + \" (A')\"\n        if [i, j] == B_POS:\n            val = str(val) + \" (B)\"\n        if [i, j] == B_PRIME_POS:\n            val = str(val) + \" (B')\"\n        \n        tb.add_cell(i, j, width, height, text=val,\n                    loc='center', facecolor='white')\n        \n    # Row and column labels...\n    for i in range(len(image)):\n        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n                    edgecolor='none', facecolor='none')\n        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n                    edgecolor='none', facecolor='none')\n\n    ax.add_table(tb)\n\ndef draw_policy(optimal_values):\n    fig, ax = plt.subplots()\n    ax.set_axis_off()\n    tb = Table(ax, bbox=[0, 0, 1, 1])\n\n    nrows, ncols = optimal_values.shape\n    width, height = 1.0 / ncols, 1.0 / nrows\n\n    # Add cells\n    for (i, j), val in np.ndenumerate(optimal_values):\n        next_vals=[]\n        for action in ACTIONS:\n            next_state, _ = step([i, j], action)\n            next_vals.append(optimal_values[next_state[0],next_state[1]])\n\n        best_actions=np.where(next_vals == np.max(next_vals))[0]\n        val=''\n        for ba in best_actions:\n            val+=ACTIONS_FIGS[ba]\n        \n        # add state labels\n        if [i, j] == A_POS:\n            val = str(val) + \" (A)\"\n        if [i, j] == A_PRIME_POS:\n            val = str(val) + \" (A')\"\n        if [i, j] == B_POS:\n            val = str(val) + \" (B)\"\n        if [i, j] == B_PRIME_POS:\n            val = str(val) + \" (B')\"\n        \n        tb.add_cell(i, j, width, height, text=val,\n                loc='center', facecolor='white')\n\n    # Row and column labels...\n    for i in range(len(optimal_values)):\n        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n                    edgecolor='none', facecolor='none')\n        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n                   edgecolor='none', facecolor='none')\n\n    ax.add_table(tb)\n\n\ndef figure_3_2():\n    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n    while True:\n        # keep iteration until convergence\n        new_value = np.zeros_like(value)\n        for i in range(WORLD_SIZE):\n            for j in range(WORLD_SIZE):\n                for action in ACTIONS:\n                    (next_i, next_j), reward = step([i, j], action)\n                    # bellman equation\n                    new_value[i, j] += \\\n                        ACTION_PROB * (\n                                reward + DISCOUNT * value[next_i, next_j]\n                        )\n                    \n        if np.sum(np.abs(value - new_value)) &lt; 1e-4:\n            draw_image(np.round(new_value, decimals=2))\n            plt.savefig('../images/figure_3_2.png')\n            plt.close()\n            break\n        value = new_value\n\ndef figure_3_2_linear_system():\n    '''\n    Here we solve the linear system of equations to find the exact solution.\n    We do this by filling the coefficients for each of the states with their respective right side constant.\n    '''\n    A = -1 * np.eye(WORLD_SIZE * WORLD_SIZE)\n    b = np.zeros(WORLD_SIZE * WORLD_SIZE)\n    for i in range(WORLD_SIZE):\n        for j in range(WORLD_SIZE):\n            s = [i, j]  # current state\n            index_s = np.ravel_multi_index(s, (WORLD_SIZE, WORLD_SIZE))\n            for a in ACTIONS:\n                s_, r = step(s, a)\n                index_s_ = np.ravel_multi_index(s_, (WORLD_SIZE, WORLD_SIZE))\n\n                A[index_s, index_s_] += ACTION_PROB * DISCOUNT\n                b[index_s] -= ACTION_PROB * r\n\n    x = np.linalg.solve(A, b)\n    draw_image(np.round(x.reshape(WORLD_SIZE, WORLD_SIZE), decimals=2))\n    plt.savefig('../images/figure_3_2_linear_system.png')\n    plt.close()\n\ndef figure_3_5():\n    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n    while True:\n        # keep iteration until convergence\n        new_value = np.zeros_like(value)\n        for i in range(WORLD_SIZE):\n            for j in range(WORLD_SIZE):\n                values = []\n                for action in ACTIONS:\n                    (next_i, next_j), reward = step([i, j], action)\n                    # value iteration\n                    values.append(reward + DISCOUNT * value[next_i, next_j])\n                new_value[i, j] = np.max(values)\n        if np.sum(np.abs(new_value - value)) &lt; 1e-4:\n            draw_image(np.round(new_value, decimals=2))\n            plt.savefig('../images/figure_3_5.png')\n            plt.close()\n            draw_policy(new_value)\n            plt.savefig('../images/figure_3_5_policy.png')\n            plt.close()\n            break\n        value = new_value\n\n\nif __name__ == '__main__':\n    figure_3_2_linear_system()\n    figure_3_2()\n    figure_3_5()\n\n\n\n\n\n\nExample 2 The state is the location of the ball. The value of a state is the negative of the number of strokes to the hole from that location. Our actions are how we aim and swing at the ball, of course, and which club we select. Let us take the former as given and consider just the choice of club, which we assume is either a putter or a driver. The upper part of Figure shows a possible state-value function, \\(v_{\\text{putt}} (s)\\), for the policy that always uses the putter.\n\n\n\n\n\n\n\nGolf example\n\n\n\n\nFigure 6: Gol example. At top the reward value of each state (ball curren position).\n\n\n\n\n\n\nOptimal Policies and Optimal Value Functions\n\n\nOptimality and Approximation\n\n\nSummary\n\n\n\n\n\n\n\nFigure 1: Recycling-robot’s graph. Taken from (Sutton and Barto 2018)\nFigure 2: \nPick and Drop game\nRewards rules for the drop game\nGrid World example. Problem scheme , possible actions and sampling reward\nGolf example\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. Adaptive Computation and Machine Learning. MIT Press, Cambridge, MA.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#goals-and-rewards",
    "href": "04-finiteMDPs/mdp.html#goals-and-rewards",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "",
    "text": "In reinforcement learning, the reward hypothesis serves as the foundation for defining the objectives of an agent operating within an environment. According to this hypothesis, the agent’s goal can be represented by the maximization of the expected cumulative reward over time, based on scalar feedback signals received from the environment.\n\nKey Points of the Reward Hypothesis:\n\nReward as a Scalar Signal: At every time step, the environment provides the agent with a simple numerical signal, \\((R_t \\in \\mathbb{R}\\), which represents the immediate reward based on the agent’s actions and the current state of the environment. This reward acts as feedback for the agent, helping it learn and adjust its strategy to achieve its ultimate goal.\nMaximizing Cumulative Reward: The agent’s goal is not just to maximize the immediate reward, but to focus on the long-term sum of rewards, known as the cumulative reward. This ensures that the agent does not become short-sighted by only pursuing short-term benefits, but rather seeks strategies that maximize its total reward across time.\nExpected Value of the Reward: Since reinforcement learning involves interaction in environments that can be stochastic (involving randomness or uncertainty), the agent aims to maximize the expected value of the cumulative reward, accounting for different possible future states and outcomes. This means the agent is interested in the average cumulative reward it would obtain over many possible sequences of interactions, rather than specific individual outcomes.\nFormalizing Goals and Purposes: According to the reward hypothesis, all goals, objectives, or purposes of the agent can be quantified by maximizing this cumulative scalar signal (reward). In other words, the “purpose” of the agent is simply to optimize the feedback it receives in the form of rewards, and this concept encapsulates everything the agent is designed to achieve.\n\n\n\nImportance in Reinforcement Learning:\nThis hypothesis is central to how reinforcement learning problems are structured. It reduces complex goals and objectives into a single, scalar value (the reward) that the agent can track and optimize over time. This abstraction makes it possible to design agents that can handle a wide variety of tasks, as long as those tasks can be expressed in terms of rewards provided by the environment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#returns-and-episodes",
    "href": "04-finiteMDPs/mdp.html#returns-and-episodes",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "Returns and Episodes",
    "text": "Returns and Episodes\nTo formalize the objective of learning in reinforcement learning, we introduce the concept of return. The return, denoted \\(G_t\\), is a function of the future rewards that the agent will receive after time step \\(t\\). The agent aims to maximize the expected return to achieve its goal. Let’s break down this formalization step by step:\n\n1. The Reward Sequence:\nAt each time step \\(t\\), the agent receives a reward \\(R_{t+1}, R_{t+2},\\dots,\\) as a result of interacting with the environment. This sequence of rewards represents the feedback the agent receives over time based on its actions.\n\n\n2. Defining the Return:\nThe return at time step \\(t\\), denoted \\(G_t\\), is the total accumulated reward from time step \\(t\\) onward. In reinforcement learning, the return can be defined in different ways, but it generally involves summing the future rewards, often discounted to account for the uncertainty or diminishing value of rewards received further in the future.\nThe undiscounted return would simply be the sum of all future rewards: \\[\n   G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} R_{t+k+1}\n\\] This sum may be infinite if the task never ends, which can be problematic. Thus, in many cases, a discount factor is applied to weight future rewards less than immediate rewards.\nLikewise if the MDP has finite horizont \\(T\\) then \\[G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\cdots + R_{T}\\]\n\n\n3. Discounted Return:\nTo address this issue, we introduce a discount factor \\(\\gamma\\), where \\(0\\leq \\gamma \\leq 1\\). The discount factor controls how much emphasis is placed on future rewards. When \\(\\gamma\\) is close to 1, future rewards are considered nearly as valuable as immediate rewards. When \\(\\gamma\\) is closer to 0, the agent focuses more on immediate rewards.\nThe discounted return is defined as: \\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots\n    = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\] Here, each future reward is multiplied by ( ^k ), where ( k ) is the number of time steps into the future. This ensures that the agent values immediate rewards more highly than rewards far into the future, which is often desirable in practical applications.\n\n\n4. Maximizing the Expected Return:\nSince the environment in reinforcement learning is often stochastic, the agent cannot guarantee a specific sequence of rewards, but it can aim to maximize the expected return. The expected return is the average return the agent would obtain by following a specific policy \\(\\pi\\), which defines the agent’s behavior.\nFormally, the agent’s objective is to maximize the expected return: \\[\n    \\mathbb{E}[G_t | \\pi]\n            = \\mathbb{E}\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\Big| \\pi\\right]\n\\] where \\(\\pi\\) is the policy being followed. This equation tells us that the agent should choose actions in a way that maximizes the long-term expected reward, considering both immediate and future rewards.\n\n\n5. Two Types of Tasks:\n\nFinite-Horizon Tasks: These tasks have a fixed time limit, and the agent’s goal is to maximize the sum of rewards within that time limit. In such cases, the discount factor \\(\\gamma\\) may not be necessary, and the return is just the sum of the finite rewards received before the task ends.\nInfinite-Horizon Tasks: In tasks that continue indefinitely, the discount factor \\(\\gamma\\) ensures that the return remains finite by reducing the impact of rewards far into the future.\n\n\n\nConclusion:\nThus, in reinforcement learning, the agent’s formal objective is to maximize the expected return, \\(\\mathbb{E}[G_t]\\), where the return \\(G_t\\) is the discounted sum of future rewards. The use of the discount factor \\(\\gamma\\) helps the agent focus more on immediate rewards, while still considering future rewards to some degree. This formalization ensures that the agent’s behavior is guided not just by immediate rewards but by a balanced approach to long-term success.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#unified-notation-for-episodic-and-continuing-tasks",
    "href": "04-finiteMDPs/mdp.html#unified-notation-for-episodic-and-continuing-tasks",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "Unified Notation for Episodic and Continuing Tasks",
    "text": "Unified Notation for Episodic and Continuing Tasks",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#policies-and-value-functions",
    "href": "04-finiteMDPs/mdp.html#policies-and-value-functions",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "Policies and Value Functions",
    "text": "Policies and Value Functions\nIn reinforcement learning (RL), value functions play a central role in determining the effectiveness of an agent’s behavior by estimating “how good” it is for the agent to be in a particular state or perform a specific action. Here, “how good” is quantified by future rewards the agent expects to accumulate, which is also known as the expected return. The expected return typically refers to the sum of future rewards, discounted over time, that an agent can expect to obtain from a particular state or state-action pair.\n\nKey Concepts:\n\n1. Value Functions:\nA value function is a mathematical function that estimates the future rewards expected when starting in a specific state or taking a particular action in a state. Two types of value functions commonly appear in RL:\n\nState value function \\((V)\\)\nThis estimates the value of a state, which is the expected return starting from that state and following a certain policy thereafter. The state value function \\(V^\\pi(s)\\) under a policy \\(\\pi\\) represents the expected return starting from state \\(s\\) and following the policy \\(\\pi\\) thereafter. Mathematically, it is defined as:\n\\[\n    V^\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s \\right]\n\\]\nwhere:\n\n\\(G_t\\) is the return from time \\(t\\), typically defined as the sum of discounted rewards: \\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\]\n\\(S_t\\) is the state at time \\(t\\), and \\(R_{t+1}\\) is the reward received after transitioning from state \\(S_t\\) to \\(S_{t+1}\\).\n\\(\\gamma\\) is the discount factor that determines the present value of future rewards, where \\(0 \\leq \\gamma \\leq 1\\).\n\nThe goal is to calculate \\(V^\\pi(s)\\), which gives the expected return if the agent starts in state \\(s\\) and follows policy \\(\\pi\\).\n\n\nAction Value Function \\(q_{\\pi}(s, a)\\)\nThe action value function \\(q_{\\pi}(s, a)\\) gives the expected return for taking action \\(a\\) in state \\(s\\) and then following the policy \\(\\pi\\). It is defined as:\n\\[\nq_{\\pi}(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\mid S_t = s, A_t = a \\right]\n\\] This function tells us how good it is to take a particular action \\(a\\) in a state \\(s\\), assuming we follow policy \\(\\pi\\) afterwards.\nThis estimates the value of taking a specific action in a given state, which represents the expected return from that state-action pair, following a particular policy from that point onwards.\n\n\n\n2.Expected Return:\n\nThe expected return is the total amount of reward an agent can anticipate from a particular point in time, considering both immediate and future rewards, usually discounted by a factor \\(\\gamma\\) (the discount factor). This discount factor weights the importance of future rewards relative to immediate rewards.\n\n\n\n3. Policies:\n\nA policy \\(\\pi\\) is the strategy or decision-making rule that defines the actions an agent will take in any given state. The value functions are always associated with a particular policy, meaning the future rewards depend on the actions dictated by the policy.\n\nValue functions are thus dependent on the agent’s policy, which determines the sequence of actions that the agent will take as it interacts with the environment. Policies can be deterministic (where each state leads to a fixed action) or stochastic (where actions are selected according to a probability distribution).\n\n\n4. Bellman Equations for \\(V^\\pi(s)\\) and \\(q_{\\pi}(s, a)\\):\nTo compute the value functions, we use the Bellman equation, which expresses the value of a state or state-action pair in terms of the immediate reward plus the discounted value of the next state.\nFor the state value function, the Bellman equation is:\n\\[\nV^\\pi(s) = \\sum_{a} \\pi(a \\mid s) \\sum_{s'} P(s' \\mid s, a) \\left[ R(s, a, s') + \\gamma V^\\pi(s') \\right]\n\\]\nwhere:\n\n\\(\\pi(a \\mid s)\\) is the probability of taking action \\(a\\) in state \\(s\\) under policy \\(\\pi\\),\n\\(P(s' \\mid s, a)\\) is the probability of transitioning to state \\(s'\\) after taking action \\(a\\) in state \\(s\\),\n\\(R(s, a, s')\\) is the reward received when transitioning from \\(s\\) to \\(s'\\) after taking action \\(a\\).\n\nFor the action value function, the Bellman equation is:\n\\[\nq_{\\pi}(s, a) = \\sum_{s'} P(s' \\mid s, a) \\left[ R(s, a, s') + \\gamma \\sum_{a'} \\pi(a' \\mid s') Q^\\pi(s', a') \\right]\n\\] These equations give a recursive way of expressing value functions, which are central to many RL algorithms.\n\n\n\n\\(Q\\)-Learning (Off-policy learning algorithm)\nQ-learning is a model-free, off-policy algorithm that seeks to find the optimal action-value function \\(Q^*(s, a)\\), which represents the maximum expected return for taking action \\(a\\) in state \\(s\\) and following the optimal policy from that point onward.\nThe Q-learning update rule is:\n\\[\nQ(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \\right]\n\\]\nWhere:\n\n\\(\\alpha\\) is the learning rate,\n\\(R_{t+1}\\) is the immediate reward,\n\\(\\gamma\\) is the discount factor,\n\\(\\max_{a'} Q(s_{t+1}, a')\\) is the maximum estimated value of the next state \\(s_{t+1}\\) over all possible actions \\(a'\\).\n\nThe key feature of Q-learning is that it is off-policy, meaning that the learning happens regardless of the policy the agent is currently following. The agent can learn the optimal policy while exploring the environment using a different policy.\n\n\nPolicy Iteration (On-policy learning algorithm)\nPolicy iteration is a classic on-policy algorithm that alternates between policy evaluation and policy improvement until the optimal policy is found.\n\nPolicy Evaluation: Given a policy \\(\\pi\\), calculate the value function \\(V^\\pi(s)\\) for all states using the Bellman equation for the state-value function.\nThis involves solving the system of equations for \\(V^\\pi(s)\\) either by iterating until convergence (infinite horizon) or using a matrix form if the state space is small.\nPolicy Improvement: Using the current value function \\(V^\\pi(s)\\), improve the policy by choosing actions that maximize the expected return:\n\n\\[\n\\pi'(s) = \\arg\\max_a \\sum_{s'} P(s' \\mid s, a) \\left[ R(s, a, s') + \\gamma V^\\pi(s') \\right]\n\\]\nThese two steps repeat: after improving the policy, we re-evaluate the value function and then improve the policy again. The algorithm converges when the policy no longer changes, indicating that the optimal policy \\(\\pi^*\\) has been found.\n\n\nValue Iteration\nValue iteration combines policy evaluation and policy improvement into a single step. Instead of fully evaluating the current policy, value iteration updates the value function directly using the Bellman optimality equation:\n\\[\nV(s) \\leftarrow \\max_a \\sum_{s'} P(s' \\mid s, a) \\left[ R(s, a, s') + \\gamma V(s') \\right]\n\\]\nOnce the value function has converged, the optimal policy \\(\\pi^*\\) is derived by choosing actions that maximize the value function.\n\n\nSummary:\nIn summary, value functions \\(V^\\pi(s)\\) and \\(q_{\\pi}(s, a)\\) estimate future rewards based on an agent’s policy. These functions are fundamental to algorithms like Q-learning (off-policy) and policy iteration (on-policy). Q-learning directly updates the Q-values, aiming to discover the optimal action-value function, while policy iteration alternates between evaluating a policy and improving it.\n\n\nPick and drop example\n\n\n\n\n\n\nPick and Drop game\n\n\n\n\nFigure 3: Pick and Drop game. See python implementation below.\n\n\n\n\n\n\n\n\n\nRewards rules for the drop game\n\n\n\n\nFigure 4: Rewards rules for the drop game. See the above scheme for reference.\n\n\n\n\n\n\n\n\n\nWe use the following class to simulate the pick and drop game accordingly the\n\n\n\n\n\nabove figure and rules.\n\n\npick_and_drop_game.py\n\nclass Field:\n    def __init__(self, size, item_pickup, item_dropout, start_position):\n        self.size = size\n        self.item_pickup = item_pickup\n        self.item_dropout = item_dropout\n        self.position = start_position\n        self.item_in_car = False\n    \n    def get_number_of_states(self):\n        return self.size * self.size * self.size * self.size * 2\n    \n    def get_state(self):\n        state = self.position[0] * self.size * self.size * self.size * 2\n        state = state + self.position[1] * self.size * self.size * 2\n        state = state + self.item_pickup[0] * self.size * 2\n        state = state + self.item_pickup[1] * 2\n        \n        if self.item_in_car:\n            state = state + 1\n        return state\n    \n    def make_action(self, action):\n        (x, y) = self.position\n        if action == 0:  # down\n            if y == self.size - 1:\n                return -10, False\n            else:\n                self.position = (x, y + 1)\n                return -1, False\n        \n        elif action == 1:  # up\n            if y == 0:\n                return -10, False\n            else:\n                self.position = (x, y - 1)\n                return -1, False\n        \n        elif action == 2:  # left\n            if x == 0:\n                return -10, False\n            else:\n                self.position = (x - 1, y)\n                return -1, False\n        \n        elif action == 3:  # right\n            if x == self.size - 1:\n                return -10, False\n            else:\n                self.position = (x + 1, y)\n                return -1, False\n        \n        elif action == 4:  # pickup\n            if self.item_in_car:\n                return -10, False\n            elif self.item_pickup != (x, y):\n                return -10, False\n            else:\n                self.item_in_car = True\n                return 20, False\n        \n        elif action == 5:  # dropout\n            if not self.item_in_car:\n                return -10, False\n            elif self.item_dropout != (x, y):\n                self.item_pickup = (x, y)\n                self.item_in_car = False\n                return -10, False\n            else:\n                self.item_in_car = False\n                return 20, True\n\n\n\n\n\n\n\n\n\n\nTo illustrate how works this class\n\n\n\n\n\n\n\ntest_pick_and_drop_game.py\n\nfrom pick_and_drop_game import Field\nsize = 10\nitem_pickup = (0, 0)\nitem_dropout = (9, 9)\nstart_position = (9, 0)\n\n\n\nif __name__ == '__main__':\n    field = Field(size, item_pickup, item_dropout, start_position)\n    print(field.position)\n    \n# manual solution\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\n# pick\nfield.make_action(4)\n\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\n\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\n\nfield.make_action(5)\n\n\n\n\n\n\n\n\n\n\nNow we implement a random but naive solution\n\n\n\n\n\n\n\npick_and_drop_naive_random_solution.py\n\nfrom matplotlib import pyplot as plt\n\nfrom pick_and_drop_game import Field\nimport random\nimport numpy as np\n\n\ndef random_solution():\n    size = 10\n    item_pickup = (0, 0)\n    item_dropout = (9, 9)\n    start_position = (9, 0)\n    \n    field = Field(size, item_pickup, item_dropout, start_position)\n    \n    done = False\n    steps = 0\n    \n    while not done:\n        action = random.randint(0, 5)\n        reward, done = field.make_action(action)\n        steps = steps + 1\n    \n    return steps\n\n\nif __name__ == '__main__':\n    steps = random_solution()\n    print(steps)\n    sampling_size = 100\n    sample = [random_solution() for _ in range(sampling_size)]\n    sample = np.array(sample)\n    no_steps_mean = sample.mean()\n    print('Mean of # steps for reach goal {:n}'.format(no_steps_mean))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\nNext we apply the \\(Q-\\)learning algorithm for impove the above solution.\n\n\n\n\n\n\n\npick_and_drop_q_learning_solution.py\n\n    def q_learning_solution():\n    epsilon = 0.1\n    alpha = 0.1\n    gamma = 0.6\n    \n    field = Field(size, item_pickup, item_drop_out, start_position)\n    done = False\n    steps = 0\n    \n    while not done:\n        state = field.get_state()\n        if random.uniform(0, 1) &lt; epsilon:\n            action = random.randint(0, 5)  # Explore\n        else:\n            action = np.argmax(q_table[state])  # Exploit\n        \n        reward, done = field.make_action(action)\n        \n        new_state = field.get_state()\n        new_state_max = np.max(q_table[new_state])\n        \n        q_table[state, action] = \\\n            (1 - alpha) * q_table[state, action] \\\n            + alpha * (\n                    reward + gamma * new_state_max\n                    - q_table[state, action]\n            )\n        steps = steps + 1\n    return steps\n\n\nsize = 10\nitem_pickup = (0, 0)\nitem_drop_out = (9, 9)\nstart_position = (9, 0)\n\nfield = Field(size, item_pickup, item_drop_out, start_position)\n\nnumber_of_states = field.get_number_of_states()\nnumber_of_actions = 6\nq_table = np.zeros((number_of_states, number_of_actions))\nepsilon = 0.1\nalpha = 0.1\ngamma = 0.6\nn_training = 100000\n# Training phase\n\nfor _ in range(n_training):\n    field = Field(size, item_pickup, item_drop_out, start_position)\n    done = False\n    while not done:\n        state = field.get_state()\n        if random.uniform(0, 1) &lt; epsilon:\n            action = random.randint(0, 5)  # Explore\n        else:\n            action = np.argmax(q_table[state])  # Exploit\n        reward, done = field.make_action(action)\n        new_state = field.get_state()\n        new_state_max = np.max(q_table[new_state])\n        # q_learning iteration as ascendant grad\n        q_table[state, action] = \\\n            (1.0 - alpha) * q_table[state, action] \\\n            + alpha * (\n                reward + gamma * new_state_max - q_table[state, action]\n            )\n\nq_learning_sampling = [q_learning_solution() for _ in range(10000)]\nfig, ax = plt.subplots()\nax.hist(q_learning_sampling, bins=100)\nax.set_title('distribution of the No. of steps with the Q-Learning sol')\nax.set_xlabel('No. of steps')\nax.set_ylabel('Count')\nfig.savefig(\"histogram_q_learning_solution.png\")\nplt.show()\n\n\n\n\n\nExample 1 (Gridworld)  \n\nActions: north, south, east, west\nRewards:\n\nActions would take the agent off the grid leave its location unchanged, but also results in a reward of-1;\nOther actions result in a reward of 0, except those that in states A and B;\nFrom state A , all four actions yield a reward of +10 and take the agent toA’ ;\nFrom state B , all four actions yield a reward of +5 and take the agentto B’ ;\nThe learning rate (\\(\\gamma\\)) for this example is 0.9\n\n\n\n\n\n\n\n\nGrid World example. Problem scheme , possible actions and sampling reward\n\n\n\n\nFigure 5: Grid World example. Problem scheme , possible actions and sampling reward\n\n\n\n\nThe Bellman equation must hold for each state for the value function \\(v_{\\pi}\\) shown in Figure fig-grid-world-exm (right). Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, 0.4, and +0.7. (These numbers are accurate only to one decimal place.)\n\n\n\n\n\n\n\npython implementation for the grid-world example\n\n\n\n\n\n\n\ngridworld.py\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.table import Table\nmatplotlib.use('Agg')\n\nWORLD_SIZE = 5\nA_POS = [0, 1]\nA_PRIME_POS = [4, 1]\nB_POS = [0, 3]\nB_PRIME_POS = [2, 3]\nDISCOUNT = 0.9\n\n# left, up, right, down\nACTIONS = [np.array([0, -1]),\n           np.array([-1, 0]),\n           np.array([0, 1]),\n           np.array([1, 0])]\nACTIONS_FIGS = ['←', '↑', '→', '↓']\nACTION_PROB = 0.25\n\ndef step(state, action):\n    if state == A_POS:\n        return A_PRIME_POS, 10\n    if state == B_POS:\n        return B_PRIME_POS, 5\n\n    next_state = (np.array(state) + action).tolist()\n    x, y = next_state\n    if x &lt; 0 or x &gt;= WORLD_SIZE or y &lt; 0 or y &gt;= WORLD_SIZE:\n        reward = -1.0\n        next_state = state\n    else:\n        reward = 0\n    return next_state, reward\n\n\ndef draw_image(image):\n    fig, ax = plt.subplots()\n    ax.set_axis_off()\n    tb = Table(ax, bbox=[0, 0, 1, 1])\n\n    nrows, ncols = image.shape\n    width, height = 1.0 / ncols, 1.0 / nrows\n\n    # Add cells\n    for (i, j), val in np.ndenumerate(image):\n\n        # add state labels\n        if [i, j] == A_POS:\n            val = str(val) + \" (A)\"\n        if [i, j] == A_PRIME_POS:\n            val = str(val) + \" (A')\"\n        if [i, j] == B_POS:\n            val = str(val) + \" (B)\"\n        if [i, j] == B_PRIME_POS:\n            val = str(val) + \" (B')\"\n        \n        tb.add_cell(i, j, width, height, text=val,\n                    loc='center', facecolor='white')\n        \n    # Row and column labels...\n    for i in range(len(image)):\n        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n                    edgecolor='none', facecolor='none')\n        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n                    edgecolor='none', facecolor='none')\n\n    ax.add_table(tb)\n\ndef draw_policy(optimal_values):\n    fig, ax = plt.subplots()\n    ax.set_axis_off()\n    tb = Table(ax, bbox=[0, 0, 1, 1])\n\n    nrows, ncols = optimal_values.shape\n    width, height = 1.0 / ncols, 1.0 / nrows\n\n    # Add cells\n    for (i, j), val in np.ndenumerate(optimal_values):\n        next_vals=[]\n        for action in ACTIONS:\n            next_state, _ = step([i, j], action)\n            next_vals.append(optimal_values[next_state[0],next_state[1]])\n\n        best_actions=np.where(next_vals == np.max(next_vals))[0]\n        val=''\n        for ba in best_actions:\n            val+=ACTIONS_FIGS[ba]\n        \n        # add state labels\n        if [i, j] == A_POS:\n            val = str(val) + \" (A)\"\n        if [i, j] == A_PRIME_POS:\n            val = str(val) + \" (A')\"\n        if [i, j] == B_POS:\n            val = str(val) + \" (B)\"\n        if [i, j] == B_PRIME_POS:\n            val = str(val) + \" (B')\"\n        \n        tb.add_cell(i, j, width, height, text=val,\n                loc='center', facecolor='white')\n\n    # Row and column labels...\n    for i in range(len(optimal_values)):\n        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n                    edgecolor='none', facecolor='none')\n        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n                   edgecolor='none', facecolor='none')\n\n    ax.add_table(tb)\n\n\ndef figure_3_2():\n    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n    while True:\n        # keep iteration until convergence\n        new_value = np.zeros_like(value)\n        for i in range(WORLD_SIZE):\n            for j in range(WORLD_SIZE):\n                for action in ACTIONS:\n                    (next_i, next_j), reward = step([i, j], action)\n                    # bellman equation\n                    new_value[i, j] += \\\n                        ACTION_PROB * (\n                                reward + DISCOUNT * value[next_i, next_j]\n                        )\n                    \n        if np.sum(np.abs(value - new_value)) &lt; 1e-4:\n            draw_image(np.round(new_value, decimals=2))\n            plt.savefig('../images/figure_3_2.png')\n            plt.close()\n            break\n        value = new_value\n\ndef figure_3_2_linear_system():\n    '''\n    Here we solve the linear system of equations to find the exact solution.\n    We do this by filling the coefficients for each of the states with their respective right side constant.\n    '''\n    A = -1 * np.eye(WORLD_SIZE * WORLD_SIZE)\n    b = np.zeros(WORLD_SIZE * WORLD_SIZE)\n    for i in range(WORLD_SIZE):\n        for j in range(WORLD_SIZE):\n            s = [i, j]  # current state\n            index_s = np.ravel_multi_index(s, (WORLD_SIZE, WORLD_SIZE))\n            for a in ACTIONS:\n                s_, r = step(s, a)\n                index_s_ = np.ravel_multi_index(s_, (WORLD_SIZE, WORLD_SIZE))\n\n                A[index_s, index_s_] += ACTION_PROB * DISCOUNT\n                b[index_s] -= ACTION_PROB * r\n\n    x = np.linalg.solve(A, b)\n    draw_image(np.round(x.reshape(WORLD_SIZE, WORLD_SIZE), decimals=2))\n    plt.savefig('../images/figure_3_2_linear_system.png')\n    plt.close()\n\ndef figure_3_5():\n    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n    while True:\n        # keep iteration until convergence\n        new_value = np.zeros_like(value)\n        for i in range(WORLD_SIZE):\n            for j in range(WORLD_SIZE):\n                values = []\n                for action in ACTIONS:\n                    (next_i, next_j), reward = step([i, j], action)\n                    # value iteration\n                    values.append(reward + DISCOUNT * value[next_i, next_j])\n                new_value[i, j] = np.max(values)\n        if np.sum(np.abs(new_value - value)) &lt; 1e-4:\n            draw_image(np.round(new_value, decimals=2))\n            plt.savefig('../images/figure_3_5.png')\n            plt.close()\n            draw_policy(new_value)\n            plt.savefig('../images/figure_3_5_policy.png')\n            plt.close()\n            break\n        value = new_value\n\n\nif __name__ == '__main__':\n    figure_3_2_linear_system()\n    figure_3_2()\n    figure_3_5()\n\n\n\n\n\n\nExample 2 The state is the location of the ball. The value of a state is the negative of the number of strokes to the hole from that location. Our actions are how we aim and swing at the ball, of course, and which club we select. Let us take the former as given and consider just the choice of club, which we assume is either a putter or a driver. The upper part of Figure shows a possible state-value function, \\(v_{\\text{putt}} (s)\\), for the policy that always uses the putter.\n\n\n\n\n\n\n\nGolf example\n\n\n\n\nFigure 6: Gol example. At top the reward value of each state (ball curren position).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#optimal-policies-and-optimal-value-functions",
    "href": "04-finiteMDPs/mdp.html#optimal-policies-and-optimal-value-functions",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "Optimal Policies and Optimal Value Functions",
    "text": "Optimal Policies and Optimal Value Functions",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#optimality-and-approximation",
    "href": "04-finiteMDPs/mdp.html#optimality-and-approximation",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "Optimality and Approximation",
    "text": "Optimality and Approximation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#summary-1",
    "href": "04-finiteMDPs/mdp.html#summary-1",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "Summary",
    "text": "Summary",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html",
    "href": "05-dynamicProgramming/dp_rl.html",
    "title": "Dynamic Programming",
    "section": "",
    "text": "Policy Evaluation (Prediction)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#policy-improvement",
    "href": "05-dynamicProgramming/dp_rl.html#policy-improvement",
    "title": "Dynamic Programming",
    "section": "Policy Improvement",
    "text": "Policy Improvement",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#policy-iteration",
    "href": "05-dynamicProgramming/dp_rl.html#policy-iteration",
    "title": "Dynamic Programming",
    "section": "Policy Iteration",
    "text": "Policy Iteration",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#value-iteration",
    "href": "05-dynamicProgramming/dp_rl.html#value-iteration",
    "title": "Dynamic Programming",
    "section": "Value Iteration",
    "text": "Value Iteration",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#asynchronous-dynamic-programming",
    "href": "05-dynamicProgramming/dp_rl.html#asynchronous-dynamic-programming",
    "title": "Dynamic Programming",
    "section": "Asynchronous Dynamic Programming",
    "text": "Asynchronous Dynamic Programming",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#generalized-policy-iteration",
    "href": "05-dynamicProgramming/dp_rl.html#generalized-policy-iteration",
    "title": "Dynamic Programming",
    "section": "Generalized Policy Iteration",
    "text": "Generalized Policy Iteration",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#efficiency-of-dynamic-programming",
    "href": "05-dynamicProgramming/dp_rl.html#efficiency-of-dynamic-programming",
    "title": "Dynamic Programming",
    "section": "Efficiency of Dynamic Programming",
    "text": "Efficiency of Dynamic Programming",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#summary",
    "href": "05-dynamicProgramming/dp_rl.html#summary",
    "title": "Dynamic Programming",
    "section": "Summary",
    "text": "Summary",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dynamic Programming</span>"
    ]
  },
  {
    "objectID": "06-applications/applications.html",
    "href": "06-applications/applications.html",
    "title": "Applications",
    "section": "",
    "text": "Recycling Robot",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applications</span>"
    ]
  },
  {
    "objectID": "06-applications/applications.html#a-robot-with-randomly-moves-in-a-grid-world.",
    "href": "06-applications/applications.html#a-robot-with-randomly-moves-in-a-grid-world.",
    "title": "Applications",
    "section": "A robot with randomly moves in a grid world.",
    "text": "A robot with randomly moves in a grid world.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Applications</span>"
    ]
  },
  {
    "objectID": "07-Project/project_proposal.html",
    "href": "07-Project/project_proposal.html",
    "title": "Project proposal",
    "section": "",
    "text": "Formulation and reinforcement learning solution to a problem\nof a sequence of decisions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Project proposal</span>"
    ]
  },
  {
    "objectID": "07-Project/project_proposal.html#death-line-december-08-2024-235900",
    "href": "07-Project/project_proposal.html#death-line-december-08-2024-235900",
    "title": "Project proposal",
    "section": "Death line: December 08, 2024-23:59:00",
    "text": "Death line: December 08, 2024-23:59:00",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Project proposal</span>"
    ]
  },
  {
    "objectID": "07-Project/project_proposal.html#sub-products",
    "href": "07-Project/project_proposal.html#sub-products",
    "title": "Project proposal",
    "section": "Sub-products",
    "text": "Sub-products\n\n\n\nDeath lines\n\n\n\n\n\nStage 01\nNovember 02, 2024-23:59\n\n\nStage 02\nDecember 05, 2024-23:59\n\n\n\n\nStage 01: Quarto book with MDP formulation\n\nThe page must encloses the report according to the template rl_bookdown_prg.qmd\n\nIntroduction\nFormulation of the Mrakov decision process\nModel dynamics\nDescription and justification of the Cost (reward)\nJustification of the actions\n\nMust include\n\nFigures to illustrates the behavior of the regarding elements:\n\nPolicy\nReward\nValue function eventuated for a one state-action and transition.\nEnvironmental model\n\nReferences via bibtex.\nOutput compilation for HTML and PDF formats.\nThe compiled version has to be mounted ing GitHub or Quarto Pub\n\n\n\n\nStage 02: Python code Implementation\n\nOnly code whit out running errors wold be accepted\nCode must follows the style guide from PEP 08\nAll functions must include doc-strings\nExtras:\nPacking and Documentation extra 200 xps\n\n\n\nStage 03: Video Presentation\nA video mounted in you-tube of at most 20 min with results and insight of your project",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Project proposal</span>"
    ]
  },
  {
    "objectID": "07-Project/project_proposal.html#suggested-project-list",
    "href": "07-Project/project_proposal.html#suggested-project-list",
    "title": "Project proposal",
    "section": "Suggested project list:",
    "text": "Suggested project list:\n\nReinforcement learning simulation of the TIC-TAC-TOE Game with SARSA or Q-learning Algorithms Bilgin (2020)\nThe movement of a Recycling Robot Bilgin (2020)\nThe replacement of a bus engine (Rust 1987) from (see pd.pdf, p.130 Stachurski. 2024)\nOptimal Inventories (see dp.pdf, p. 147 Stachurski. 2024)\nMulti-Armed Bandits Bilgin (2020)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Project proposal</span>"
    ]
  },
  {
    "objectID": "07-Project/project_proposal.html#project-lits",
    "href": "07-Project/project_proposal.html#project-lits",
    "title": "Project proposal",
    "section": "Project Lits",
    "text": "Project Lits\n\nProject list\n\n\n\n\n\n\n\nProject\nAuthor\nReference\n\n\n\n\nDynamic Portfolio Analysis\nGABRIEL MIRANDA GAMEZ\n(Sec. 4.3, Bertsekas 2005)\n\n\nLearning the Best Diabetes Medication\nEDGAR EVERARDO MARTINEZ GARCIA\n(Ch.4 4, Powell et al. 2022)\n\n\nA MDP model for the collective behavior in vaccination campaigns\nIRASEMA PEDROZA MEZA\n\n\n\nModelo de inventario para alimentos pedecederos\nDAVID PEÑA PERALTA\n(Stachurski. 2024)\n\n\nA inventory model\nJAZMIN SARAHI FLORES GOMEZ\n(Levhari and Mirman 1982)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Project proposal</span>"
    ]
  },
  {
    "objectID": "07-Project/project_proposal.html#configuration-to-build-with-spanish-language",
    "href": "07-Project/project_proposal.html#configuration-to-build-with-spanish-language",
    "title": "Project proposal",
    "section": "Configuration to build with Spanish language",
    "text": "Configuration to build with Spanish language\nAdapt to your project accordingly to your .png files another sources.\n\n\n_quarto.yml\n\n  project:\n  type: book\n  output-dir: _book\n\nwebsite:\n  favicon: FCFMLOGO.png\n  reader-mode: true\n  search:\n    location: sidebar\n    type: overlay\n  comments:\n    hypothesis: true\n\nbook:\n  title: \"Análisis comparativo del desempeño en métodos para el pronóstico de series temporales\"\n  reader-mode: true\n  language: es\n  date: \"02/14/2024\"\n  output-file: \"Tesis_JSLG\"\n  # image: logofcfm.png\n  # cover-image: FCFMLOGO.png\n  sharing: [twitter, facebook]\n  downloads: [pdf, epub]\n  # favicon: logofcfm.png\n  sidebar:\n  #  logo: LOGO50.png\n    style: floating\n    collapse-level: 2\n    border: true\n    search: true\n  open-graph: true\n  twitter-card: true\n  #repo-url: https://github.com/Jennlg/Tesis\n  repo-actions: [edit, issue, source]\n  page-navigation: true\n  chapters:\n    - index.qmd\n    - intro.qmd\n    - objetivos.qmd\n\n    - part: 'Preliminares'\n      chapters:\n        - tconjuntos.qmd\n        - probabilidad.qmd\n        - estadistica.qmd\n        - procesos.qmd\n    - part: 'Series de tiempo'\n      chapters:\n        - series.qmd\n    - part: 'Redes neuronales'\n      chapters:\n        - redes.qmd\n    - part: estudio.qmd\n      chapters:\n        - metodologia.qmd\n        - confirmados.qmd\n        - muertes.qmd\n    - conclusiones.qmd\n\n    - references.qmd\n\ncomments:\n    hypothesis: true\n\nbibliography: references.bib\n\nformat:\n  html:\n    theme:\n      dark: darkly\n      light: cerulean\n    highlight-style: a11y\n    lang: es\n    html-math-method: mathjax\n    grid:\n      sidebar-width: 300px\n      body-width: 900px\n      margin-width: 300px\n      gutter-width: 1.5rem\n    code-copy: true\n    code-fold: true\n  pdf:\n    lang: es\n    include-in-header:\n      - packa.tex\n    template-partials:\n      - before-body.tex\n    documentclass: scrreprt\n    papersize: us-letter\n    #titlegraphic: FCFMLOGO.png\n    institution: Universidad Autónoma de Chiapas\n    email: jennifer.lopez67@unach.mx\n    keep-tex: true\n  epub:\n    cover-image: FCFMLOGO.png\neditor: visual\n\nWe also need the following .tex in the root folder\n\\usepackage{upgreek}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\newcommand{\\dashedbox}[1]{\n  \\begin{tikzpicture}\n    \\node[draw, dashed, rounded corners=5pt, inner sep=10pt] {\n      \\begin{minipage}{0.8\\textwidth} % Establece el ancho del minipage\n        #1\n      \\end{minipage}\n    };\n  \\end{tikzpicture}\n}\n\n\n\n\n\n\nBertsekas, Dimitri P. 2005. Dynamic Programming and Optimal Control. Vol. I. Third. Athena Scientific, Belmont, MA.\n\n\nBilgin, E. 2020. Mastering Reinforcement Learning with Python: Build Next-Generation, Self-Learning Models Using Reinforcement Learning Techniques and Best Practices. Packt Publishing. https://books.google.com.mx/books?id=s0MQEAAAQBAJ.\n\n\nLevhari, DAVID, and Leonard J Mirman. 1982. “The Great Fish War: An Example Using a Dynamic Cournot-Nash Solution.” Essays in the Economics of Renewable Resources, LJ Mirman and DF Spulber (Eds.), North-Holland, 243–58.\n\n\nPowell, Warren B et al. 2022. “Sequential Decision Analytics and Modeling: Modeling with Python.”\n\n\nRust, John. 1987. “Optimal Replacement of GMC Bus Engines: An Empirical Model of Harold Zurcher.” Econometrica 55 (5): 999. https://doi.org/10.2307/1911259.\n\n\nStachurski., John. 2024. “Dynamic Programming Volume 1.” GitHub Repository. https://github.com/QuantEcon/book-dp1-public; GitHub.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Project proposal</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "For the cured R-quarto material see https://github.com/SaulDiazInfante/intro-quarto-unison-2024.\n\n\nBertsekas, Dimitri P. 2005. Dynamic Programming and Optimal Control.\nVol. I. Third. Athena Scientific,\nBelmont, MA.\n\n\nBilgin, E. 2020. Mastering Reinforcement Learning with Python: Build\nNext-Generation, Self-Learning Models Using Reinforcement Learning\nTechniques and Best Practices. Packt Publishing. https://books.google.com.mx/books?id=s0MQEAAAQBAJ.\n\n\nBrandimarte, Paolo. 2013. Numerical Methods in Finance and\nEconomics: A MATLAB-Based Introduction. 2nd ed. Hoboken, New\nJersey: John Wiley & Sons.\n\n\nBrunton, Steven L., and J. Nathan Kutz. 2019. Data-Driven Science\nand Engineering. Cambridge University Press, Cambridge. https://doi.org/10.1017/9781108380690.\n\n\nLevhari, DAVID, and Leonard J Mirman. 1982. “The Great Fish War:\nAn Example Using a Dynamic Cournot-Nash Solution.” Essays in\nthe Economics of Renewable Resources, LJ Mirman and DF Spulber (Eds.),\nNorth-Holland, 243–58.\n\n\nPowell, Warren B et al. 2022. “Sequential Decision Analytics and\nModeling: Modeling with Python.”\n\n\nRust, John. 1987. “Optimal Replacement of GMC\nBus Engines: An Empirical Model of Harold Zurcher.”\nEconometrica 55 (5): 999. https://doi.org/10.2307/1911259.\n\n\nStachurski., John. 2024. “Dynamic Programming Volume 1.”\nGitHub Repository. https://github.com/QuantEcon/book-dp1-public; GitHub.\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement\nLearning: An Introduction. Second. Adaptive Computation and Machine\nLearning. MIT Press, Cambridge, MA.\n\n\nSzepesvári, Csaba. 2022. Algorithms for Reinforcement Learning.\nVol. 9. Synthesis Lectures on Artificial Intelligence and Machine\nLearning. Springer, Cham. https://doi.org/10.1007/978-3-031-01551-9.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "homeworks/home_works_list.html",
    "href": "homeworks/home_works_list.html",
    "title": "List of Home Works and due dates",
    "section": "",
    "text": "Homework 001 due date: september 20, 2024-12:00:00",
    "crumbs": [
      "List of Home Works and due dates"
    ]
  },
  {
    "objectID": "homeworks/home_works_list.html#homework-001-due-date-september-20-2024-120000",
    "href": "homeworks/home_works_list.html#homework-001-due-date-september-20-2024-120000",
    "title": "List of Home Works and due dates",
    "section": "",
    "text": "Exercise 1 Read (Sec 1.1, pp 1-2 Sutton and Barto 2018) and answer the following.\nExplain why Reinforcement Learning differs for supervised and unsupervised learning.\n\n\nExercise 2 See the first Steve Brunton’s youtube video about Reinforcement Learning. Then accordingly to its presentation explain what is the meaning of the following expression:\n\\[\n  V_{\\pi}(s) = \\mathbb{E}\n\\left(\n   \\sum_{t} \\gamma ^ {t} r_t | s_0 = s\n\\right).\n\\]\n\n\nExercise 3 Form (see Sutton and Barto 2018, sec. 1.7) obtain a time line pear year from 1950 to 2012.\nUse the following format see https://kim.quarto.pub/milestones–bar-timelines/\n\nCodelibrary(bibtex)\n## Activate the Core Packages\nbiblio &lt;- bibtex::read.bib(\"../references.bib\")\n\nignoring entry 'powell2022sequential' (line 154) because :\n    A bibentry of bibtype 'Article' has to specify the field: journal\n\nCodelibrary(tidyverse) ## Brings in a core of useful functions\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nCodelibrary(gt)        ## Tables\n## Specific packages\nlibrary(milestones)\n## Initialize defaults\n## Initialize defaults\ncolumn &lt;- lolli_styles()\n\ndata &lt;- read_csv(col_names=TRUE, show_col_types=FALSE, file='rl_time_line.csv')\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\nCode## Sort the table by date\ndata &lt;- data |&gt;\n  arrange(date)\n\n## Build a table\ngt(data) |&gt;\n  #cols_hide(columns = event) |&gt;\n  tab_style(cell_text(v_align = \"top\"),\n            locations = cells_body(columns = date)) |&gt;\n  tab_source_note(source_note = \"Source: Sutton and Barto (2018)\") \n\n\n\n\n\n\ndate\nevent\nReference\n\n\n\n1957\nThe term “optimal control” appears\nMR0090477 Bellman, Richard Dynamic programming. Princeton University Press, Princeton, NJ, 1957. xxv+342 pp.\n\n\n1957\nDefinition of the term MDPs\nMR0091859 Bellman, Richard A Markovian decision process.J. Math. Mech.6(1957), 679–684.\n\n\n1985\nApplication of MDPs\nMR1295629 White, D. J. Markov decision processes.John Wiley & Sons, Ltd., Chichester, 1993. xiv+224 pp. ISBN:0-471-93627-8\n\n\n1988\nApplications of MDPs\nMR1295629 White, D. J. Markov decision processes.John Wiley & Sons, Ltd., Chichester, 1993. xiv+224 pp. ISBN:0-471-93627-8\n\n\n1991\nPartial observable MDP\nMR1105166 Lovejoy, William S.A survey of algorithmic methods for partially observed Markov decision processes.Ann. Oper. Res.28(1991), no.1-4, 47–65.\n\n\n1993\nApplications of MDPs\nMR1200993 White, D. J.Markov decision processes: discounted expected reward or average expected reward?.J. Math. Anal. Appl.172(1993), no.2, 375–384.\n\n\n1996\nNumerical DP\nMR1416619 Rust, John Numerical dynamic programming in economics.Handbook of computational economics, Vol. I, 619–729. Handbooks in Econom., 13 North-Holland Publishing Co., Amsterdam, 1996 ISBN:0-444-89857-3\n\n\n\nSource: Sutton and Barto (2018)\n\n\n\n\n\n\n\nCode## Adjust some defaults\ncolumn$color &lt;- \"orange\"\ncolumn$size  &lt;- 15\ncolumn$source_info &lt;- \"Source: Sutton and Barto (2018)\"\n\n## Milestones timeline\nmilestones(datatable = data, styles = column)\n\n\n\n\n\n\n\n\n\nExercise 4 Consider the following consumption–saving problem with dynamics \\[\n  x_{k+1}\n  = (1+r)(x_k-a_k),\\qquad k=0,1,\\ldots,N-1,\n\\] and utility function\n\\[\n  \\beta^N(x_N)^{1-\\gamma}\n    + \\sum_{k=0}^{N-1}\\beta^k (a_k)^{1-\\gamma}.\n\\]\nShow that the value functions of the DP algorithm take the form \\[J_k(x)=A_k\\beta^kx^{1-\\gamma},\\] where \\(A_N=1\\) and for \\(k=N-1,\\ldots,0\\),\n\\[  A_k = [1 + ((1+r)\\beta A_{k+1})^{1/\\gamma} ]^\\gamma.  \\] Show also that the optimal policies are \\(h_k(x)=A_k^{-1/\\gamma} x\\). for \\(k=N-1,\\ldots,0\\).\n\n\nExercise 5 Consider now the infinite–horizon version of the above consumption–saving problem.\n\nWrite down the associated Bellman equation.\nArgue why a solution to the Bellman equation should be of the form \\[v(x)=cx^{1-\\gamma},\\] where \\(c\\) is constant. Find the constant \\(c\\) and the stationary optimal policy 1.\n\n\n\nExercise 6 Let \\(\\{\\xi_k\\}\\) be a sequence of iid random variables such that \\(E[\\xi]=0\\) and \\(E[\\xi^2]=d\\). Consider the dynamics \\[\n  x_{k+1} = x_k + a_k + \\xi_k, \\qquad\n  k= 0,1,2,\\ldots,\n\\] and the discounted cost \\[\n  E \\sum \\beta^k(a_k^2+x_k^2).\n\\]\n\nWrite down the associated Bellman equation.\nConjecture that the solution to the Bellman equation takes the form \\(v(x)=ax^2+b\\), where \\(a\\) and \\(b\\) are constant.\nDetermine the constants \\(a\\) and \\(b\\).\nConjecture that the solution to the Bellman equation takes the form \\(v(x)=ax^2+b\\), where \\(a\\) and \\(b\\) are constant. Determine the constants \\(a\\) and \\(b\\).",
    "crumbs": [
      "List of Home Works and due dates"
    ]
  },
  {
    "objectID": "homeworks/home_works_list.html#homework-002-due-date-october-25-2024-120000",
    "href": "homeworks/home_works_list.html#homework-002-due-date-october-25-2024-120000",
    "title": "List of Home Works and due dates",
    "section": "Homework 002 due date: October 25, 2024-12:00:00",
    "text": "Homework 002 due date: October 25, 2024-12:00:00\n\nExercise 7 Make the docstrings regarding all python source code for each reported script.\n\nIncremental Implementation\nOptimistic Initial Values\nUpper-Confidence-Bound Action Selection\nGradient Bandit method\nScripts for visualization\n\n\n\nExercise 8 Explain and illustrate the experiments about optimistic initial values. Include the plots similar to Figure 2.2 from (p. 29, Sutton and Barto 2018).\n\n\nExercise 9 Explain and illustrate the experiments about optimistic initial values. Include the plots similar to Figure 2.3 from (p. 34, Sutton and Barto 2018).\n\n\nExercise 10 Explain and describe the experiments about average performance of UCB. Include the plots similar to Figure 2.4 from (p. 36, Sutton and Barto 2018).\n\n\nExercise 11 Explain and describe the experiments about average performance of the gradient bandit algorithm with and without a reward baseline. Include the plots similar to Figure 2.5 from (p. 38 Sutton and Barto 2018).\n\n\nExercise 12 Show that for two actions, the soft-max (Boltzmann or Gibbs) distribution is equivalent to the logistic, or sigmoid, function commonly used in statistics and artificial neural networks.",
    "crumbs": [
      "List of Home Works and due dates"
    ]
  },
  {
    "objectID": "homeworks/home_works_list.html#homework-003-due-date-november-08-2024-120000",
    "href": "homeworks/home_works_list.html#homework-003-due-date-november-08-2024-120000",
    "title": "List of Home Works and due dates",
    "section": "Homework 003 due date: November 08, 2024-12:00:00",
    "text": "Homework 003 due date: November 08, 2024-12:00:00\n\nExercise 13 Suppose \\(\\gamma= 0.5\\) and the following sequence of rewards is received \\(R_1 = 1\\),\\(R_2 = 2\\), \\(R_3 = 6\\), \\(R_4 = 3\\), and \\(R_5 = 2\\), with \\(T = 5\\). What are \\(G_0 , G_1 , \\cdots, G_5\\)? 2\n\n\nExercise 14 Give a table analogous to that in (p.53, Ex. 3.3, Sutton and Barto 2018), but for \\(p(s_0 , r |s, a)\\). It should have columns for \\(s\\), \\(a\\), \\(s_0\\) , \\(r\\), and \\(p(s_0 , r |s, a)\\), and a row for every 4-tuple for which \\(p(s_0 , r |s, a) &gt; 0\\).\n\n\nExercise 15 If the current state is \\(S_t\\) , and actions are selected according to a stochastic policy \\(\\pi\\), then what is the expectation of \\(R_{t+1}\\) in terms of \\(\\pi\\) and the four-argument function \\(p\\) in ((3.2) p.48, Sutton and Barto 2018)?\n\nLet the value function of a state \\(s\\) under a policy \\(\\pi\\) denoted and defined by \\[\n\\begin{aligned}\n    v_{\\pi}(s):=  &\n        \\mathbb{E_{\\pi}}\n        \\left[\n            G_t | S_t = s\n        \\right]\n        \\\\\n        = &\n        \\mathbb{E_{\\pi}}\n        \\left[\n            \\sum_{k=0}^{\\infty}\n                R_{t+1+k}\n                \\Big |\n                S_t = s\n        \\right], \\quad \\text{for all } s\\in \\mathcal{S}.\n\\end{aligned}\n\\] Similarly we denote and define the value of taking action \\(a\\) in state \\(s\\) under a policy \\(\\pi\\) by \\[\n\\begin{aligned}\n    q_{\\pi}(s,a):=  &\n        \\mathbb{E_{\\pi}}\n        \\left[\n            G_t | S_t = s, A_t = a\n        \\right]\n        \\\\\n        = &\n        \\mathbb{E_{\\pi}}\n        \\left[\n            \\sum_{k=0}^{\\infty}\n                R_{t+1+k}\n                \\Big |\n                S_t = s, A_t = a\n        \\right], \\quad \\text{for all } (s,a) \\in \\mathcal{S}\\times \\mathcal{A}(s).\n\\end{aligned}\n\\]\n\nExercise 16 Give an equation for \\(v_{\\pi}\\) in terms of \\(q_{\\pi}\\) and \\(\\pi\\).\n\n\nExercise 17  \n\nGive an equation for \\(q_{\\pi}\\) in terms of \\(v_{\\pi}\\) and the four-argument \\(p(\\cdot,\\cdot|\\cdot,\\cdot)\\)\n\nGive the Bellman equation for \\(q_{\\pi}\\) for the recycling robot.\n\n\n\nExercise 18 Document the numerical experiment regarding to the Gridworld example\n\n\n\n\n\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second. Adaptive Computation and Machine Learning. MIT Press, Cambridge, MA.",
    "crumbs": [
      "List of Home Works and due dates"
    ]
  },
  {
    "objectID": "homeworks/home_works_list.html#footnotes",
    "href": "homeworks/home_works_list.html#footnotes",
    "title": "List of Home Works and due dates",
    "section": "",
    "text": "Hint: Insert \\(v(x)=cx^{1-\\gamma}\\) into the Bellman equation and solve the minimization problem.↩︎\nHint: Work backwards.↩︎",
    "crumbs": [
      "List of Home Works and due dates"
    ]
  }
]