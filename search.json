[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "1 Outline\nThis notes are based in the course from Berstekas for the MIT see all lectures and other resources for complete the understanding.\nThe textbook for chapter one is Bertsekas’ book [1]. Chapters 2 and 3 are adapted from Sutton’s book [Ch. 3, Ch. 4, 5]. For application and broad connection with more machine learning applications, we refer to [3]. Also, we recommend a handbook of algorithms [6]. For applications with implemented code, we follow the books [2,4]. The source code for multiarmed bandits algorhims: https://github.com/terrence-ou/Reinforcement-Learning-2nd-Edition-Notes-Codes.git",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "index.html#bibliography",
    "href": "index.html#bibliography",
    "title": "Preface",
    "section": "1.1 Bibliography",
    "text": "1.1 Bibliography",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "01-introduction/general_intro.html",
    "href": "01-introduction/general_intro.html",
    "title": "Abstract",
    "section": "",
    "text": "To fix ideas\nWe present an introduction to the solution of multi-stage optimization problems. Starting from the dynamic programming algorithm, we consider theoretical and computational aspects, mainly of deterministic problems, and discuss how to generalize some of the results to Markovian decision processes.\nThe classic problem with which the essential ideas of dynamic programming can be introduced is the optimal route problem (shortest route considering distances,or cheapest route considering costs) [1,2]. A simple version of this problem is shown schematically in Figure fig-brandimarte_net. By exhaustion, we deploy the cost of path in Table tbl-brandimarte_paths.\nIn this problem, the optimal route to go from the node labeled \\(0\\) to the node labeled \\(7\\) is to be determined. The costs between each pair of nodes connected by an arrow are represented by a number next to it. For example, the cost to go from node 0 to node 2 is 6. The optimal route will be the one for which the sum of their costs is minimum. This optimal route can be obtained by an exhaustive enumeration; generating all possible routes from node \\(0\\) to node \\(7\\) and choosing the minimum cost route.\nThe optimal path is \\[\n  0 \\to   1 \\to 3 \\to 5 \\to  7,\n\\] with cost 16.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abstract</span>"
    ]
  },
  {
    "objectID": "01-introduction/general_intro.html#bibliography",
    "href": "01-introduction/general_intro.html#bibliography",
    "title": "Abstract",
    "section": "Bibliography",
    "text": "Bibliography",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Abstract</span>"
    ]
  },
  {
    "objectID": "02-introductionToRL/intro.html",
    "href": "02-introductionToRL/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Trial-Error\nReinforcement Learning is part of a decades-long trend within artificial intelligence and machine learning toward greater integration with statistics, optimization, and other mathematical subjects. For example, the ability of some reinforcement learning methods to learn with parameterized approximators addresses the classical “curse of dimensionality” in operations research and control theory. More distinctively, reinforcement learning has also interacted strongly with psychology and neuroscience, with substantial benefits going both ways. Of all the forms of machine learning, reinforcement learning is the closest to the kind of learning that humans and other animals do, and many of the core algorithms of reinforcement learning were originally inspired by biological learning systems.\nAccording to Richard S. Sutton and Andrew G. Barto [1]–the first authors to use the term–Reinforced Learning Reinforcement learning is about what to do, that is, how to map situations to action so that we optimize a reward. The learner must discover which action yield the best reward by trying them. In the most general sense, action may not only affect immediate reward but also the next situation and, through that, all subsequent rewards.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-introductionToRL/intro.html#sensation-action-and-goal",
    "href": "02-introductionToRL/intro.html#sensation-action-and-goal",
    "title": "Introduction",
    "section": "Sensation action and goal",
    "text": "Sensation action and goal\nAt the same time, Reinforcement Learning encloses a problem, a class of solution methods, and the field that studies this problem and its solutions. Its formalism is based on the theory of controlled dynamical systems, with a strong focus on the optimal control of partially known Markov decision processes. Then, the core idea consists of capturing the essence of the problem when an agent learns through experience and interaction to reach a goal. This agent can sense the state of its environment to some extent and must be able to take action that affects the state. The agent also must have a goal or goals related to the state of the environment.\nMDPs are designed to incorporate three essential elements: sensation, action, and goal. Therefore, any approach suitable for solving such problems should be considered a potential method for Reinforcement Learning.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-introductionToRL/intro.html#bibliography--1",
    "href": "02-introductionToRL/intro.html#bibliography--1",
    "title": "Introduction",
    "section": "Refrences",
    "text": "Refrences",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-introductionToRL/intro.html#bibliography--5",
    "href": "02-introductionToRL/intro.html#bibliography--5",
    "title": "Introduction",
    "section": "Refrences",
    "text": "Refrences",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-introductionToRL/intro.html#bibliography",
    "href": "02-introductionToRL/intro.html#bibliography",
    "title": "Introduction",
    "section": "Bibliography",
    "text": "Bibliography",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html",
    "href": "03-multiArmedBandit/multiarmed_bandits.html",
    "title": "Multi-armed Bandits",
    "section": "",
    "text": "A \\(k\\)-armed Bandit Problem\nA very important feature distinguishing reinforcement learning from other types of learning is that it uses training information to evaluate the actions taken, rather than instruct by giving correct actions.\nWe consider the following setup:\nWe denote the action selected on time step \\(t\\) as \\(A_t\\) and the corresponding reward as \\(R_t\\). Each of the \\(k\\) actions has an expected or mean reward given that that action is selected; let us call this the value of that action.\nThe value then of an arbitrary action \\(a\\), denoted \\(q_{*} (a)\\), is the expected reward given that \\(a\\) is selected:\n\\[\n    q_{*}(a): = \\mathbb{E} \\left[ R_t | A_t =a\\right].\n\\]\nIf you knew the value of each action, then we solve the \\(k\\)-armed bandit problem—you would always select the action with highest value.\nWe assume that you may not have precise knowledge of the action values, although you may have some estimates. We denote this estimated value of action \\(a\\) at time step \\(t\\) as \\(Q_t(a)\\). Thus, we would like that \\[\n    Q_t(a) \\approx q_{*}(a).\n\\]\nIf you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are exploiting your current knowledge of the values of the actions. If instead you select one of the non-greedy actions, then we say you are exploring, because this enables you to improve your estimate of the non-greedy action’s value.\nExploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.\nReward is lower in the short run, during exploration, but higher in the long run because after you have discovered the better actions, you can exploit them many times. Because it is not possible both to explore and to exploit with any single action selection, one often refers to the “conflict” between exploration and exploitation.\nIn any specific case, whether it is better to explore or exploit depends in a complex way on the precise values of the estimates, uncertainties, and the number of remaining steps. There are many sophisticated methods for balancing exploration and exploitation for particular mathematical formulations of the \\(k\\)-armed bandit and related problems.\nHowever, most of these methods make strong assumptions about stationary and prior knowledge that are either violated or impossible to verify in most applications.\nThe guarantees of optimality or bounded loss for these methods offer little comfort when the assumptions of their theory do not apply.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#set-up",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#set-up",
    "title": "Multi-armed Bandits",
    "section": "Set up",
    "text": "Set up\n\n\n\n\n\n\nThe experiment runs as follows.\n\n\n\n\nConsider a \\(k\\)-bandit problem with \\(k=10\\)\nFor each bandit problem, the action values\n\n\\[\n  q_{*}(a) \\sim \\mathcal{N}(0,1)\n\\]\n\nThen when choosing an action \\(A_t\\) the corresponding reward \\(R_t\\) is sampling from a Gaussian distribution \\[\nR_t \\sim \\mathcal{N}(q_{*}(A_t), 1)  \n\\]\n\n\n\n\n\nk_armed_testbed.py\n\n#k_armed_testbed.py\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\n# Randomly sample mean reward for each action\nmeans = np.random.normal(size=(10, ))\n\n# Generate sample data based on normal distribution\ndata = [np.random.normal(mean, 1.0, 2000) for mean in means]\n\n# Create violin plot\nplt.figure(figsize=(8, 6), dpi=150)\nplt.violinplot(\n  dataset=data,\n  showextrema=False,\n  showmeans=False,\n  points=2000\n)\n\n# Draw mean marks\nfor i, mean in enumerate(means):\n    idx = i + 1\n    plt.plot([idx - 0.3, idx + 0.3], [mean, mean],\n             c='black',\n             linewidth=1)\n    plt.text(idx + 0.2, mean - 0.2, \n             s=f\"$q_*({idx})$\",\n             fontsize=8)\n\n# Draw 0-value dashed line\nplt.plot(np.arange(0, 12), np.zeros(12), \n            c='gray', \n            linewidth=0.5,\n            linestyle=(5, (20, 10)))\nplt.tick_params(axis='both', labelsize=10)\nplt.xticks(np.arange(1, 11))\n\n# get rid of the frame\nfor i, spine in enumerate(plt.gca().spines.values()):\n    if i == 2: continue\n    spine.set_visible(False)\n    \n\n# Draw labels\nlabel_font = {\n    'fontsize': 12,\n    'fontweight': 'bold'\n}\n\nplt.xlabel('Action', fontdict=label_font)\nplt.ylabel('Reward distribution', fontdict=label_font)\nplt.margins(0)\n\nplt.tight_layout()\nplt.show()\n\nWe consider a set of 2000 randomly generated \\(k\\)-armed bandit problems with \\(k\\) = 10. For each bandit problem, such as the one shown in the output of the above code. The action values, \\(q_{*} (a), a = 1, . . . , 10\\), were selected according to a normal (Gaussian) distribution with mean 0 and variance 1. Thus when we apply a learning method to this problem, the selected action \\(A_t\\) a time step \\(t\\) the regarding reward \\(R_t\\) is sampling from a normal distribution \\[\n  R_{t} \\sim \\mathcal{N}(q_{*}(A_t), 1).\n\\] Sutton and Barto [1] calls this suite of test tasks the 10-armed test-bed. By using this suit of benchmarks, we can measure the performance of any learning method. In fact we also can observe its behavior while the learning improves with experience of 1000 time steps, when it is applied to a selected bandit of this bed. This makes up one run. Thus, if we iterate 2000 independent runs, each with different bandit problem, we can obtain a measure of learning algorithm’s average behavior.\n\n[1] R.S. Sutton, A.G. Barto, Reinforcement learning: An introduction, Second, MIT Press, Cambridge, MA, 2018.\nNext we code functions to deploy the above experiment with \\(\\epsilon\\)-greedy actions\n\n\nutils.py\n\nfrom typing import Any\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom numpy import dtype, ndarray, signedinteger\n\n\n# Get the action with the max Q value\ndef get_argmax(G:np.ndarray) -&gt; ndarray[Any, dtype[signedinteger[Any]]]:\n    candidates = np.argwhere(G == G.max()).flatten()\n    # return the only index if there's only one max\n    if len(candidates) == 1:\n        return candidates[0]\n    else:\n        # instead break the tie randomly\n        return np.random.choice(candidates)\n\n\n# Select arm and get the reward\ndef bandit(q_star:np.ndarray, \n           act:int) -&gt; tuple:\n    real_rewards = np.random.normal(q_star, 1.0)\n    # optim_choice = int(real_rewards[act] == real_rewards.max())\n    optim_choice = int(q_star[act] == q_star.max())\n    return real_rewards[act], optim_choice\n\nPlease save the above script as utils.py in the firs level of the regrding project such that we can imported by the ist name fora example by from utils import bandit, plots",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#bibliography--3",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#bibliography--3",
    "title": "Multi-armed Bandits",
    "section": "Refrences",
    "text": "Refrences",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#incremental-update-formula-for-action-value-estimation",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#incremental-update-formula-for-action-value-estimation",
    "title": "Multi-armed Bandits",
    "section": "Incremental Update Formula for Action-Value Estimation",
    "text": "Incremental Update Formula for Action-Value Estimation\nLet \\(R_i\\) denote the reward received after the \\(i\\)-th selection of the action.\\(Q_n\\) denote the estimate of the action value after the action has been chosen \\(n-1\\) times.\nInstead of computing \\(Q_n\\) as the sample average of all observed rewards (which requires storing and summing all rewards), we use the incremental formula:\n\\[\n    Q_n = Q_{n-1} + \\alpha \\left(R_n - Q_{n-1}\\right)\n\\]\nWhere: \\(Q_{n-1}\\) is the previous estimate of the action value. \\(R_n\\) is the reward received on the \\(n\\)-th selection. \\(\\alpha\\) is a constant step size, often set as \\(\\dfrac{1}{n}\\) to mimic the behavior of sample averaging when the number of observations grows.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#derivation-of-the-incremental-formula",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#derivation-of-the-incremental-formula",
    "title": "Multi-armed Bandits",
    "section": "Derivation of the Incremental Formula",
    "text": "Derivation of the Incremental Formula\n\nStart with the definition of the action value as the sample mean: \\(\\displaystyle Q_n =\\dfrac{1}{n} \\sum_{i=1}^{n} R_i\\)\nExpress \\(Q_n\\) in terms of \\(Q_{n-1}\\): \\[\nQ_n = \\frac{1}{n}\n\\left[ \\sum_{i=1}^{n-1} R_i + R_n \\right]\n\\]\n\nThis can be rearranged as: \\[\n        Q_n = \\frac{n-1}{n} \\cdot Q_{n-1} +\n    \\frac{1}{n} \\cdot R_n.\n\\]\n\nNotice that \\(\\displaystyle\n    \\frac{n-1}{n} \\cdot Q_{n-1} = Q_{n-1} - \\frac{1}{n}\\cdot Q_{n-1}\\), so: \\[\n    Q_n = Q_{n-1} + \\frac{1}{n} \\left(R_n -Q_{n-1}\\right)\n\\]\n\nHere, \\(\\alpha = \\dfrac{1}{n}\\) adapts to the number of observations, ensuring the update balances the influence of new and past rewards.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#advantages-of-the-incremental-method",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#advantages-of-the-incremental-method",
    "title": "Multi-armed Bandits",
    "section": "Advantages of the Incremental Method",
    "text": "Advantages of the Incremental Method\n\nConstant Memory: The method only requires storing \\(Q_{n-1}\\) and \\(R_n\\), avoiding the need to keep all past rewards.\nFixed Computation: Each update involves a fixed, small number of operations, regardless of \\(n\\).\n\nThis approach efficiently updates the action-value estimate with minimal resources, making it suitable for online learning algorithms and scenarios where computational efficiency is critical.\n\n\n\nIncremental algorithm\n\n\nBellow a python implementation.\n\n\nexample_2_2_bandits_algo.py\n\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.use('qt5agg')\nimport pickle\n\nfrom utils import get_argmax, bandit\n\n#SEED = 123456\n#np.random.seed(SEED)\n\n# running the k-armed bandit algorithm\ndef run_bandit(K:int, \n            q_star:np.ndarray,\n            rewards:np.ndarray,\n            optim_acts_ratio:np.ndarray,\n            epsilon:float, \n            num_steps:int=1000) -&gt; None:\n    \n    Q = np.zeros(K)     # Initialize Q values\n    N = np.zeros(K)     # The number of times each action been selected\n    ttl_optim_acts = 0\n\n    for i in range(num_steps):\n        # get action\n        A = None\n        if np.random.random() &gt; epsilon:\n            A = get_argmax(Q)\n        else:\n            A = np.random.randint(0, K)\n        \n        R, is_optim = bandit(q_star, A)\n        N[A] += 1\n        Q[A] += (R - Q[A]) / N[A]\n\n        ttl_optim_acts += is_optim\n        rewards[i] = R\n        optim_acts_ratio[i] = ttl_optim_acts / (i + 1)\n\n\nif __name__ == \"__main__\":\n\n    # Initializing the hyperparameters\n    K = 10  # Number of arms\n    epsilons = [0.0, 0.01, 0.1]\n    num_steps = 1000\n    total_rounds = 1000\n\n    # Initialize the environment\n    q_star = np.random.normal(loc=0, scale=1.0, size=K)\n    rewards = np.zeros(shape=(len(epsilons), total_rounds, num_steps))\n    optim_acts_ratio = np.zeros(shape=(len(epsilons), total_rounds, num_steps))\n    \n    # Run the k-armed bandits alg.\n    for i, epsilon in enumerate(epsilons):\n        for curr_round in range(total_rounds):\n            run_bandit(K, q_star, \n                       rewards[i, curr_round], \n                       optim_acts_ratio[i, curr_round], \n                       epsilon, \n                       num_steps)\n    \n    rewards = rewards.mean(axis=1)\n    optim_acts_ratio = optim_acts_ratio.mean(axis=1)\n\n    record = {\n        'hyper_params': epsilons, \n        'rewards': rewards,\n        'optim_acts_ratio': optim_acts_ratio\n    }\n\n    fig_01, ax_01 = plt.subplots()\n    fig_02, ax_02 = plt.subplots()\n    for i, ratio in enumerate(optim_acts_ratio):\n        ax_01.plot(\n                ratio,\n                label=r'$\\epsilon={epsilon_i}$'.format(epsilon_i=epsilons[i])\n        )\n    \n    for i, reward in enumerate(rewards):\n        ax_02.plot(\n                reward,\n                label=r'$\\epsilon={epsilon_i}$'.format(epsilon_i=epsilons[i])\n                )\n    ax_01.set_xlabel(r'$t$', fontsize=12)\n    ax_01.set_ylabel(r'Optimal Action', fontsize=12)\n    ax_01.legend(loc='best')\n    ax_02.set_xlabel(r'$t$', fontsize=12)\n    ax_02.set_ylabel(r'Reward', fontsize=12)\n    ax_02.legend(loc='best')\n    plt.show()\n    \n    # with open('./history/record.pkl', 'wb') as f:\n    #     pickle.dump(record, f)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#scripts-for-visualization",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#scripts-for-visualization",
    "title": "Multi-armed Bandits",
    "section": "Scripts for visualization",
    "text": "Scripts for visualization\nRun the following script and save the output.\n\n\nplot_gradient.py\n\n\nimport matplotlib.pyplot as plt\nimport pickle\nimport numpy as np\n\n\n# Plot results\ndef plot(\n        data: np.ndarray,\n        legends: list,\n        xlabel: str,\n        ylabel: str,\n        filename: str = None,\n        fn=lambda: None, ) -&gt; None:\n    fontdict = {\n            'fontsize': 12,\n            'fontweight': 'bold',\n            }\n    \n    plt.figure(figsize=(10, 6), dpi=150)\n    plt.grid(c='lightgray')\n    plt.margins(0.02)\n    \n    # revers the loop for a better visualization\n    colors = ['navy', 'lightblue', 'tomato', 'pink']\n    for i in range(len(data) - 1, -1, -1):\n        # data[i] = uniform_filter(data[i])\n        plt.plot(data[i], label=legends[i], linewidth=1.5, c=colors[i])\n    \n    # get rid of the top/right frame lines\n    for i, spine in enumerate(plt.gca().spines.values()):\n        if i in [0, 2]:\n            spine.set_linewidth(1.5)\n            continue\n        spine.set_visible(False)\n    \n    plt.tick_params(axis='both', labelsize=10)\n    plt.xlabel(xlabel, fontdict=fontdict)\n    plt.ylabel(ylabel, fontdict=fontdict)\n    # plt.legend(loc=4, fontsize=13)\n    fn()\n    \n    plt.text(500, 57, s=\"$\\\\alpha = 0.4$\", c=colors[3], fontsize=14)\n    plt.text(500, 28, s=\"$\\\\alpha = 0.4$\", c=colors[1], fontsize=14)\n    plt.text(900, 72, s=\"$\\\\alpha = 0.1$\", c=colors[2], fontsize=14)\n    plt.text(900, 52, s=\"$\\\\alpha = 0.1$\", c=colors[0], fontsize=14)\n    \n    plt.text(770, 65, s=\"with baseline\", c=colors[2], fontsize=12)\n    plt.text(770, 42, s=\"without baseline\", c=colors[0], fontsize=12)\n    \n    if not filename:\n        plt.show()\n    else:\n        plt.savefig(f'./plots/{filename}')\n\n\ndef plot_result(\n        optim_ratio: np.ndarray,\n        legends: list,\n        output_name: str = None\n        ):\n    # Set tick labels\n    fn = lambda: plt.yticks(\n        np.arange(0, 100, 10), labels=[f'{val}%' for val in range(0, 100, 10)]\n        )\n    plot(\n        optim_ratio,\n        legends,\n        xlabel='Time step',\n        ylabel='% Optimal actions',\n        filename=output_name,\n        fn=fn\n        )\n\n\nif __name__ == \"__main__\":\n    with open('./history/sga_record.pkl', 'rb') as f:\n        history = pickle.load(f)\n    \n    optim_ratio = history['optim_acts_ratio'] * 100\n    hyper_params = history['hyper_params']\n    \n    # plot_result(optim_ratio, hyper_params, output_name=\"example_2_5_sga.png\")\n    plot_result(optim_ratio, hyper_params, output_name=None)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#summary",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#summary",
    "title": "Multi-armed Bandits",
    "section": "Summary",
    "text": "Summary\nOutline:\n\n\\(\\epsilon\\)-greedy Method:\n\nOverview of the method.\nMathematical formulation.\nParameter sensitivity and tuning.\n\nUCB (Upper Confidence Bound) Method:\n\nExplanation of UCB and its deterministic exploration mechanism.\nThe mathematical equation governing UCB.\nParameter considerations and impact on performance.\n\nGradient Bandit Algorithm:\n\nIntroduction to action preferences and how they differ from action values.\nDerivation of the softmax probability distribution.\nDiscussion on parameter choice and influence on outcomes.\n\nOptimistic Initialization:\n\nHow optimistic estimates influence exploration.\nComparison with \\(\\epsilon\\)-greedy methods.\n\n\nLet’s begin with the \\(\\epsilon\\) -greedy method:\n\n1. \\(\\epsilon\\)-greedy Method\nThis is one of the simplest algorithms to balance exploration and exploitation. The method works by choosing a random action with probability \\(\\epsilon\\) (exploration) and the action with the highest estimated value (exploitation) with probability \\(1 - \\epsilon\\).\n\nMathematical Formulation\n\nLet \\(Q(a)\\) be the estimated value of action \\(a\\).\nAt each time step \\(t\\), the agent chooses:\n\nA random action \\(a\\) with probability \\(\\epsilon\\).\nThe action with the maximum \\(Q(a)\\),\n\\(\\text{argmax}_a Q(a)\\), with probability \\({1 - \\epsilon}.\\)\n\n\n\n\nUpdate Rule\nThe estimate for the value of an action, \\(Q(a)\\), is updated using the following equation after observing a reward \\(R_t\\) for taking action \\(a\\): \\[\nQ_{t+1}(a) = Q_t(a) + \\alpha \\left( R_t - Q_t(a) \\right)\n\\]\nWhere:\n\n\\(\\alpha\\) is the step size or learning rate, determining how much the estimate is updated based on new information.\n\\(R_t\\) is the reward received after action \\(a\\) at time \\(t\\).\n\n\n\nParameter Sensitivity\n\n\\(\\epsilon\\): A small value of \\(\\epsilon\\) (e.g., 0.01) results in a mostly greedy policy with occasional exploration, while a larger \\(\\epsilon\\) (e.g., 0.1) encourages moreexploration. The optimal value balances sufficient exploration to discover rewarding actions while exploiting known high-value actions effectively.\n\n\n\n\n2. UCB (Upper Confidence Bound) Method\nUCB addresses the exploration-exploitation dilemma by adding a confidence term to the action value. The idea is to choose actions that might not have the highest estimated value but have been less explored, thus increasing exploration in a systematic way.\n\nMathematical Equation\nThe action \\(a_t\\) selected at time \\(t\\) is:\n\\[a_t = \\text{argmax}_a\n    \\left(\n        Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right)\n\\]\nWhere:\n\n\\(Q_t(a)\\) is the estimated value of action \\(a\\) at time \\(t\\).\n\\(c\\) is a parameter controlling the degree of exploration. A larger \\(c\\) increases exploration.\n\\(N_t(a)\\) is the number of times action \\(a\\) has been selected so far.\n\\(\\ln t\\) scales the confidence bound logarithmically with time.\n\nThis approach encourages the selection of actions with high uncertainty (lower ( N_t(a) )), balancing exploration based on how frequently each action has been tried.\n\n\nParameter Considerations\n\nThe parameter \\(c\\) is crucial. If \\(c\\) is too low, the algorithm might not explore enough; if too high, it may explore excessively. The optimal \\(c\\) varies depending on the problem setting.\n\n\n\n\n3. Gradient Bandit Algorithm\nUnlike \\(\\epsilon\\)-greedy and UCB methods that estimate action values, gradient bandit algorithms estimate action preferences, denoted \\(H(a)\\). These preferences are used to determine the probability of selecting each action.\n\nSoftmax Distribution\nThe probability of selecting action \\(a\\) is given by: \\[\n    \\pi(a) =\\dfrac{e^{H(a)}}{\\displaystyle \\sum_{b=1} ^{k} e^{H(b)}}\n\\]\nWhere:\n\n\\(H(a)\\) is the preference for action \\(a\\).\n\\(\\pi(a)\\) represents the probability of taking action \\(a\\).\n\n\n\nUpdate Rule\nThe preferences are updated based on the received reward as follows:\n\\[\n\\begin{aligned}\n    H_{t+1}(a) &=\n        H_t(a) + \\alpha (R_t - \\bar{R}_t)(1 - \\pi_t(a))\n        \\quad \\text{if action } a \\text{ was chosen}\n        \\\\  \n    H_{t+1}(b) &=\n        H_t(b) - \\alpha (R_t - \\bar{R}_t)\\pi_t(b)\n        \\quad \\text{for all other actions } b\n\\end{aligned}\n\\]\nWhere:\n\n\\(\\alpha\\) is the learning rate.\n\\(\\bar{R}_t\\) is the average reward received so far, acting as a baseline to stabilize learning.\n\nThis update rule encourages actions that receive above-average rewards while discouraging less rewarding ones.\n\n\n\n4. Optimistic Initialization\nThis is a simple method where the initial estimates \\(Q_0(a)\\) are set to high values, encouraging the algorithm to explore different actions because all initial action values seem promising.\n\nComparison with \\(\\epsilon\\)-greedy\n\nUnlike \\(\\epsilon\\)-greedy, which uses a random chance for exploration, optimistic initialization drives exploration until the agent converges on accurate value estimates.\nIt is especially useful when the reward distribution is unknown but expected to have some higher values.\n\n\n\n\nFinal Remarks:\nTo determine which algorithm is most effective in practice:\n\nA parameter study is essential, as highlighted in the passage. This involves varying parameters (like \\(\\epsilon\\), \\(c\\), or \\(\\alpha\\)) to find the optimal range for each algorithm.\nLearning curves provide insight into how each algorithm performs over time. Averaging these curves over several runs ensures statistical reliability.\n\n\n\nexample_2_6_summary.py\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom collections import namedtuple\nimport tqdm\nimport pickle\n\nfrom example_2_2_bandits_algo import run_bandit as e_greedy\nfrom example_2_3_OIV import run_bandit as OIV\nfrom example_2_4_UCB import run_bandit_UCB as UCB\nfrom example_2_5_gradient import run_bandit as gradient\n\nSEED = 200\nnp.random.seed(SEED)\n\n\n# A wrapper function for running different algorithms\ndef run_algorithm(\n        fn_name: str,\n        fn: 'function',\n        params: np.ndarray,\n        args: dict,\n        total_rounds: int\n        ) -&gt; np.ndarray:\n    global hyper_param\n    if fn_name == 'e_greedy':\n        hyper_param = 'epsilon'\n    elif fn_name == 'ucb':\n        hyper_param = 'c'\n    elif fn_name == 'gradient':\n        hyper_param = 'alpha'\n    elif fn_name == 'oiv':\n        hyper_param = 'init_val'\n    \n    args[hyper_param] = None\n    \n    rewards_hist = np.zeros(\n        shape=(len(params), total_rounds, args['num_steps'])\n        )\n    optm_acts_hist = np.zeros_like(rewards_hist)\n    for i, param in tqdm.tqdm(enumerate(params), desc=fn_name, position=0):\n        args[hyper_param] = param\n        for curr_round in tqdm.tqdm(\n                range(total_rounds),\n                desc=f'{fn_name}: {param}',\n                position=i+1,\n                leave=True\n                ):\n            fn(\n                **args,\n                rewards=rewards_hist[i, curr_round],\n                optim_acts_ratio=optm_acts_hist[i, curr_round]\n                )\n    print('\\n')\n    return rewards_hist.mean(axis=1).mean(axis=1)\n\n\nif __name__ == \"__main__\":\n    K = 10\n    num_steps = 1000\n    total_rounds = 2000\n    q_star = np.random.normal(loc=0, scale=1.0, size=K)\n    \n    # Creating parameter array: [1/128, 1/64, 1/32, 1/16, ...]\n    multiplier = np.exp2(np.arange(10))\n    params = np.ones(10) * (1 / 128)\n    params *= multiplier\n    x_labels = ['1/128', '1/64', '1/32', '1/16', '1/8', '1/4', '1/2', '1', '2',\n                '4']\n    \n    # Creating a dict to record running histories\n    records = {\n            'params': params,\n            'x_labels': x_labels\n            }\n    history = namedtuple('history', ['bounds', 'data'])\n    \n    base_args = {\n            'K': K,\n            'q_star': q_star,\n            'num_steps': num_steps\n            }\n    \n    # ======== e_greedy ========\n    eps_bounds = [0, 6]\n    fn_params = params[eps_bounds[0]: eps_bounds[1]]\n    \n    eps_rewards = run_algorithm(\n        'e_greedy', e_greedy, fn_params, base_args.copy(), total_rounds\n        )\n    records['e_greedy'] = history(eps_bounds, eps_rewards)\n    \n    # ======== UCB ========\n    ucb_bounds = [3, 10]\n    fn_params = params[ucb_bounds[0]: ucb_bounds[1]]\n    \n    ucb_rewards = run_algorithm(\n        'ucb', UCB, fn_params, base_args.copy(), total_rounds\n        )\n    records['ucb'] = history(ucb_bounds, ucb_rewards)\n    \n    # ======== Gradient ========\n    gd_bounds = [2, 9]\n    fn_params = params[gd_bounds[0]:gd_bounds[1]]\n    gd_args = base_args.copy()\n    gd_args['baseline'] = True\n    \n    gd_rewards = run_algorithm(\n        'gradient', gradient, fn_params, gd_args, total_rounds\n        )\n    records['gradient'] = history(gd_bounds, gd_rewards)\n    \n    # ======== OIV ========\n    oiv_bounds = [5, 10]\n    fn_params = params[oiv_bounds[0]:oiv_bounds[1]]\n    oiv_args = base_args.copy()\n    oiv_args['epsilon'] = 0.0\n    oiv_rewards = run_algorithm('oiv', OIV, fn_params, oiv_args, total_rounds)\n    records['oiv'] = history(oiv_bounds, oiv_rewards)\n    \n    with open('./history/summary.pkl', 'wb') as f:\n        pickle.dump(records, f)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#visualization",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#visualization",
    "title": "Multi-armed Bandits",
    "section": "Visualization",
    "text": "Visualization\n\n\nplot_sumary.py\n\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom collections import namedtuple\n\nhistory = namedtuple('history', ['bounds', 'data'])\nalgos = ['e_greedy', 'gradient', 'ucb', 'oiv']\n\nif __name__ == '__main__':\n    \n    with open('./history/summary.pkl', 'rb') as f:\n        histories = pickle.load(f)\n        coords = [[0.95, 1.55], [6.5, 1.45], [3, 1.82], [8.5, 1.82]]\n        legend_loc = 2\n        filename = './plots/example_2_6_summary.png'\n    \n    # with open('./history/exercise_2_6.pkl', 'rb') as f:\n    #     histories = pickle.load(f)\n    #     coords = [[2.5, 6.0], [7.0, 3.5], [7.5, 5.0], [6.5, 5.7]]\n    #     legend_loc = 0\n    #     filename = './plots/exercise_2_6.png'\n    \n    x_ticks = histories['x_labels']\n    \n    plt.figure(figsize=(10, 6), dpi=150)\n    plt.grid(c='lightgray')\n    plt.margins(0.02)\n    \n    fontdict = {\n            'fontsize': 12,\n            'fontweight': 'bold',\n            }\n    \n    legends = ['$\\epsilon$', '$\\\\alpha$', '$c$', '$Q_0$']\n    colors = ['tomato', 'mediumseagreen', 'steelblue', 'orchid']\n    \n    for i, key in enumerate(algos):\n        record = histories[key]\n        bounds = record.bounds\n        data = record.data\n        \n        plt.plot(\n            np.arange(bounds[0], bounds[1]), data, label=legends[i], c=colors[i]\n            )\n    \n    for i, spine in enumerate(plt.gca().spines.values()):\n        if i in [0, 2]:\n            spine.set_linewidth(1.5)\n            continue\n        spine.set_visible(False)\n    \n    plt.tick_params(axis='both', labelsize=10)\n    plt.xticks(np.arange(10), x_ticks)\n    \n    # x labels\n    plt.legend(loc=legend_loc, fontsize=12, title='Hyper Param.')\n    plt.xlabel('Hyper parameter value', fontdict=fontdict)\n    plt.ylabel(\n        'Average reward over first 1000 steps',\n        fontdict=fontdict\n        )\n    \n    plt.text(*coords[0], '$\\epsilon$-greedy', c=colors[0], fontsize=12)\n    plt.text(\n        *coords[1], 'gradient\\nbandit', c=colors[1], fontsize=12,\n        horizontalalignment='center'\n        )\n    plt.text(*coords[2], 'UCB', c=colors[2], fontsize=12)\n    plt.text(\n        *coords[3], 'greedy with\\noptimistic\\ninitializatio\\n$\\\\alpha=0.1$',\n        c=colors[3], fontsize=12, horizontalalignment='center'\n        )\n    \n    # plt.show()\n    plt.savefig(filename)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "03-multiArmedBandit/multiarmed_bandits.html#bibliography",
    "href": "03-multiArmedBandit/multiarmed_bandits.html#bibliography",
    "title": "Multi-armed Bandits",
    "section": "Bibliography",
    "text": "Bibliography",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multi-armed Bandits</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html",
    "href": "04-finiteMDPs/mdp.html",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "",
    "text": "The Agent–Environment Interface\nThat is what ChatGPT would answer to a 5-year-old kid. Alright, let’s imagine you have a little robot friend named Robo. Robo likes to explore and do different things, but Robo doesn’t always know what to do next. A Markov Decision Process (MDP) is like giving Robo a set of rules to help it decide what to do next based on where it is and what it knows.\nImagine Robo is in a room full of toys. Each toy is like a different choice Robo can make, like playing with blocks or reading a book. But Robo can’t see the whole room at once, so it has to decide what to do based on what it can see and remember.\nIn an MDP, Robo learns from its past experiences. If it finds that playing with blocks usually makes it happy, it’s more likely to choose that again next time. But if it tries reading a book and doesn’t like it, it might choose something else next time.\nSo, a Markov Decision Process helps Robo make decisions by learning from what it’s done before and what it can see around it, kind of like how you learn from playing with different toys and remembering which ones you like best.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#the-agentenvironment-interface",
    "href": "04-finiteMDPs/mdp.html#the-agentenvironment-interface",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "",
    "text": "Example 1 A mobile robot has the job of collecting empty soda cans in an office environment. It has sensors for detecting cans, and an arm and gripper that can pick them up and place them in an onboard bin; it runs on a rechargeable battery. The robot’s control system has components for interpreting sensory information, for navigating, and for controlling the arm and gripper. High-level decisions about how to search for cans are made by a reinforcement learning agent based on the current charge level of the battery. To make a simple example, we assume that only two charge levels can be distinguished, comprising a small state set \\(\\mathcal{S} = \\{\\texttt{high}, \\texttt{low} \\}\\). In each state, the agent can decide whether to\n\nactively search for a can for a certain period of time,\nremain stationary and wait for someone to bring it a can, or\nhead back to its home base to recharge its battery.\n\nWhen the energy level is high, recharging would always be foolish h, so we do not include it in the action set for this state. The action sets are then \\(\\mathcal{A}(\\texttt{high}) = \\{\\texttt{search}, \\texttt{wait}\\}\\) and \\(\\mathcal{A}(\\texttt{low}) = \\{\\texttt{search}, \\texttt{wait}, \\texttt{recharge}\\}\\).\nThe rewards are zero most of the time, but become positive when the robot secures an empty can, or large and negative if the battery runs all the way down. The best way to find cans is to actively search for them, but this runs down the robot’s battery, whereas waiting does not. Whenever the robot is searching, the possibility exists that its battery will become depleted. In this case the robot must shut down and wait to be rescued (producing a low reward).\nIf the energy level is high, then a period of active search can always be completed without risk of depleting the battery. A period of searching that begins with a high energy level leaves the energy level high with probability \\(\\alpha\\) and reduces it to low with probability \\(1 - \\alpha\\). On the other hand, a period of searching undertaken when the energy level is low leaves it low with probability \\(\\beta\\) and depletes the battery with probability \\(1 - \\beta\\). In the latter case, the robot must be rescued, and the battery is then recharged back to high. Each can collected by the robot counts as a unit reward, whereas a reward of \\(-3\\) results whenever the robot has to be rescued. Let \\(r_{\\texttt{search}}\\) and \\(r_{\\texttt{wait}}\\), with \\(r_{\\texttt{search}} &gt; r_{\\texttt{wait}}\\), denote the expected numbers of cans the robot will collect (and hence the expected reward) while searching and while waiting respectively. Finally, suppose that no cans can be collected during a run home for recharging, and that no cans can be collected on a step in which the battery is depleted. This system is then a finite MDP, and we can write down the transition probabilities and the expected rewards, with dynamics as indicated in the table on the left:\n\n\n\n\n\n\nFigure 1: Recycling-robot’s graph. Taken from [@Sutton2018]\n\n\n\n\n\n\n\n\n\nFigure 2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#key-points-of-the-reward-hypothesis",
    "href": "04-finiteMDPs/mdp.html#key-points-of-the-reward-hypothesis",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "Key Points of the Reward Hypothesis:",
    "text": "Key Points of the Reward Hypothesis:\n\nReward as a Scalar Signal: At every time step, the environment provides the agent with a simple numerical signal, \\((R_t \\in \\mathbb{R}\\), which represents the immediate reward based on the agent’s actions and the current state of the environment. This reward acts as feedback for the agent, helping it learn and adjust its strategy to achieve its ultimate goal.\nMaximizing Cumulative Reward: The agent’s goal is not just to maximize the immediate reward, but to focus on the long-term sum of rewards, known as the cumulative reward. This ensures that the agent does not become short-sighted by only pursuing short-term benefits, but rather seeks strategies that maximize its total reward across time.\nExpected Value of the Reward: Since reinforcement learning involves interaction in environments that can be stochastic (involving randomness or uncertainty), the agent aims to maximize the expected value of the cumulative reward, accounting for different possible future states and outcomes. This means the agent is interested in the average cumulative reward it would obtain over many possible sequences of interactions, rather than specific individual outcomes.\nFormalizing Goals and Purposes: According to the reward hypothesis, all goals, objectives, or purposes of the agent can be quantified by maximizing this cumulative scalar signal (reward). In other words, the “purpose” of the agent is simply to optimize the feedback it receives in the form of rewards, and this concept encapsulates everything the agent is designed to achieve.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#importance-in-reinforcement-learning",
    "href": "04-finiteMDPs/mdp.html#importance-in-reinforcement-learning",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "Importance in Reinforcement Learning:",
    "text": "Importance in Reinforcement Learning:\nThis hypothesis is central to how reinforcement learning problems are structured. It reduces complex goals and objectives into a single, scalar value (the reward) that the agent can track and optimize over time. This abstraction makes it possible to design agents that can handle a wide variety of tasks, as long as those tasks can be expressed in terms of rewards provided by the environment.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#the-reward-sequence",
    "href": "04-finiteMDPs/mdp.html#the-reward-sequence",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "1. The Reward Sequence:",
    "text": "1. The Reward Sequence:\nAt each time step \\(t\\), the agent receives a reward \\(R_{t+1}, R_{t+2},\\dots,\\) as a result of interacting with the environment. This sequence of rewards represents the feedback the agent receives over time based on its actions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#defining-the-return",
    "href": "04-finiteMDPs/mdp.html#defining-the-return",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "2. Defining the Return:",
    "text": "2. Defining the Return:\nThe return at time step \\(t\\), denoted \\(G_t\\), is the total accumulated reward from time step \\(t\\) onward. In reinforcement learning, the return can be defined in different ways, but it generally involves summing the future rewards, often discounted to account for the uncertainty or diminishing value of rewards received further in the future.\nThe undiscounted return would simply be the sum of all future rewards: \\[\n   G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\cdots = \\sum_{k=0}^{\\infty} R_{t+k+1}\n\\] This sum may be infinite if the task never ends, which can be problematic. Thus, in many cases, a discount factor is applied to weight future rewards less than immediate rewards.\nLikewise if the MDP has finite horizont \\(T\\) then \\[G_t = R_{t+1} + R_{t+2} + R_{t+3} + \\cdots + R_{T}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#discounted-return",
    "href": "04-finiteMDPs/mdp.html#discounted-return",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "3. Discounted Return:",
    "text": "3. Discounted Return:\nTo address this issue, we introduce a discount factor \\(\\gamma\\), where \\(0\\leq \\gamma \\leq 1\\). The discount factor controls how much emphasis is placed on future rewards. When \\(\\gamma\\) is close to 1, future rewards are considered nearly as valuable as immediate rewards. When \\(\\gamma\\) is closer to 0, the agent focuses more on immediate rewards.\nThe discounted return is defined as: \\[\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots\n    = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\\] Here, each future reward is multiplied by ( ^k ), where ( k ) is the number of time steps into the future. This ensures that the agent values immediate rewards more highly than rewards far into the future, which is often desirable in practical applications.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#maximizing-the-expected-return",
    "href": "04-finiteMDPs/mdp.html#maximizing-the-expected-return",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "4. Maximizing the Expected Return:",
    "text": "4. Maximizing the Expected Return:\nSince the environment in reinforcement learning is often stochastic, the agent cannot guarantee a specific sequence of rewards, but it can aim to maximize the expected return. The expected return is the average return the agent would obtain by following a specific policy \\(\\pi\\), which defines the agent’s behavior.\nFormally, the agent’s objective is to maximize the expected return: \\[\n    \\mathbb{E}[G_t | \\pi]\n            = \\mathbb{E}\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\Big| \\pi\\right]\n\\] where \\(\\pi\\) is the policy being followed. This equation tells us that the agent should choose actions in a way that maximizes the long-term expected reward, considering both immediate and future rewards.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#two-types-of-tasks",
    "href": "04-finiteMDPs/mdp.html#two-types-of-tasks",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "5. Two Types of Tasks:",
    "text": "5. Two Types of Tasks:\n\nFinite-Horizon Tasks: These tasks have a fixed time limit, and the agent’s goal is to maximize the sum of rewards within that time limit. In such cases, the discount factor \\(\\gamma\\) may not be necessary, and the return is just the sum of the finite rewards received before the task ends.\nInfinite-Horizon Tasks: In tasks that continue indefinitely, the discount factor \\(\\gamma\\) ensures that the return remains finite by reducing the impact of rewards far into the future.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#conclusion",
    "href": "04-finiteMDPs/mdp.html#conclusion",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "Conclusion:",
    "text": "Conclusion:\nThus, in reinforcement learning, the agent’s formal objective is to maximize the expected return, \\(\\mathbb{E}[G_t]\\), where the return \\(G_t\\) is the discounted sum of future rewards. The use of the discount factor \\(\\gamma\\) helps the agent focus more on immediate rewards, while still considering future rewards to some degree. This formalization ensures that the agent’s behavior is guided not just by immediate rewards but by a balanced approach to long-term success.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#pick-and-drop-example",
    "href": "04-finiteMDPs/mdp.html#pick-and-drop-example",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "Pick and drop example",
    "text": "Pick and drop example\n\n\n\n\n\n\nPick and Drop game\n\n\n\n\nFigure 4.1: Pick and Drop game. See python implementation below.\n\n\n\n\n\n\n\n\n\nRewards rules for the drop game\n\n\n\n\nFigure 4.2: Rewards rules for the drop game. See the above scheme for reference.\n\n\n\n\n\n\n\n\n\nWe use the following class to simulate the pick and drop game accordingly the\n\n\n\n\n\nabove figure and rules.\n\n\npick_and_drop_game.py\n\nclass Field:\n    def __init__(self, size, item_pickup, item_dropout, start_position):\n        self.size = size\n        self.item_pickup = item_pickup\n        self.item_dropout = item_dropout\n        self.position = start_position\n        self.item_in_car = False\n    \n    def get_number_of_states(self):\n        return self.size * self.size * self.size * self.size * 2\n    \n    def get_state(self):\n        state = self.position[0] * self.size * self.size * self.size * 2\n        state = state + self.position[1] * self.size * self.size * 2\n        state = state + self.item_pickup[0] * self.size * 2\n        state = state + self.item_pickup[1] * 2\n        \n        if self.item_in_car:\n            state = state + 1\n        return state\n    \n    def make_action(self, action):\n        (x, y) = self.position\n        if action == 0:  # down\n            if y == self.size - 1:\n                return -10, False\n            else:\n                self.position = (x, y + 1)\n                return -1, False\n        \n        elif action == 1:  # up\n            if y == 0:\n                return -10, False\n            else:\n                self.position = (x, y - 1)\n                return -1, False\n        \n        elif action == 2:  # left\n            if x == 0:\n                return -10, False\n            else:\n                self.position = (x - 1, y)\n                return -1, False\n        \n        elif action == 3:  # right\n            if x == self.size - 1:\n                return -10, False\n            else:\n                self.position = (x + 1, y)\n                return -1, False\n        \n        elif action == 4:  # pickup\n            if self.item_in_car:\n                return -10, False\n            elif self.item_pickup != (x, y):\n                return -10, False\n            else:\n                self.item_in_car = True\n                return 20, False\n        \n        elif action == 5:  # dropout\n            if not self.item_in_car:\n                return -10, False\n            elif self.item_dropout != (x, y):\n                self.item_pickup = (x, y)\n                self.item_in_car = False\n                return -10, False\n            else:\n                self.item_in_car = False\n                return 20, True\n\n\n\n\n\n\n\n\n\n\nTo illustrate how works this class\n\n\n\n\n\n\n\ntest_pick_and_drop_game.py\n\nfrom pick_and_drop_game import Field\nsize = 10\nitem_pickup = (0, 0)\nitem_dropout = (9, 9)\nstart_position = (9, 0)\n\n\n\nif __name__ == '__main__':\n    field = Field(size, item_pickup, item_dropout, start_position)\n    print(field.position)\n    \n# manual solution\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\nfield.make_action(2)\n# pick\nfield.make_action(4)\n\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\nfield.make_action(0)\n\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\nfield.make_action(3)\n\nfield.make_action(5)\n\n\n\n\n\n\n\n\n\n\nNow we implement a random but naive solution\n\n\n\n\n\n\n\npick_and_drop_naive_random_solution.py\n\nfrom matplotlib import pyplot as plt\n\nfrom pick_and_drop_game import Field\nimport random\nimport numpy as np\n\n\ndef random_solution():\n    size = 10\n    item_pickup = (0, 0)\n    item_dropout = (9, 9)\n    start_position = (9, 0)\n    \n    field = Field(size, item_pickup, item_dropout, start_position)\n    \n    done = False\n    steps = 0\n    \n    while not done:\n        action = random.randint(0, 5)\n        reward, done = field.make_action(action)\n        steps = steps + 1\n    \n    return steps\n\n\nif __name__ == '__main__':\n    steps = random_solution()\n    print(steps)\n    sampling_size = 100\n    sample = [random_solution() for _ in range(sampling_size)]\n    sample = np.array(sample)\n    no_steps_mean = sample.mean()\n    print('Mean of # steps for reach goal {:n}'.format(no_steps_mean))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\nNext we apply the \\(Q-\\)learning algorithm for impove the above solution.\n\n\n\n\n\n\n\npick_and_drop_q_learning_solution.py\n\n    def q_learning_solution():\n    epsilon = 0.1\n    alpha = 0.1\n    gamma = 0.6\n    \n    field = Field(size, item_pickup, item_drop_out, start_position)\n    done = False\n    steps = 0\n    \n    while not done:\n        state = field.get_state()\n        if random.uniform(0, 1) &lt; epsilon:\n            action = random.randint(0, 5)  # Explore\n        else:\n            action = np.argmax(q_table[state])  # Exploit\n        \n        reward, done = field.make_action(action)\n        \n        new_state = field.get_state()\n        new_state_max = np.max(q_table[new_state])\n        \n        q_table[state, action] = \\\n            (1 - alpha) * q_table[state, action] \\\n            + alpha * (\n                    reward + gamma * new_state_max\n                    - q_table[state, action]\n            )\n        steps = steps + 1\n    return steps\n\n\nsize = 10\nitem_pickup = (0, 0)\nitem_drop_out = (9, 9)\nstart_position = (9, 0)\n\nfield = Field(size, item_pickup, item_drop_out, start_position)\n\nnumber_of_states = field.get_number_of_states()\nnumber_of_actions = 6\nq_table = np.zeros((number_of_states, number_of_actions))\nepsilon = 0.1\nalpha = 0.1\ngamma = 0.6\nn_training = 100000\n# Training phase\n\nfor _ in range(n_training):\n    field = Field(size, item_pickup, item_drop_out, start_position)\n    done = False\n    while not done:\n        state = field.get_state()\n        if random.uniform(0, 1) &lt; epsilon:\n            action = random.randint(0, 5)  # Explore\n        else:\n            action = np.argmax(q_table[state])  # Exploit\n        reward, done = field.make_action(action)\n        new_state = field.get_state()\n        new_state_max = np.max(q_table[new_state])\n        # q_learning iteration as ascendant grad\n        q_table[state, action] = \\\n            (1.0 - alpha) * q_table[state, action] \\\n            + alpha * (\n                reward + gamma * new_state_max - q_table[state, action]\n            )\n\nq_learning_sampling = [q_learning_solution() for _ in range(10000)]\nfig, ax = plt.subplots()\nax.hist(q_learning_sampling, bins=100)\nax.set_title('distribution of the No. of steps with the Q-Learning sol')\nax.set_xlabel('No. of steps')\nax.set_ylabel('Count')\nfig.savefig(\"histogram_q_learning_solution.png\")\nplt.show()\n\n\n\n\n\nExample 4.1 (Gridworld)  \n\nActions: north, south, east, west\nRewards:\n\nActions would take the agent off the grid leave its location unchanged, but also results in a reward of-1;\nOther actions result in a reward of 0, except those that in states A and B;\nFrom state A , all four actions yield a reward of +10 and take the agent toA’ ;\nFrom state B , all four actions yield a reward of +5 and take the agentto B’ ;\nThe learning rate (\\(\\gamma\\)) for this example is 0.9\n\n\n\n\n\n\n\n\nGrid World example. Problem scheme , possible actions and sampling reward\n\n\n\n\nFigure 4.3: Grid World example. Problem scheme , possible actions and sampling reward\n\n\n\n\nThe Bellman equation must hold for each state for the value function \\(v_{\\pi}\\) shown in Figure fig-grid-world-exm (right). Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, 0.4, and +0.7. (These numbers are accurate only to one decimal place.)\n\n\n\n\n\n\n\npython implementation for the grid-world example\n\n\n\n\n\n\n\ngridworld.py\n\n#| lst-label: lst-import\n#| lst-cap: Import pyplot\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.table import Table\nmatplotlib.use('Agg')\n\nWORLD_SIZE = 5\nA_POS = [0, 1]\nA_PRIME_POS = [4, 1]\nB_POS = [0, 3]\nB_PRIME_POS = [2, 3]\nDISCOUNT = 0.9\n\n# left, up, right, down\nACTIONS = [np.array([0, -1]),\n           np.array([-1, 0]),\n           np.array([0, 1]),\n           np.array([1, 0])]\nACTIONS_FIGS = ['←', '↑', '→', '↓']\nACTION_PROB = 0.25\n\ndef step(state, action):\n    if state == A_POS:\n        return A_PRIME_POS, 10\n    if state == B_POS:\n        return B_PRIME_POS, 5\n\n    next_state = (np.array(state) + action).tolist()\n    x, y = next_state\n    if x &lt; 0 or x &gt;= WORLD_SIZE or y &lt; 0 or y &gt;= WORLD_SIZE:\n        reward = -1.0\n        next_state = state\n    else:\n        reward = 0\n    return next_state, reward\n\n\ndef draw_image(image):\n    fig, ax = plt.subplots()\n    ax.set_axis_off()\n    tb = Table(ax, bbox=[0, 0, 1, 1])\n\n    nrows, ncols = image.shape\n    width, height = 1.0 / ncols, 1.0 / nrows\n\n    # Add cells\n    for (i, j), val in np.ndenumerate(image):\n\n        # add state labels\n        if [i, j] == A_POS:\n            val = str(val) + \" (A)\"\n        if [i, j] == A_PRIME_POS:\n            val = str(val) + \" (A')\"\n        if [i, j] == B_POS:\n            val = str(val) + \" (B)\"\n        if [i, j] == B_PRIME_POS:\n            val = str(val) + \" (B')\"\n        \n        tb.add_cell(i, j, width, height, text=val,\n                    loc='center', facecolor='white')\n        \n    # Row and column labels...\n    for i in range(len(image)):\n        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n                    edgecolor='none', facecolor='none')\n        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n                    edgecolor='none', facecolor='none')\n\n    ax.add_table(tb)\n\ndef draw_policy(optimal_values):\n    fig, ax = plt.subplots()\n    ax.set_axis_off()\n    tb = Table(ax, bbox=[0, 0, 1, 1])\n\n    nrows, ncols = optimal_values.shape\n    width, height = 1.0 / ncols, 1.0 / nrows\n\n    # Add cells\n    for (i, j), val in np.ndenumerate(optimal_values):\n        next_vals=[]\n        for action in ACTIONS:\n            next_state, _ = step([i, j], action)\n            next_vals.append(optimal_values[next_state[0],next_state[1]])\n\n        best_actions=np.where(next_vals == np.max(next_vals))[0]\n        val=''\n        for ba in best_actions:\n            val+=ACTIONS_FIGS[ba]\n        \n        # add state labels\n        if [i, j] == A_POS:\n            val = str(val) + \" (A)\"\n        if [i, j] == A_PRIME_POS:\n            val = str(val) + \" (A')\"\n        if [i, j] == B_POS:\n            val = str(val) + \" (B)\"\n        if [i, j] == B_PRIME_POS:\n            val = str(val) + \" (B')\"\n        \n        tb.add_cell(i, j, width, height, text=val,\n                loc='center', facecolor='white')\n\n    # Row and column labels...\n    for i in range(len(optimal_values)):\n        tb.add_cell(i, -1, width, height, text=i+1, loc='right',\n                    edgecolor='none', facecolor='none')\n        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center',\n                   edgecolor='none', facecolor='none')\n\n    ax.add_table(tb)\n\n\ndef figure_3_2():\n    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n    while True:\n        # keep iteration until convergence\n        new_value = np.zeros_like(value)\n        for i in range(WORLD_SIZE):\n            for j in range(WORLD_SIZE):\n                for action in ACTIONS:\n                    (next_i, next_j), reward = step([i, j], action)\n                    # bellman equation\n                    new_value[i, j] += \\\n                        ACTION_PROB * (\n                                reward + DISCOUNT * value[next_i, next_j]\n                        )\n                    \n        if np.sum(np.abs(value - new_value)) &lt; 1e-4:\n            draw_image(np.round(new_value, decimals=2))\n            plt.savefig('../images/figure_3_2.png')\n            plt.close()\n            break\n        value = new_value\n\ndef figure_3_2_linear_system():\n    '''\n    Here we solve the linear system of equations to find the exact solution.\n    We do this by filling the coefficients for each of the states with their respective right side constant.\n    '''\n    A = -1 * np.eye(WORLD_SIZE * WORLD_SIZE)\n    b = np.zeros(WORLD_SIZE * WORLD_SIZE)\n    for i in range(WORLD_SIZE):\n        for j in range(WORLD_SIZE):\n            s = [i, j]  # current state\n            index_s = np.ravel_multi_index(s, (WORLD_SIZE, WORLD_SIZE))\n            for a in ACTIONS:\n                s_, r = step(s, a)\n                index_s_ = np.ravel_multi_index(s_, (WORLD_SIZE, WORLD_SIZE))\n\n                A[index_s, index_s_] += ACTION_PROB * DISCOUNT\n                b[index_s] -= ACTION_PROB * r\n\n    x = np.linalg.solve(A, b)\n    draw_image(np.round(x.reshape(WORLD_SIZE, WORLD_SIZE), decimals=2))\n    plt.savefig('../images/figure_3_2_linear_system.png')\n    plt.close()\n\ndef figure_3_5():\n    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n    while True:\n        # keep iteration until convergence\n        new_value = np.zeros_like(value)\n        for i in range(WORLD_SIZE):\n            for j in range(WORLD_SIZE):\n                values = []\n                for action in ACTIONS:\n                    (next_i, next_j), reward = step([i, j], action)\n                    # value iteration\n                    values.append(reward + DISCOUNT * value[next_i, next_j])\n                new_value[i, j] = np.max(values)\n        if np.sum(np.abs(new_value - value)) &lt; 1e-4:\n            draw_image(np.round(new_value, decimals=2))\n            plt.savefig('../images/figure_3_5.png')\n            plt.close()\n            draw_policy(new_value)\n            plt.savefig('../images/figure_3_5_policy.png')\n            plt.close()\n            break\n        value = new_value\n\n\nif __name__ == '__main__':\n    figure_3_2_linear_system()\n    figure_3_2()\n    figure_3_5()\n\n\n\n\n\n\nExample 4.2 The state is the location of the ball. The value of a state is the negative of the number of strokes to the hole from that location. Our actions are how we aim and swing at the ball, of course, and which club we select. Let us take the former as given and consider just the choice of club, which we assume is either a putter or a driver. The upper part of Figure shows a possible state-value function, \\(v_{\\text{putt}} (s)\\), for the policy that always uses the putter.\n\n\n\n\n\n\n\nGolf example\n\n\n\n\nFigure 4.4: Gol example. At top the reward value of each state (ball curren position).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "04-finiteMDPs/mdp.html#bibliography",
    "href": "04-finiteMDPs/mdp.html#bibliography",
    "title": "Finite Markov Decision Processes (MDPs)",
    "section": "Bibliography",
    "text": "Bibliography",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finite Markov Decision Processes (MDPs)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html",
    "href": "05-dynamicProgramming/dp_rl.html",
    "title": "Dynamic Programming (DP)",
    "section": "",
    "text": "Policy Evaluation (Prediction)\nAn explanation from ChatGPT. Alright, imagine you have a big puzzle to solve, but it’s too big for you to finish in one go. So, you decide to break it into smaller puzzles, and you solve each of these small puzzles one by one. But, here’s the clever part: as you solve these small puzzles, you remember the solutions. That way, if you come across the same small puzzle again, you don’t have to solve it all over again. You already know the answer! Dynamic programming is like solving a big puzzle by breaking it into smaller ones and remembering the solutions to the smaller ones to make solving the big puzzle easier and faster.\nFor this algorithm we iterate as\n\\[\n    \\begin{aligned}\n        v_{k+1}(s) := & \\mathbb{E}_{\\pi} \\left[\n            R_{t+1} + \\gamma v_k(S_{t+1})\n            | S_t=s\n        \\right]\n        \\\\\n        = &\n            \\sum_{a\\in\\mathcal{A}(s)}\n            \\pi(a|s)\n                \\sum_{\n                    \\substack{s^{\\prime}\\in\\mathcal{S},\n                    \\\\\n                    r \\in \\mathcal{R}}\n                }\n                p(s^{\\prime}, r |s ,a)\n                \\left[\n                    r +\\gamma v(s^{\\prime})\n                \\right]\n    \\end{aligned}\n\\tag{1.1}\\]\nTo write a sequential computer program to implement iterative policy evaluation as given by Eq. eq-policy_evaluation you would have to use two arrays, one for the old values, \\(v_k(s)\\), and one for the new values, \\(v_{k+1} (s)\\). With two arrays, the new values can be computed one by one from the old values without the old values being changed. This procedure leads to the following synchronized updated algorhitm",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#synchronized-policy-evaluation",
    "href": "05-dynamicProgramming/dp_rl.html#synchronized-policy-evaluation",
    "title": "Dynamic Programming (DP)",
    "section": "Synchronized policy evaluation",
    "text": "Synchronized policy evaluation\n\n\n\\begin{algorithm} \\caption{Iterative Policy Evaluation, for estimating $V \\approx v_{\\pi}$ synchronous version} \\begin{algorithmic} \\Require $\\pi$, the policy to be evaluated, $\\theta$ tolerance precision \\Ensure $\\|v(\\cdot)- V(\\cdot)\\|&lt; \\theta$ \\Function{PolicyEvaluation}{$\\pi$, $\\theta$} \\State $cond \\leftarrow$ True \\While{cond} \\State $v \\leftarrow V$ \\For{$s \\in \\mathcal{S}$} \\State $ \\displaystyle V(s) \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{ \\substack{s^{\\prime}\\in\\mathcal{S}, \\\\ r \\in \\mathcal{R}} } p(s^{\\prime}, r |s ,a) \\left[ r +\\gamma v(s^{\\prime}) \\right] $ \\EndFor \\State $\\Delta \\leftarrow \\|v -V \\|$ \\State $cond \\leftarrow (\\theta &lt; \\Delta)$ \\EndWhile \\EndFunction \\end{algorithmic} \\end{algorithm}\n\n\nAlternatively, you could use one array and update the values in-place, that is, with each new value immediately overwriting the old one. Then, depending on the order in which the states are updated, sometimes new values are used instead of old ones on the right-hand side of Equation eq-policy_evaluation .",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#in-place-version",
    "href": "05-dynamicProgramming/dp_rl.html#in-place-version",
    "title": "Dynamic Programming (DP)",
    "section": "In-place version",
    "text": "In-place version\n\n\n\\begin{algorithm} \\caption{Iterative Policy Evaluation, for estimating $V \\approx v_{\\pi}$ in place update version } \\begin{algorithmic} \\Require $\\pi$, the policy to be evaluated, $\\theta$ tolerance precision \\Ensure $\\|v(\\cdot)- V(\\cdot)\\|&lt; \\theta$ \\Function{PolicyEvaluation}{$\\pi$, $\\theta$} \\State $cond \\leftarrow$ True \\While{cond} \\State $v \\leftarrow V$ \\For{$s \\in \\mathcal{S}$} \\State $ \\displaystyle V(s) \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{ \\substack{s^{\\prime}\\in\\mathcal{S}, \\\\ r \\in \\mathcal{R}} } p(s^{\\prime}, r |s ,a) \\left[ r +\\gamma V(s^{\\prime}) \\right] $ \\EndFor \\State $\\Delta \\leftarrow \\|v -V \\|$ \\State $cond \\leftarrow (\\theta &lt; \\Delta)$ \\EndWhile \\EndFunction \\end{algorithmic} \\end{algorithm}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#the-policy-improvement-theorem",
    "href": "05-dynamicProgramming/dp_rl.html#the-policy-improvement-theorem",
    "title": "Dynamic Programming (DP)",
    "section": "The policy improvement Theorem",
    "text": "The policy improvement Theorem\nBecause we know how good is to follow the current policy \\(\\pi\\) from \\(s\\)–that is \\(v_{\\pi}(s)\\)–we can estimate if it better or not to change to a new policy \\(\\pi^{\\prime}\\) which is equal the the original policy \\(\\pi\\) except at state \\(s\\). To this end we can sellect \\(a = \\pi^{\\prime}(s)\\) always \\(s\\) appears and there after use the original policy \\(\\pi\\). The new value of this argument results\n\\[\n\\begin{aligned}\n    q_{\\pi}(s, a)\n        :=&\n            \\mathbb{E}\n                \\left[\n                    R_{t+1} + \\gamma v_{\\pi}(S_{t+1})\n                    | S_t=s ,A_t=a\n                \\right]\n        \\\\\n        =&\n            \\sum_{s^{\\prime},r}\n                p(s^{\\prime}, r | s,a)\n                \\left[\n                    r + \\gamma v_{\\pi}(s^{\\prime})\n                \\right].\n    \\end{aligned}\n\\tag{3.1}\\]\nRegarding this line of thinking we have the following result\n\nTheorem 3.1 (policy improvement) Let \\(\\pi\\) \\(\\pi^{\\prime}\\) two deterministic given policies such that for all \\(s \\in \\mathcal{S}\\), \\[\n    q_{\\pi}(s, \\pi^{\\prime})(s) \\geq v_{\\pi}(s).\n\\tag{3.2}\\]\nThen the policy \\(\\pi^{\\prime}\\) satisfies\n\\[\n    v_{\\pi^{\\prime}} (s) \\geq  v_{\\pi} (s) \\qquad \\text{for all } s\\in\\mathcal{S}.\n\\tag{3.3}\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#policy-iteration-1",
    "href": "05-dynamicProgramming/dp_rl.html#policy-iteration-1",
    "title": "Dynamic Programming (DP)",
    "section": "Policy iteration",
    "text": "Policy iteration\n\n\n\\begin{algorithm} \\caption{ Policy Iteration, for estimating $V \\approx v_{*}$, $\\pi \\approx \\pi_{*}$ }\\begin{algorithmic} \\Require $\\pi_0$, the arbritrary initial policy , $\\theta$ precision tolerance \\State Initilize $V(s) = 0$, for all $s \\in \\mathcal S^+$ \\Function{PolicyEvaluation}{$\\pi$, $\\theta$} \\State $\\mathrm{cond} \\gets$ True \\While{cond} \\State $v \\gets V$ \\ForAll{$ s \\in \\mathcal{S}$} \\State $ \\displaystyle V(s) \\gets \\sum_{a} \\pi(a|s) \\sum_{ \\substack{s^{\\prime}\\in\\mathcal{S}, \\\\ r \\in \\mathcal{R}} } p(s^{\\prime}, r |s ,a) \\left[ r +\\gamma V(s^{\\prime}) \\right] $ \\EndFor \\State $\\Delta \\leftarrow \\|v -V \\|$ \\State $\\mathrm{cond} \\leftarrow (\\theta &lt; \\Delta)$ \\EndWhile \\Return $V \\approx v_{\\pi}$ \\EndFunction \\Function{PolicyIteration}{$\\pi_0$, $\\theta$} \\State $\\verb |policy_stable| \\gets $ False \\While{$\\neg \\verb |policy_stable|$} \\State $V \\gets $ \\Call{PolicyEvaluation}{$\\pi$, $\\theta$} \\State $\\verb |policy_stable| \\gets $ True \\ForAll{$s \\in \\mathcal{S}$} \\Comment{Loop for Policy improvement} \\State $\\verb|old_action| \\gets \\pi(s)$ \\State $ \\displaystyle \\pi(s) \\gets \\underset{a\\in \\mathcal{A}(s)}{\\operatorname{argmax}} \\sum_{ \\substack{s^{\\prime}\\in\\mathcal{S}, \\\\ r \\in \\mathcal{R}} } p(s^{\\prime}, r |s ,a) \\left[ r +\\gamma V(s^{\\prime}) \\right] $ \\If{$\\verb|old_action| \\neq \\pi_0(s)$} \\State $\\verb|policy_stable| \\gets$ False \\EndIf \\endFor \\EndWhile \\Return $V \\approx v_{*}, \\pi \\approx \\pi{*}$ \\EndFunction \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\nPython script for the example solution of the Jack’s rental cars example with policy iteration\n\n\n\n\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom scipy.stats import poisson\n\nmatplotlib.use('Agg')\n\n# maximum # of cars in each location\nMAX_CARS = 20\n\n# maximum # of cars to move during night\nMAX_MOVE_OF_CARS = 5\n\n# expectation for rental requests in first location\nRENTAL_REQUEST_FIRST_LOC = 3\n\n# expectation for rental requests in second location\nRENTAL_REQUEST_SECOND_LOC = 4\n\n# expectation for # of cars returned in first location\nRETURNS_FIRST_LOC = 3\n\n# expectation for # of cars returned in second location\nRETURNS_SECOND_LOC = 2\n\nDISCOUNT = 0.9\n\n# credit earned by a car\nRENTAL_CREDIT = 10\n\n# cost of moving a car\nMOVE_CAR_COST = 2\n\n# all possible actions\nactions = np.arange(-MAX_MOVE_OF_CARS, MAX_MOVE_OF_CARS + 1)\n\n# An up bound for poisson distribution\n# If n is greater than this value, then the probability of getting n is truncated to 0\nPOISSON_UPPER_BOUND = 11\n\n# Probability for poisson distribution\n# @lam: lambda should be less than 10 for this function\npoisson_cache = dict()\n\n\ndef poisson_probability(n, lam):\n    global poisson_cache\n    key = n * 10 + lam\n    if key not in poisson_cache:\n        poisson_cache[key] = poisson.pmf(n, lam)\n    return poisson_cache[key]\n\n\ndef expected_return(state, action, state_value, constant_returned_cars):\n    \"\"\"\n    @state: [# of cars in first location, # of cars in second location]\n    @action: positive if moving cars from first location to second location,\n            negative if moving cars from second location to first location\n    @stateValue: state value matrix\n    @constant_returned_cars:  if set True, model is simplified such that\n    the # of cars returned in daytime becomes constant\n    rather than a random value from poisson distribution, which will reduce calculation time\n    and leave the optimal policy/value state matrix almost the same\n    \"\"\"\n    # initailize total return\n    returns = 0.0\n\n    # cost for moving cars\n    returns -= MOVE_CAR_COST * abs(action)\n\n    # moving cars\n    NUM_OF_CARS_FIRST_LOC = min(state[0] - action, MAX_CARS)\n    NUM_OF_CARS_SECOND_LOC = min(state[1] + action, MAX_CARS)\n\n    # go through all possible rental requests\n    for rental_request_first_loc in range(POISSON_UPPER_BOUND):\n        for rental_request_second_loc in range(POISSON_UPPER_BOUND):\n            # probability for current combination of rental requests\n            prob = poisson_probability(rental_request_first_loc, RENTAL_REQUEST_FIRST_LOC) * \\\n                poisson_probability(rental_request_second_loc, RENTAL_REQUEST_SECOND_LOC)\n\n            num_of_cars_first_loc = NUM_OF_CARS_FIRST_LOC\n            num_of_cars_second_loc = NUM_OF_CARS_SECOND_LOC\n\n            # valid rental requests should be less than actual # of cars\n            valid_rental_first_loc = min(num_of_cars_first_loc, rental_request_first_loc)\n            valid_rental_second_loc = min(num_of_cars_second_loc, rental_request_second_loc)\n\n            # get credits for renting\n            reward = (valid_rental_first_loc + valid_rental_second_loc) * RENTAL_CREDIT\n            num_of_cars_first_loc -= valid_rental_first_loc\n            num_of_cars_second_loc -= valid_rental_second_loc\n\n            if constant_returned_cars:\n                # get returned cars, those cars can be used for renting tomorrow\n                returned_cars_first_loc = RETURNS_FIRST_LOC\n                returned_cars_second_loc = RETURNS_SECOND_LOC\n                num_of_cars_first_loc = min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS)\n                num_of_cars_second_loc = min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS)\n                returns += prob * (reward + DISCOUNT * state_value[num_of_cars_first_loc, num_of_cars_second_loc])\n            else:\n                for returned_cars_first_loc in range(POISSON_UPPER_BOUND):\n                    for returned_cars_second_loc in range(POISSON_UPPER_BOUND):\n                        prob_return = poisson_probability(\n                            returned_cars_first_loc, RETURNS_FIRST_LOC) * poisson_probability(returned_cars_second_loc, RETURNS_SECOND_LOC)\n                        num_of_cars_first_loc_ = min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS)\n                        num_of_cars_second_loc_ = min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS)\n                        prob_ = prob_return * prob\n                        returns += prob_ * (reward + DISCOUNT *\n                                            state_value[num_of_cars_first_loc_, num_of_cars_second_loc_])\n    return returns\n\n\ndef figure_4_2(constant_returned_cars=True):\n    value = np.zeros((MAX_CARS + 1, MAX_CARS + 1))\n    policy = np.zeros(value.shape, dtype=np.int64)\n\n    iterations = 0\n    _, axes = plt.subplots(2, 3, figsize=(40, 20))\n    plt.subplots_adjust(wspace=0.1, hspace=0.2)\n    axes = axes.flatten()\n    while True:\n        fig = sns.heatmap(np.flipud(policy), cmap=\"YlGnBu\", ax=axes[iterations])\n        fig.set_ylabel('# cars at first location', fontsize=30)\n        fig.set_yticks(list(reversed(range(MAX_CARS + 1))))\n        fig.set_xlabel('# cars at second location', fontsize=30)\n        fig.set_title('policy {}'.format(iterations), fontsize=30)\n\n        # policy evaluation (in-place)\n        while True:\n            old_value = value.copy()\n            for i in range(MAX_CARS + 1):\n                for j in range(MAX_CARS + 1):\n                    new_state_value = expected_return([i, j], policy[i, j], value, constant_returned_cars)\n                    value[i, j] = new_state_value\n            max_value_change = abs(old_value - value).max()\n            print('max value change {}'.format(max_value_change))\n            if max_value_change &lt; 1e-4:\n                break\n\n        # policy improvement\n        policy_stable = True\n        for i in range(MAX_CARS + 1):\n            for j in range(MAX_CARS + 1):\n                old_action = policy[i, j]\n                action_returns = []\n                for action in actions:\n                    if (0 &lt;= action &lt;= i) or (-j &lt;= action &lt;= 0):\n                        action_returns.append(expected_return([i, j], action, value, constant_returned_cars))\n                    else:\n                        action_returns.append(-np.inf)\n                new_action = actions[np.argmax(action_returns)]\n                policy[i, j] = new_action\n                if policy_stable and old_action != new_action:\n                    policy_stable = False\n        print('policy stable {}'.format(policy_stable))\n\n        if policy_stable:\n            fig = sns.heatmap(np.flipud(value), cmap=\"YlGnBu\", ax=axes[-1])\n            fig.set_ylabel('# cars at first location', fontsize=30)\n            fig.set_yticks(list(reversed(range(MAX_CARS + 1))))\n            fig.set_xlabel('# cars at second location', fontsize=30)\n            fig.set_title('optimal value', fontsize=30)\n            break\n\n        iterations += 1\n    plt.savefig('../images/figure_4_2.png')\n    plt.close()\n\n\nif __name__ == '__main__':\n    figure_4_2()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#bibliography",
    "href": "05-dynamicProgramming/dp_rl.html#bibliography",
    "title": "Dynamic Programming (DP)",
    "section": "Bibliography",
    "text": "Bibliography\n\n\n\nTo illustrate how works the policy iteration algorithm, we will develope a python implementation for the so called Jack’s car rentals. We also discusse about the OpenAI’s library gym and how applied to compare our results.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#states",
    "href": "05-dynamicProgramming/dp_rl.html#states",
    "title": "Dynamic Programming (DP)",
    "section": "States",
    "text": "States\nQuestion:\n\nWhat are the valid states of the Jacks car rental enviroment?\n\nAnswer:\n\nNumber of cars in each location ready for rent (e.g. the tuple (3,5) means 3 cars in location \\(A\\) and 5 cars in location \\(B\\)).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#probability-to-be-in-a-state",
    "href": "05-dynamicProgramming/dp_rl.html#probability-to-be-in-a-state",
    "title": "Dynamic Programming (DP)",
    "section": "Probability to be in a state",
    "text": "Probability to be in a state\nThe probability for the states can be represented as a vector. For Jacks Car Rental:\n\\[\n    p(\\vec s) =\n        \\begin{pmatrix}\n            p(S=s_0)\n            \\\\\n            p(S=s_1)\n            \\\\ p(S=s_2)\n            \\\\ \\dots\n            \\\\ p(S=s_{21})\n            \\\\ p(S=s_{22})\n            \\\\ \\dots\n            \\\\ p(S=s_{440})\n        \\end{pmatrix}\n        \\begin{matrix}(\n            0,0)\n            \\\\\n            (0,1)\n            \\\\\n            (0,2)\n            \\\\\n            \\dots\n            \\\\\n            (1,0)\n            \\\\\n            (1,1)\n            \\\\\n            \\dots\n            \\\\\n            (20,20)\n        \\end{matrix}\n\\tag{5.2}\\]\nFor the Jacks Car Rental problem it’s sometimes easier to work with a state probability matrix:\n\\[\n    P(\\hat S) =\n        \\begin{pmatrix}\n                P(S=s_{0})\n                & P(S=s_{1})\n                & \\dots\n                & P(S=s_{20})\n            \\\\\n                P(S=s_{21})\n                & P(S=s_{22})\n                & \\dots\n                & P(S=s_{41})\n            \\\\\n                \\vdots\n                & \\vdots\n                & \\ddots\n                & \\vdots\n            \\\\\n                \\cdot\n                & \\cdot\n                & \\dots\n                & p(S=s_{440})\n        \\end{pmatrix}\n\\tag{5.3}\\]\nThe indices of \\(P(\\hat S)\\) correspond directly to the probability of the number of cars at each location.\nNote that we can use np.reshape to switch between the different representations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#python-modules",
    "href": "05-dynamicProgramming/dp_rl.html#python-modules",
    "title": "Dynamic Programming (DP)",
    "section": "Python Modules",
    "text": "Python Modules\nAll python modules needed to run would staty at the begining of the script.\n\n\n\n\n\n\nPython modules and libraries\n\n\n\n\n\n# Python Standard Library\nimport os\n# External Modules\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport mpl_toolkits.mplot3d\nfrom matplotlib import cm\nfrom scipy.stats import poisson\n\n\n\n\n\n\n\n\n\nNow we make the setuo of the problem with the pareameters for\n\n\n\n\n\nthe sampling like the intensity \\(\\lambda_i\\) for the regarding Poisson r.v. discount factor \\(\\gamma\\) and others.\nREQUEST_RATE = (3., 4.)      \nRETURN_RATE  = (3., 2.)\n\nGAMMA = 0.9\nRENTAL_INCOME = 10\nTRANSFER_COST = 2\nTRANSFER_MAX  = 5\nMAX_CAPACITY  = 20\n\n# location indicies\nA = 0\nB = 1\n\nstate_ = np.zeros([MAX_CAPACITY+1, MAX_CAPACITY+1])\nstate_[11, 15] = 1. # 11 cars at A and 15 cars at B\nstate_.shape\n\n# Print all content of state_\nstate_\n\n# TODO\nstate = state_.reshape(-1)\nprint(state.shape)\nstate_11_15 = state\n\n\n\n\n\n\n\n\n\nNext, we define a function to get the most probable location of a given state:\n\n\n\n\n\ndef get_most_probable_location(state):\n    i = state.argmax()\n    return (i//(MAX_CAPACITY+1), i%(MAX_CAPACITY+1))\nand tested in console by\nget_most_probable_location(state)\n\n\n\n\n\n\n\n\n\nNow, we define a function to get the state vector\n\n\n\n\n\nfor given \\(a\\) and \\(b\\), where \\(a\\) is the amount of cars at \\(A\\) and \\(b\\) is the amount of cars at \\(B\\).\ndef get_state_vector(a, b):\n    s = np.zeros((MAX_CAPACITY+1)**2)\n    s[a*(MAX_CAPACITY+1)+b] = 1\n    return s\nTo test it we put in console:\nassert get_most_probable_location(get_state_vector(11,12)) == (11,12)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#actions",
    "href": "05-dynamicProgramming/dp_rl.html#actions",
    "title": "Dynamic Programming (DP)",
    "section": "Actions",
    "text": "Actions\nActions are the nightly movements of the cars. We encode them as a number between -5 to 5, e.g.\n\nAction +3: move three cars from A to B.\nAction -1: move one car from B to A.\n\n\n\n\n\n\n\nThe python code reads as\n\n\n\n\n\naction_space = np.arange(-TRANSFER_MAX, TRANSFER_MAX+1)\nprint(action_space)  # cars moved  \nprint(np.arange(11)) # action indices",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#state-transition-probability-kernel-for-jacks-car-rental",
    "href": "05-dynamicProgramming/dp_rl.html#state-transition-probability-kernel-for-jacks-car-rental",
    "title": "Dynamic Programming (DP)",
    "section": "State transition probability kernel for Jacks Car Rental",
    "text": "State transition probability kernel for Jacks Car Rental\nDefinition . Let \\(P\\) a stochastic matrix such that \\[\n    \\begin{equation*}\n        \\vec s' = P \\vec s\n    \\end{equation*}\n\\]\nwith\n\n\\(\\vec s\\): probabilities for the states \\(s\\) at time \\(t\\)\n\\(\\vec s'\\): probabilities for the states \\(s^{\\prime}\\) at time \\(t+1\\)\n$P = $: State Transition Probability Kernel\n\nIn the following we construct the (transpose of the) state transition probability kernel. First note that the kernel decomposes as\n\\[ P = P_{ret} P_{req} P_{move} \\]\nwith\n\n\\(P_{move}\\): The kernel for the nightly moves according to the actions.\n\\(P_{req}\\): The kernel for the requests of cars.\n\\(P_{ret}\\): The kernel for the returns of cars.\n\nNote: for \\(P_{req}\\) and \\(P_{ret}\\) we don’t have actions.\nForm of the Transition Matrix: - for earch old state we have a column - for each new states we have a row\nNote:\nHere we have 441 possible states (for the old and new ones). So the transition matrix has dimension (441, 441). Each column of the transition matrix describes how a states is transformed.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#request-state-transition-probability-kernel",
    "href": "05-dynamicProgramming/dp_rl.html#request-state-transition-probability-kernel",
    "title": "Dynamic Programming (DP)",
    "section": "Request State Transition Probability Kernel",
    "text": "Request State Transition Probability Kernel\nAccording wiht the problem setup, we define a function that calculates the transitions for one given location. So, we use a Poisson probability mass function (PMF) for the sampling. To this end we define the function |get_request_transitions_for_one_location()|\n\n\n\n\n\n\nConstruct request transitions for one location:\n\n\n\n\n\ne.g. \\(\\verb|state_1| = ((0,*), (1,*), ... (20,*))\\) are only the probabilities for location 1\nMAX_PMF = 30\n\ndef get_request_transitions_for_one_location(loc):\n    \"\"\"\n    Construct transition matrix P_{to, from} for one location only for requests.\n    The matrix has form (21, 21). \n    \n    Parameters\n    ----------\n    loc : int\n        Location: 0 or 1\n    \n    Returns\n    -------\n    numpy.ndarray\n        request transition matrix\n\n    \"\"\"\n    assert(loc==A or loc==B)\n    # transition matrix P_{to, from} for one location only requests\n    transition_matrix = np.zeros([MAX_CAPACITY + 1, MAX_CAPACITY + 1])\n    \n    request_pmf = poisson.pmf(np.arange(MAX_PMF), REQUEST_RATE[loc])\n    np.testing.assert_almost_equal(request_pmf[-1], 0., decimal=12)\n    for i in range(MAX_CAPACITY+1):  \n        for j in range(MAX_CAPACITY+1):  \n            if j==0:\n                transition_matrix[i,j] = request_pmf[i:].sum()\n            elif j&lt;=i:    \n                transition_matrix[i,j] = request_pmf[i-j]             \n    return transition_matrix.T\n\nP_request_A_one_loc = get_request_transitions_for_one_location(A)\n# all colums should sum to one\nnp.testing.assert_allclose(P_request_A_one_loc.sum(axis=0), 1.)\nP_request_A_one_loc.shape\n\n\n\nIf we have e.g. eight cars at location \\(A\\), after the renting the requested cars\n\n\n\n\n\n\nwe get the following distribution:\n\n\n\n\n\nplt.plot(np.arange(MAX_CAPACITY + 1), P_request_A_one_loc[:,8], '*b')\nplt.xticks(np.arange(0, 21, step=1))\nplt.xlabel(\"number of cars\")\nplt.ylabel(\"probability\");\n\nP_request_A_one_loc[:,0]\n\nP_request_A_one_loc[:,1]\n\n# for testing the one location transition matricies\ns = np.zeros(MAX_CAPACITY+1)\ns[3] = 1\na = np.dot(P_request_A_one_loc, s)\nnp.testing.assert_almost_equal(a.sum(), 1.)\na\n\n\n\nNote that the defined matrix has shape \\(21 \\times 21\\). So the index considers only the location \\(A\\). A full state is given by the pair (location A, location B). So we must extend the matrix to a “full” transition matrix \\(441\\times 441\\).\nWe define a function to retrieve the “full” transition matrix \\(A\\) for a given location.\n\n\n\n\n\n\nfull” transition matrix \\(A\\)\n\n\n\n\n\ndef full_transition_matrix_A(transition_one_loc):\n    block_size = MAX_CAPACITY + 1 # for convenience\n    transition_matrix = np.zeros([block_size**2, block_size**2])\n    for i in range(block_size):\n        transition_matrix[\n            i:block_size ** 2: block_size,\n          i:block_size ** 2: block_size\n        ] = transition_one_loc\n    return transition_matrix\n\nP_request_A = full_transition_matrix_A(P_request_A_one_loc)\nP_request_A#.shape\n\n# should mix only states of A: \nnp.testing.assert_almost_equal(\n    np.dot(\n        P_request_A,\n        state_11_15\n    ).reshape(MAX_CAPACITY+1,-1).sum(),1.\n)\nnp.testing.assert_almost_equal(\n    np.dot(\n        P_request_A,\n        state_11_15\n    ).reshape(MAX_CAPACITY + 1 ,-1)[:,15].sum(),\n    1.\n)\nnp.testing.assert_almost_equal(\n    np.dot(\n        P_request_A,\n        state_11_15\n    ).reshape(MAX_CAPACITY + 1, -1)[:12,15].sum(),\n    1.\n)\n\n\n\nNext, we define a function to retrieve the full transition matrix \\(B\\) for a given location. Note, that the positions of equal \\(B\\) locations in the full state vector a different a for location \\(A\\).\nFull state vector: \\[\n    \\begin{bmatrix}\n            (0,0)\n        \\\\\n            (0,1)\n        \\\\\n            (0,2)\n        \\\\\n            \\dots\n        \\\\\n            (1,0)\n        \\\\\n            (1,1)\n        \\\\\n        \\dots\n        \\\\\n            (20,20)\n    \\end{bmatrix}\n\\tag{5.4}\\]\ne.g. \n\nindices for location \\(A: 0, 1, \\dots, 20\\)\nindices for location \\(B: 0, 21, 42, \\dots, 420\\)\n\n\n\n\n\n\n\nfull matrix \\(B\\)\n\n\n\n\n\ndef full_transition_matrix_B(transition_one_loc):\n    block_size = MAX_CAPACITY+1 # for convenience\n    transition_matrix = np.zeros([block_size**2, block_size**2])\n    for i in range(block_size):\n        transition_matrix[\n          i*block_size:(i*block_size)+block_size,\n          i*block_size:(i*block_size)+block_size\n        ] = transition_one_loc\n    return transition_matrix\n\nP_request_B_one_loc = get_request_transitions_for_one_location(1)\nP_request_B = full_transition_matrix_B(P_request_B_one_loc)\n\n# should mix only states of B: \nnp.testing.assert_almost_equal(\n    np.dot(P_request_B, state_11_15).reshape(MAX_CAPACITY+1,-1).sum(),\n    1.\n)\nnp.testing.assert_almost_equal(\n    np.dot(\n        P_request_B,\n        state_11_15\n    ).reshape(MAX_CAPACITY + 1, -1)[11].sum(), 1.\n)\nnp.testing.assert_almost_equal(\n    np.dot(\n        P_request_B,\n        state_11_15\n    ).reshape(MAX_CAPACITY + 1,-1)[11,:16].sum(),\n1.)\n\n# with the two request matricies for each location we \n# can construct the full request matrix\n\nP_request = np.dot(P_request_A, P_request_B)\n\n# The order of application shouldn't matter, so the commutator should be zero:\nnp.testing.assert_allclose(\n    np.dot(\n        P_request_A,\n        P_request_B\n    ),\n    np.dot(\n        P_request_B,\n        P_request_A\n    )\n)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#return-state-transition-probability-kernel",
    "href": "05-dynamicProgramming/dp_rl.html#return-state-transition-probability-kernel",
    "title": "Dynamic Programming (DP)",
    "section": "Return State Transition Probability Kernel",
    "text": "Return State Transition Probability Kernel\nSimilar to the requests, we define the return state transition probability kernel.\n\n\n\n\n\n\nPutting all together\n\n\n\n\n\nMAX_PMF = 30\n\ndef get_return_transition_matrix_one_location(loc):\n    \"\"\"\n    Construct transition matrix P_{to, from} for one location only for returns\n    \n    Parameters\n    ----------\n    loc : int\n        Location: 0 or 1\n    \n    Returns\n    -------\n    numpy.ndarray\n        transition matrix\n\n    \"\"\"\n    assert(loc==0 or loc==1)\n    transition_matrix = np.zeros([MAX_CAPACITY+1, MAX_CAPACITY+1])\n\n    return_pmf = poisson.pmf(np.arange(MAX_PMF), RETURN_RATE[loc])\n    np.testing.assert_almost_equal(return_pmf[-1], 0., decimal=12)\n    for i in range(MAX_CAPACITY+1):  \n        for j in range(MAX_CAPACITY+1):  \n            if j==MAX_CAPACITY:\n                transition_matrix[i,j] = return_pmf[j-i:].sum()\n            elif j&gt;=i and j&lt;MAX_CAPACITY:    \n                transition_matrix[i,j] = return_pmf[j-i]     \n    return transition_matrix.T\nTo test this function we put the following in the console.\n\nP_return_A_one_loc = get_return_transition_matrix_one_location(0)\nnp.testing.assert_almost_equal(P_return_A_one_loc.sum(axis=0), 1.)\n\nP_return_A = full_transition_matrix_A(P_return_A_one_loc)\n\n# should mix only states of A: \nnp.testing.assert_almost_equal(\n    np.dot(\n        P_return_A,\n        state_11_15\n    ).reshape(MAX_CAPACITY + 1, -1).sum(), 1.)\nnp.testing.assert_almost_equal(\n    np.dot(\n        P_return_A,\n        state_11_15\n).reshape(MAX_CAPACITY+1,-1)[:,15].sum(), 1.)\nnp.testing.assert_almost_equal(\n    np.dot(\n        P_return_A,\n            state_11_15\n    ).reshape(MAX_CAPACITY + 1, -1)[11:,15].sum(),\n    1.\n)\ns = np.zeros(MAX_CAPACITY + 1)\ns[17] = 1\n\na = np.dot(P_return_A_one_loc, s)\nst = np.dot(P_return_A , get_state_vector(17, 5))\nnp.testing.assert_almost_equal(a, st.reshape(MAX_CAPACITY + 1, -1)[:,5])\nP_return_B_one_loc = get_return_transition_matrix_one_location(B)\nP_return_B = full_transition_matrix_B(P_return_B_one_loc)\n\n# should mix only states of B: \nnp.testing.assert_almost_equal(\n    np.dot(\n        P_return_B,\n        state_11_15\n    ).reshape(MAX_CAPACITY + 1, -1).sum(),\n    1.\n)\nnp.testing.assert_almost_equal(\n    np.dot(\n        P_return_B,\n        state_11_15\n    ).reshape(MAX_CAPACITY + 1, -1)[11].sum(),\n    1.\n)\nnp.testing.assert_almost_equal(\n    np.dot(\n        P_return_B,\n        state_11_15\n    ).reshape(MAX_CAPACITY + 1, -1)[11,15:].sum(),\n    1.\n)\nP_return = np.dot(P_return_B, P_return_A)\n\nnp.testing.assert_allclose(\n    np.dot(\n        P_return_B,\n        P_return_A\n    ),\n    np.dot(\n        P_return_A,\n        P_return_B\n    )\n)\n\n\n\nThe combinations of requests and returns is given by the dot product:\n\\[ P_{ret/req} = P_{ret} P_{req} \\]\nP_return_request = np.dot(P_return, P_request)\n\nNightly Moves State Transition Probability Kernel\nWe implement \\(P_{move}\\) as a 3d numpy array with indices [to_state_index, from_state_index, action_index].\n\n\n\n\n\n\nMoves\n\n\n\n\n\ndef get_moves(a, b, action):\n    if action &gt; 0: # from A to B\n        return min(a, action)\n    else:\n        return max(-b, action)\n\ndef get_nightly_moves():\n    transition_matrix = np.zeros([(MAX_CAPACITY+1)**2, (MAX_CAPACITY+1)**2, action_space.shape[0]])\n    for a in range(MAX_CAPACITY+1):\n        for b in range(MAX_CAPACITY+1):\n            for i, action in enumerate(action_space):\n                old_state_index = a*(MAX_CAPACITY+1)+b\n                moves = get_moves(a, b, action)\n                new_a = min(a - moves, MAX_CAPACITY)\n                new_b = min(b + moves, MAX_CAPACITY)\n                new_state_index = new_a *(MAX_CAPACITY+1) + new_b\n                transition_matrix[new_state_index, old_state_index, i] = 1.\n    return transition_matrix\n\n\n\n\n\n\n\n\n\nIn this implementation:\n\n\n\nactions and indices for actions differ, e.g. action 2 has index 2 + 5.\n\n\n\n\n\n\n\n\nRun the following to verify this fact.\n\n\n\n\n\nP_move = get_nightly_moves()\n\nnp.testing.assert_allclose(P_move.sum(axis=0), 1.)\n\n# check some moves\n# assert P_move[:,21*car_at_A+cars_at_B,action+TRANSFER_MAX].reshape(MAX_CAPACITY+1, -1)[new_cars_at_A, new_cars_at_B] == 1.\nassert P_move[:,0,0].reshape(MAX_CAPACITY+1, -1)[0,0] == 1.\n\n# e.g. from state [1,0] and action 1 =&gt; new state should be  [0,1]\nassert P_move[:,21*1+0,1+TRANSFER_MAX].reshape(MAX_CAPACITY+1, -1)[0,1] == 1. \n\nassert P_move[:,21*1+1,-2+TRANSFER_MAX].reshape(MAX_CAPACITY+1, -1)[2,0] == 1. \nassert P_move[:,21*9+5,0+TRANSFER_MAX].reshape(MAX_CAPACITY+1, -1)[9,5] == 1. \nassert P_move[:,21*9+5,3+TRANSFER_MAX].reshape(MAX_CAPACITY+1, -1)[6,8] == 1. \nassert P_move[:,21*9+5,-3+TRANSFER_MAX].reshape(MAX_CAPACITY+1, -1)[12,2] == 1.\nassert P_move[:,21*20+20,5+TRANSFER_MAX].reshape(MAX_CAPACITY+1, -1)[15,20] == 1. \nassert P_move[:,21*20+20,-4+TRANSFER_MAX].reshape(MAX_CAPACITY+1, -1)[20,16] == 1. \nP_move.shape\n\n\n\nNext, we construct the full transition probability kernel (request, returns and nightly shifts):\n\n\n\n\n\n\nA tensor implementation of the transition probability kernel\n\n\n\n\n\n# this is the transpose of the state transition probability kernel\nP = np.ndarray(\n    (\n        (MAX_CAPACITY + 1) ** 2,\n        (MAX_CAPACITY + 1) ** 2,\n        action_space.shape[0]\n    )\n)\nfor i in range(action_space.shape[0]): # TODO: without a loop?\n    P[:,:,i] = np.dot(P_return_request, P_move[:,:,i]) \nP.shape\n\n\n\n\\(P\\) is the full state transition probability kernel for the Jack’s car rental problem.\n\\(P\\) has the following indices: \\((s', s, a)\\)\n\n\n\n\n\n\nTo test that our kernel is a stochastic Tensor\n\n\n\n\n\n    np.testing.assert_almost_equal(P.sum(axis=0), 1.)\nfor s_a in range(21):\n    for s_b in range(21):\n        for a in range(11):\n            s_ = np.dot(P[:,:,a], get_state_vector(s_a, s_b))\n            np.testing.assert_almost_equal(s_.sum(), 1.0)\nnp.testing.assert_almost_equal(P.sum(axis=0), 1.)\n\n\n\nNow we visualize the underlying distribution\n\n\n\n\n\n\nPlot over states\n\n\n\n\n\ndef plot3d_over_states(f, zlabel=\"\", ):\n    A = np.arange(0, MAX_CAPACITY + 1)\n    B = np.arange(0, MAX_CAPACITY + 1)\n    # B, A !!!\n    B, A = np.meshgrid(B, A)\n    V = f.reshape(MAX_CAPACITY+1,-1)\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111, projection='3d')\n    #ax = fig.gca(projection='3d')\n    surf = ax.plot_surface(\n      A,\n      B,\n      V,\n      rstride=1,\n      cstride=1,\n      cmap=cm.coolwarm,\n      linewidth=0,\n      antialiased=False\n    )\n    fig.colorbar(surf, shrink=0.5, aspect=5)\n    # ax.scatter(A, B, V, c='b', marker='.')\n    ax.set_xlabel(\"cars at A\")\n    ax.set_ylabel(\"cars at B\")\n    ax.set_zlabel(zlabel)\n    \n    ax.set_xticks(np.arange(0,21,1))\n    ax.set_yticks(np.arange(0,21,1))\n    #plt.xticks(np.arange(0,1,21))\n    #ax.view_init(elev=10., azim=10)\n    plt.show()\n# do action 3 in cars_at_loc (10,5), i.e move three car from A to B:\ns_ = np.dot(P[:,:,3+TRANSFER_MAX], get_state_vector(10, 5))\nplot3d_over_states(s_, 'probability')\n\n\n\nTo approximate the stationary distribution we make a product of 500 transtions\n\n\n\n\n\n\nStationary distribution\n\n\n\n\n\ns_ = state_11_15\nfor i in range (500):\n    s_ = np.dot(P[:,:, 1 + TRANSFER_MAX], s_)\n    \nprint (get_most_probable_location(s_))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#reward-function",
    "href": "05-dynamicProgramming/dp_rl.html#reward-function",
    "title": "Dynamic Programming (DP)",
    "section": "Reward Function",
    "text": "Reward Function\nLet’s construct the expected reward aka reward function \\[ \\mathcal R(s,a) = \\mathcal R_s^a = \\mathbb E[R \\mid s, a] \\]\n\n\n\n\n\n\nget_reward()\n\n\n\n\n\ndef get_reward():\n    \n    poisson_mask = np.zeros((2, MAX_CAPACITY+1, MAX_CAPACITY+1))\n    po = (poisson.pmf(np.arange(MAX_CAPACITY+1), REQUEST_RATE[A]),\n          poisson.pmf(np.arange(MAX_CAPACITY+1), REQUEST_RATE[B]))\n    for loc in (A,B):\n        for i in range(MAX_CAPACITY+1):\n            poisson_mask[loc, i, :i] = po[loc][:i]\n            poisson_mask[loc, i, i] = po[loc][i:].sum()\n    # the poisson mask contains the probability distribution for renting x cars (x column) \n    # in each row j, with j the number of cars available at the location\n\n    reward = np.zeros([MAX_CAPACITY+1, MAX_CAPACITY+1, 2*TRANSFER_MAX+1])\n    for a in range(MAX_CAPACITY+1):\n        for b in range(MAX_CAPACITY+1):\n            for action in range(-TRANSFER_MAX, TRANSFER_MAX+1):\n                moved_cars = min(action, a) if action&gt;=0 else max(action, -b)\n                a_ = a - moved_cars\n                a_ = min(MAX_CAPACITY, max(0, a_))\n                b_ = b + moved_cars\n                b_ = min(MAX_CAPACITY, max(0, b_))\n                reward_a = np.dot(poisson_mask[A, a_], np.arange(MAX_CAPACITY+1)) \n                reward_b = np.dot(poisson_mask[B, b_], np.arange(MAX_CAPACITY+1))     \n                reward[a, b, action+TRANSFER_MAX] = ( \n                            (reward_a + reward_b) * RENTAL_INCOME -\n                            np.abs(action) * TRANSFER_COST )\n                #if a==20 and b==20 and action==0:\n                #    print (a_,b_, action)\n                #    print (reward_a, reward_b)\n                #    print (reward[a, b, action+TRANSFER_MAX])\n        \n    return reward\nReward = get_reward()\nReward = Reward.reshape(441,11)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#policy",
    "href": "05-dynamicProgramming/dp_rl.html#policy",
    "title": "Dynamic Programming (DP)",
    "section": "Policy",
    "text": "Policy\nA policy \\(\\pi\\) is the behaviour of an agent. It maps states \\(s\\) to actions \\(a\\):\n\nDeterministic Policies: \\(a = \\pi(s)\\)\nStochastic Policies: \\(\\pi(a\\mid s) = P(a\\mid s)\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#markov-reward-process",
    "href": "05-dynamicProgramming/dp_rl.html#markov-reward-process",
    "title": "Dynamic Programming (DP)",
    "section": "Markov Reward Process",
    "text": "Markov Reward Process\nA stationary policy \\(\\pi\\) and a MDP induce a MRP. Choose always the action \\(a\\) according to the policy \\(\\pi\\).\nA Markov Reward Process (MRP) is a tuple \\(\\langle \\mathcal{S}, \\mathcal P_0'\\rangle\\) with\n\n\\(\\mathcal S\\): finite set of states \\(s\\)\n\\(\\mathcal P'_0\\): state transition Kernel matrix: \\(\\mathcal P'_0\\) assigns to each state \\((S=s) \\in \\mathcal S\\) a probability measure over \\(\\mathcal S \\times \\mathbb R\\): \\(P'_0(\\cdot \\mid s)\\) with the following semantics: For \\(U \\in \\mathcal S \\times \\mathbb R\\) is \\(P'_0(U \\mid s)\\) the probability that the next states and the reward \\(R\\) belongs to the set \\(U\\) given that the current state is \\(s\\).\n\nThis implies again a reward function \\(\\mathcal R^\\pi(s)\\) resp. \\(\\mathcal R^\\pi_s\\) as for the MDP.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#state-transition-probability-matrix-for-the-mrp",
    "href": "05-dynamicProgramming/dp_rl.html#state-transition-probability-matrix-for-the-mrp",
    "title": "Dynamic Programming (DP)",
    "section": "State Transition Probability Matrix for the MRP",
    "text": "State Transition Probability Matrix for the MRP\n\\[\n    p(s^{\\prime} | s, \\pi(s))\n        = \\mathbb P [S_{t+1} = s' \\mid S_t = s , a = \\pi(s)]\n\\]\n\\[\n    \\mathcal P^\\pi =\n    \\begin{pmatrix}\n            \\mathcal P_{11}\n            & \\dots\n            & \\mathcal P_{1n}\n        \\\\\n            \\vdots\n            &\n            &\n        \\\\\n            \\mathcal P_{n1}\n            & \\dots\n            & \\mathcal P_{nn}\n    \\end{pmatrix}\\]\nNote that the indices of the matrix are \\(ss'\\) in contrast to the above constructed state transition kernel which has the indices \\(s'sa\\).\nGiven a MDP \\(\\langle \\mathcal{S, A, \\mathcal P_o} \\rangle\\) and a policy \\(\\pi\\):\n\nThe state sequence \\(s_1, s_2, \\dots\\) is a Markov process\nThe state and reward sequence \\(s_1, r_2, s_2, \\dots\\) is a Markov Reward Process where\n\n\\(\\mathcal P^\\pi_{s,s'} = \\sum_{a\\in \\mathcal A} \\pi(a \\mid s) \\mathcal P^a_{s,s'}\\)\n\\(\\mathcal R^\\pi_{s} = \\sum_{a\\in \\mathcal A} \\pi(a \\mid s) \\mathcal R^a_{s}\\)\n\n\n\n\n\n\n\n\nChecking the Transition matrix for our MRP\n\n\n\n\n\n# choose in each state action 2 (move two cars from A to B)\npolicy = np.ones((MAX_CAPACITY+1)**2, dtype=int) * 2\npolicy.shape\ndef get_transition_kernel_for_policy(policy):\n    # use advanced indexing to get the entiers \n    return P[:, range((MAX_CAPACITY+1)**2), policy+TRANSFER_MAX]\nP_pi = get_transition_kernel_for_policy(policy)\nnp.testing.assert_allclose(P_pi.sum(axis=0), 1.) \nReward.shape\ndef get_P_reward_for_policy(policy):\n    P_pi = get_transition_kernel_for_policy(policy)\n    return P_pi, Reward[range((MAX_CAPACITY+1)**2), policy+TRANSFER_MAX]\nget_P_reward_for_policy(policy)[1].shape\npolicy = np.ones((MAX_CAPACITY+1)**2, dtype=int) * 5\nP_pi, reward = get_P_reward_for_policy(policy)\nplot3d_over_states(reward, 'avg reward')",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#total-discounted-reward",
    "href": "05-dynamicProgramming/dp_rl.html#total-discounted-reward",
    "title": "Dynamic Programming (DP)",
    "section": "Total Discounted Reward",
    "text": "Total Discounted Reward\nThe return \\(G_t\\) is defined as the total discounted reward:\n\\[\n    G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots\n        = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#sum-of-discounted-rewards-is-finite",
    "href": "05-dynamicProgramming/dp_rl.html#sum-of-discounted-rewards-is-finite",
    "title": "Dynamic Programming (DP)",
    "section": "Sum of discounted rewards is finite",
    "text": "Sum of discounted rewards is finite\nNote that with all rewards \\(R_t&gt;0\\)\n\\[ \\sum_{t=0}^\\infty R_t = \\infty \\]\nIndependent of the actual values of \\(R_t\\). So we can’t compare the (expected) reward of different states respectivly policies (which cases the state sequence).\nBut \\[ \\sum_{t=0}^\\infty \\gamma^t R_t \\]\nwith the discount \\(\\gamma\\)\n\\(0&lt;\\gamma&lt;1\\)\nhas as result a finite number, because with the maximum reward \\(R_{max} = \\max_t (R_t)\\) in the serie it is bounded by\n\\[\n  \\sum_{t=0}^\\infty\n    \\gamma^t R_t\n    \\leq\n    \\sum_{t=0}^\\infty\n        \\gamma^t R_{max}\n        = \\frac{R_{max}}{1-\\gamma}\n\\]\n\nProof of \\(\\sum_i \\gamma^i = 1/(1-\\gamma)\\)\n\\[\n  x = \\gamma^0 +\\gamma^1 + \\gamma^2 + \\dots\n\\]\n\\[\n  \\gamma^0 +\\gamma^1\n  + \\gamma^2 + \\dots\n    = \\gamma^0\n      + \\gamma (\\gamma^0 +\\gamma^1 + \\gamma^2 + \\dots)\n\\]\n\\[\n  x = \\gamma^0 + \\gamma x = 1 + \\gamma x\n\\]\n\\[\n  x(1-\\gamma) = 1\n\\]\n\\[ x = \\frac{1}{1-\\gamma} \\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#value-functions",
    "href": "05-dynamicProgramming/dp_rl.html#value-functions",
    "title": "Dynamic Programming (DP)",
    "section": "Value functions",
    "text": "Value functions\nThe state-value function \\(v_\\pi (s)\\) of an MDP is the expected return starting from state \\(s\\), and then follow policy \\(\\pi\\)\n\\[\n  \\begin{aligned}\n    v_\\pi (s) = &\n      \\mathbb E_\\pi[G_t \\mid S_t=s]\n    \\\\\n      = &\n      \\mathbb E \\left[\n        \\sum_{t'=t}^\\infty\n          \\gamma^{t'-t} R_{t'+1} \\mid S_t=s\n      \\right], s \\in\\mathcal S\n  \\end{aligned}\n\\tag{5.5}\\]\nThe value function can be decomposed into an immediate part and the discounted value function of the successor states:\n\\[\n  \\begin{aligned}\n    v_\\pi(s)\n    & = \\mathbb E[G_t \\mid S_t =s]\n  \\\\\n    & = \\mathbb E [R_{t+1}+γR_{t+2}+\\gamma^2 R_{t+3} + \\dots \\mid S_t =s]\n  \\\\\n    & = \\mathbb E [\n      R_{t+1}\n      + \\gamma (\n        R_{t+2}\n       + \\gamma R_{t+3}\n       + \\dots\n      )\n      \\mid S_t = s\n  ]\n  \\\\\n    &=\n    \\mathbb E [\n      R_{t+1}\n      + \\gamma G_{t+1} \\mid S_t = s\n    ]\n  \\\\\n    & =\n    \\mathbb E [\n      R_{t+1} \\mid S_t = s\n    ]\n    + \\gamma v_\\pi(S_{t+1})\n  \\end{aligned}  \n\\tag{5.6}\\]\nso, we have the Bellmann Equation (for deterministic policies):\n\\[\n  v_\\pi(s) =\n    \\mathcal R_s^\\pi\n    + \\gamma\n    \\sum_{s' \\in \\mathcal S}\n    P_{ss'} ^ \\pi v_\\pi (s')\n\\]\nNote: $ v(s’)$ are the values of the successor states of \\(s\\), so in \\[ \\sum_{s'\\in \\mathcal S} P_{ss'} v(s')\\] the weights \\(P_{ss'}\\) of the values \\(v(s')\\) are the transitions back in time.\n\nBellman Equation\nAlso for non deterministic policies:\n\\[\n  v_\\pi(s) =\n    \\sum_a \\pi(a \\mid s)\n      \\left(\n        \\mathcal R_s^a\n        + \\gamma\n        \\sum_{s' \\in \\mathcal S}\n        \\mathcal P^a_{s,s'} v_\\pi(s')\n      \\right)\n\\]\nBellmann equations in vector-matrix form:\n\\[ \\vec v = \\mathcal R^{\\pi} + \\gamma P^{\\pi} \\vec v \\]\nHere, we can directly solve the Bellmann equation:\n\\[ \\vec v = (I - \\gamma \\mathcal P^\\pi)^{-1} \\mathcal R^\\pi \\]\n\n\n\n\n\n\nPloting a vaule function\n\n\n\n\n\ndef evaluate_policy(policy):\n    P_pi_transpose, reward = get_P_reward_for_policy(policy)\n    \n    ### Here we must use P_pi_transpose.T - transformation back in time to the previous state!\n    values = np.dot(np.linalg.inv(np.eye((MAX_CAPACITY+1)**2) - GAMMA * P_pi_transpose.T), reward)\n    return values\npolicy_ = np.zeros(((MAX_CAPACITY+1)**2), dtype=int)\nvalues = evaluate_policy(policy_)\nplot3d_over_states(values, 'v')\n\n\n\nFor large state spaces computing the inverse is costly in terms of space and time. However, the bellman equation can be solved by an iteration:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#policy-evaluation",
    "href": "05-dynamicProgramming/dp_rl.html#policy-evaluation",
    "title": "Dynamic Programming (DP)",
    "section": "Policy Evaluation",
    "text": "Policy Evaluation\nComputing the state-value function \\(v^\\pi(s)\\) for an arbitrary policy by an iterative application of Bellman expectation backup is called policy evaluation:\n\n\\(v_1 \\rightarrow v_2 \\rightarrow v_3 \\rightarrow \\dots \\rightarrow v_\\pi\\)\nloop until convergence (using synchronous backups)\n\nat each iteration \\(k+1\\)\nfor all states \\(s \\in \\mathcal S\\)\nupdate \\(v_{k+1}(s)\\) from \\(v_k(s')\\), where \\(s'\\) is a successor state of \\(s\\)\n\n\n\\[\n  \\vec v_{k+1} = \\vec {\\mathcal R}^\\pi + \\gamma {\\mathcal P}^\\pi \\vec v_{k}\n\\]\n\n\n\n\n\n\nPolicy evaluation\n\n\n\n\n\ndef evaluate_policy_by_iteration(policy, values = np.zeros((MAX_CAPACITY+1)**2)):\n    P_pi, reward = get_P_reward_for_policy(policy)\n    converged = False\n    while not converged:\n        new_values = reward + GAMMA * np.dot (P_pi.T, values)\n        if np.allclose(new_values, values, rtol=1e-07, atol=1e-10):\n            converged = True\n        values = new_values\n    return values\nvalues_ = evaluate_policy_by_iteration(policy_)\nnp.testing.assert_almost_equal(values, values_, decimal=3)\n\n\n\n\nOptimal Value Function\n\\[ v^* (s) = \\max_\\pi v_\\pi(s) \\]\nThe optimal value \\(v^*(s)\\) of state \\(s \\in \\mathcal S\\) is defined as the highest achievable expected return when the process is started from state \\(s\\). The function \\(v^* : \\mathcal S \\rightarrow \\mathbb R\\) is called the optimal value function.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#policy-iteration-2",
    "href": "05-dynamicProgramming/dp_rl.html#policy-iteration-2",
    "title": "Dynamic Programming (DP)",
    "section": "Policy Iteration",
    "text": "Policy Iteration\n\nStarting with an arbitrary policy\nIteration until the values converge (to the optimal policy). Alternating sequence of\n\nPolicy Evaluation : Compute the values of the policy\nGreedy Policy Improvement: Use the “best” action in each state, i.e. chose the action which result in the highest return for each state.\n\n\n\nGreedy Policy Improvement\nFor greedy policy improvement we need the model (transition probabilities) if we are using the value function \\(v(s)\\): For each action \\(a\\) we compute the probabilities of the next states and calculate the average value of the discounted return.\nIf we start from state \\(s\\) with action \\(a\\), and then follow policy \\(\\pi\\) we get as expected return\n\\[\n  q_\\pi (s,a) = \\mathbb E\n    \\left[\n      R \\mid s,a\n    \\right]\n   + \\mathbb E \\left[\n     \\gamma \\sum_{s'\\in \\mathcal S} P_{ss'}^\\pi v_\\pi (s')\n   \\right]\n\\]\n\\(q_\\pi (s,a)\\) is called the action-value function \\(q_\\pi (s,a)\\).\nHere, we compute \\(q\\) explicitly for each state \\(s\\) and action \\(a\\) (respectivly in matrix form). For each state we chose the action that results in the highest value.\nTo understand why Greedy Policy Improvement works read the section of Sutton’s book [1].\n\n[1] R.S. Sutton, A.G. Barto, Reinforcement learning: An introduction, Second, MIT Press, Cambridge, MA, 2018.\n\n\n\n\n\n\nPolicy improvement\n\n\n\n\n\ndef greedy_improve(values, P=P):\n    P_ = P.transpose(1, 0, 2) # we used the model for improvement\n    all_pure_states = np.eye((MAX_CAPACITY+1)**2)\n    new_states = np.dot(all_pure_states, P_) \n    q = np.dot(new_states.transpose(2,1,0), values) \n    q = q.T + Reward\n    policy_indices = np.argmax(q, axis=1)\n    policy = policy_indices - TRANSFER_MAX\n    return policy\ndef improve_policy(policy):\n    values = evaluate_policy_by_iteration(policy)\n    not_converged = True\n    while not_converged:\n        print (values.mean())\n        new_policy = greedy_improve(values)\n        #new_values = evaluate_policy(new_policy)\n        new_values = evaluate_policy_by_iteration(new_policy, values)\n        if np.allclose(new_values, values, rtol=1e-02):\n            not_converged = False\n        values = new_values \n    return new_policy\ndef plot_policy(policy):\n    A = np.arange(0, MAX_CAPACITY+1)\n    B = np.arange(0, MAX_CAPACITY+1)\n    A, B = np.meshgrid(A, B)\n    Po = policy.reshape(MAX_CAPACITY+1,-1)\n    levels = range(-5,6,1)\n    plt.figure(figsize=(7,6))\n    CS = plt.contourf(A, B, Po, levels)\n    cbar = plt.colorbar(CS)\n    cbar.ax.set_ylabel('actions')\n    #plt.clabel(CS, inline=1, fontsize=10)\n    plt.title('Policy')\n    plt.xlabel(\"cars at B\")\n    plt.ylabel(\"cars at A\")\npolicy = improve_policy(policy)\nvalues_ = evaluate_policy_by_iteration(policy)\nplot3d_over_states(values_, 'v') \n#| scrolled: true\nplot_policy(policy)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#q-function",
    "href": "05-dynamicProgramming/dp_rl.html#q-function",
    "title": "Dynamic Programming (DP)",
    "section": "Q-Function",
    "text": "Q-Function\nThe action-value function \\(q_\\pi (s,a)\\) of an MDP is the expected return starting from state \\(s\\) with action \\(a\\), and then follow policy \\(\\pi\\)\n\\[\n  q^\\pi (s,a) = \\mathbb E_\\pi[G_t \\mid S_t=s, A_t=a]\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#optimal-action-value-function",
    "href": "05-dynamicProgramming/dp_rl.html#optimal-action-value-function",
    "title": "Dynamic Programming (DP)",
    "section": "Optimal action-value function",
    "text": "Optimal action-value function\n\\[ q^* (s, a) = \\max_\\pi q^\\pi(s, a) \\]\nThe optimal action-value \\(q^*(s, a)\\) of state-action pair \\((s,a) \\in \\mathcal S \\times \\mathcal A\\) gives the highest achievable expected return when the process is started from state \\(s\\), and the first action chosen is \\(a\\). The function \\(q^* : \\mathcal S \\times \\mathcal A \\rightarrow \\mathbb R\\) is called the optimal action-value function.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#connection-between-optimal-value--and-action-value-function",
    "href": "05-dynamicProgramming/dp_rl.html#connection-between-optimal-value--and-action-value-function",
    "title": "Dynamic Programming (DP)",
    "section": "Connection between optimal value- and action-value function",
    "text": "Connection between optimal value- and action-value function\n\\[ v^*(s) = \\max_{a \\in \\mathcal A} q^*(s,a), s \\in \\mathcal S \\]\n\\[\n  q^*(s,a) =\n    \\mathcal R_s^a + \\gamma\\sum_{s' \\in \\mathcal S} P(s,a,s')v^*(s');\n  s \\in \\mathcal S, a \\in \\mathcal A\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#bellman-equation-for-q",
    "href": "05-dynamicProgramming/dp_rl.html#bellman-equation-for-q",
    "title": "Dynamic Programming (DP)",
    "section": "Bellman Equation for Q",
    "text": "Bellman Equation for Q\n\\[\n  q_\\pi(s, a) =\n    \\mathcal R_s^a\n    + \\gamma \\sum_{s' \\in \\mathcal S}\n      \\mathcal P^a_{s,s'}\n      \\left(\n        \\sum_{a'\\in \\mathcal A}\n          \\pi(a' \\mid s') q_\\pi(s',a')\n      \\right)\n\\]\nfor a deterministic policy:\n\\[\n  q_\\pi(s, a) =\n    \\mathcal R_s^a\n    + \\gamma \\sum_{s' \\in \\mathcal S} \\mathcal P^a_{s,s'} q_\\pi(s',\\pi(s'))\n\\]\n\\[ \\hat Q^\\pi = \\hat R + \\gamma \\mathcal P \\vec Q^\\pi_\\pi \\] with\n\n\\(\\vec \\pi\\): Policy vector\nMatrices \\(\\hat Q^\\pi, \\hat R\\) with entries \\(\\cdot_{sa}\\)\n3d Array \\(\\mathcal P\\) with entries \\(\\cdot_{sas'}\\)\n\\(\\vec Q^\\pi_\\pi = \\vec v^\\pi\\)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#policy-evaluation-for-q",
    "href": "05-dynamicProgramming/dp_rl.html#policy-evaluation-for-q",
    "title": "Dynamic Programming (DP)",
    "section": "Policy Evaluation for Q",
    "text": "Policy Evaluation for Q\nComputing the state-action-value function \\(q^\\pi(s, a)\\) for an arbitrary policy by an iterative application of Bellman expectation backup is also called policy evaluation:\n\n\\(q_1 \\rightarrow q_2 \\rightarrow q_3 \\rightarrow \\dots \\rightarrow q_\\pi\\)\nusing synchronous backups\n\nat each iteration \\(k+1\\)\nfor all states \\(s \\in \\mathcal S\\) and \\(a \\in \\mathcal A\\)\nupdate \\(q_{k+1}(s, a)\\) from \\(q_k(s', \\cdot)\\), where \\(s'\\) is a successor state of \\(s\\).\n\n\n\\[ \\hat Q^{k+1} = \\hat R + \\gamma \\mathcal P \\vec Q^k_{k} \\]\n\n\n\n\n\n\n\\(Q\\)-Policy evaluation\n\n\n\n\n\n# Iterative update of the bellmann equation for Q\ndef evaluate_policy_Q_by_iteration(\n  policy,\n  Q = np.zeros([(MAX_CAPACITY+1) ** 2,\n  2*TRANSFER_MAX+1])\n):\n    converged = False\n    P_ = P.transpose(1,2,0)\n    while not converged:\n        policy_index = policy + TRANSFER_MAX\n        Q_pi = Q[np.arange((MAX_CAPACITY+1)**2), policy_index]\n        new_Q = Reward + GAMMA * np.dot(P_, Q_pi)\n        if np.allclose(new_Q, Q):\n            converged = True\n        Q = new_Q\n    return Q",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#policy-iteration-for-q",
    "href": "05-dynamicProgramming/dp_rl.html#policy-iteration-for-q",
    "title": "Dynamic Programming (DP)",
    "section": "Policy Iteration for Q",
    "text": "Policy Iteration for Q\n\nStarting with an arbitrary policy\nIteration until the values converge (to the optimal policy). Alternating sequence of\n\nPolicy Evaluation : Compute the values of the policy\nGreedy Policy Improvement:\n\n\\(\\pi'(s) = \\text{arg} \\max_{a \\in \\mathcal A} q^\\pi(s,a)\\)\nNote: Here we don’t use the (transition) model.\n\n\n\n\n\n\n\n\n\n\\(Q\\)-Policy iteration\n\n\n\n\n\ndef greedy_improve_Q(Q):\n    return np.argmax(Q, axis=1) - TRANSFER_MAX\n\n\npolicy = np.ones((MAX_CAPACITY+1)**2, dtype=int) * 0\nQ = np.zeros([(MAX_CAPACITY+1)**2, 2*TRANSFER_MAX+1])\n\nconverged=False\nwhile not converged:\n    new_Q = evaluate_policy_Q_by_iteration(policy, Q)\n    policy = greedy_improve_Q(Q)\n    if np.allclose(new_Q, Q):\n            converged = True\n    Q = new_Q\n    \nplot_policy(policy)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#bellman-optimality-equation",
    "href": "05-dynamicProgramming/dp_rl.html#bellman-optimality-equation",
    "title": "Dynamic Programming (DP)",
    "section": "Bellman Optimality Equation",
    "text": "Bellman Optimality Equation\n\\[\n    \\begin{aligned}\n        v^*(s) & =\n            \\max_a \\mathbb E\n            \\left[\n                R_{t+1} + \\gamma v^*(S_{t+1}) \\mid S_t = s, A_t = a\n            \\right]\n            \\\\\n                & =\n                \\max_a\n                \\left(\n                    \\mathcal R^a_{s} + \\gamma \\sum_{s'} P_{ss'}^a \\mathcal v^*(S_{t+1}=s')\n                \\right)\n    \\end{aligned}\n\\]\n\\[\n    \\begin{aligned}\n        q^*(s, a) &=\n            \\mathbb E \\left[\n              R_{t+1} + \\gamma \\max_{a'} q^*(S_{t+1},A_{t+1}=a') \\mid S_t = s, A_t = a\n            \\right]\n        \\\\\n            &=\n            \\mathcal R^a_{s}\n                + \\gamma \\sum_{s'} P_{ss'}^a\n                \\mathcal \\max_{a'} q^*(S_{t+1}=s',A_{t+1}=a')\n        \\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#value-iteration",
    "href": "05-dynamicProgramming/dp_rl.html#value-iteration",
    "title": "Dynamic Programming (DP)",
    "section": "Value Iteration",
    "text": "Value Iteration\nFrom the Bellmann optimality equation we can derive an iterative method to find the optimal value function:\n\\[\n    \\begin{aligned}\n        v_{k+1}(S_t = s)\n            &=\n            \\max_a \\mathbb E[R_{t+1} + \\gamma v_k(S_{t+1}=s') \\mid S_t=s, A_t=a]\n            \\\\\n            & = \\max_a\n            \\left(\n                \\mathcal R_{s}^a + \\gamma \\sum_{s'} \\mathcal P_{ss'}^a v_k(s')\n        \\right)\n    \\end{aligned}\n\\]\n\n\n\n\n\n\nValue iteration\n\n\n\n\n\nconverged = False\nvalues = np.zeros((MAX_CAPACITY+1)**2)\nP_ = P.transpose(1,2,0)\nwhile not converged:\n    rs = Reward + GAMMA * np.dot(P_, values) \n    new_values = np.max(rs, axis = 1)\n    if np.allclose(values, new_values):\n        converged = True\n    values = new_values\npolicy = np.argmax(Reward + GAMMA * np.dot(P_, values) , axis=1) - TRANSFER_MAX\nplot_policy(policy)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "05-dynamicProgramming/dp_rl.html#bibliography--5",
    "href": "05-dynamicProgramming/dp_rl.html#bibliography--5",
    "title": "Dynamic Programming (DP)",
    "section": "Refrences",
    "text": "Refrences",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Dynamic Programming (DP)</span>"
    ]
  },
  {
    "objectID": "06-applications/applications.html",
    "href": "06-applications/applications.html",
    "title": "Applications",
    "section": "",
    "text": "Recycling Robot",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Applications</span>"
    ]
  },
  {
    "objectID": "06-applications/applications.html#a-robot-with-randomly-moves-in-a-grid-world.",
    "href": "06-applications/applications.html#a-robot-with-randomly-moves-in-a-grid-world.",
    "title": "Applications",
    "section": "A robot with randomly moves in a grid world.",
    "text": "A robot with randomly moves in a grid world.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Applications</span>"
    ]
  },
  {
    "objectID": "07-Project/project_proposal.html",
    "href": "07-Project/project_proposal.html",
    "title": "Project proposal",
    "section": "",
    "text": "Formulation and reinforcement learning solution to a problem\nof a sequence of decisions.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project proposal</span>"
    ]
  },
  {
    "objectID": "07-Project/project_proposal.html#death-line-december-08-2024-235900",
    "href": "07-Project/project_proposal.html#death-line-december-08-2024-235900",
    "title": "Project proposal",
    "section": "Death line: December 08, 2024-23:59:00",
    "text": "Death line: December 08, 2024-23:59:00",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project proposal</span>"
    ]
  },
  {
    "objectID": "07-Project/project_proposal.html#sub-products",
    "href": "07-Project/project_proposal.html#sub-products",
    "title": "Project proposal",
    "section": "Sub-products",
    "text": "Sub-products\n\n\n\nDeath lines\n\n\n\n\n\nStage 01\nNovember 20, 2024-23:59\n\n\nStage 02\nDecember 15, 2024-23:59\n\n\nStage 02\nDecember 18, 2024-23:59\n\n\n\n\nStage 01: Quarto book with MDP formulation\n\nThe page must encloses the report according to the template rl_bookdown_prg.qmd\n\nIntroduction\nFormulation of the Mrakov decision process\nModel dynamics\nDescription and justification of the Cost (reward)\nJustification of the actions\n\nMust include\n\nFigures to illustrates the behavior of the regarding elements:\n\nPolicy\nReward\nValue function eventuated for a one state-action and transition.\nEnvironmental model\n\nReferences via bibtex.\nOutput compilation for HTML and PDF formats.\nThe compiled version has to be mounted ing GitHub or Quarto Pub\n\n\n\n\nStage 02: Python code Implementation\n\nOnly code whit out running errors wold be accepted\nCode must follows the style guide from PEP 08\nAll functions must include doc-strings\nExtras:\nPacking and Documentation extra 200 xps\n\n\n\nStage 03: Video Presentation\nA video mounted in you-tube of at most 20 min with results and insight of your project",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project proposal</span>"
    ]
  },
  {
    "objectID": "07-Project/project_proposal.html#suggested-project-list",
    "href": "07-Project/project_proposal.html#suggested-project-list",
    "title": "Project proposal",
    "section": "Suggested project list:",
    "text": "Suggested project list:\n\nReinforcement learning simulation of the TIC-TAC-TOE Game with SARSA or Q-learning Algorithms [2]\nThe movement of a Recycling Robot [2]\nThe replacement of a bus engine [5] from [see pd.pdf, p.130 6]\nOptimal Inventories [see dp.pdf, p. 147 6]\nMulti-Armed Bandits [2]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project proposal</span>"
    ]
  },
  {
    "objectID": "07-Project/project_proposal.html#project-lits",
    "href": "07-Project/project_proposal.html#project-lits",
    "title": "Project proposal",
    "section": "Project Lits",
    "text": "Project Lits\n\nProject list\n\n\n\n\n\n\n\n\nProject\nAuthor\nReference\nGitHub Repo\n\n\n\n\nDynamic Portfolio Analysis\nGABRIEL MIRANDA GAMEZ\n[Sec. 4.3, 1]\n(gh_page)[https://gabo-always-learning.quarto.pub/project-dpa/]\n\n\nAnalyzing Learned Markov Decision Processes using Model Checking for\n\n\n\n\n\nProviding Tactical Advice in Professional Soccer\nJOSE ITALO SANCHEZ BERMUDEZ\n\n\n\n\n[3]\n(gh_page)[https://italosanchezb.github.io/Proyecto-Final-RL/]\n\n\n\n\nA MDP model for the collective behavior in vaccination campaigns\nIRASEMA PEDROZA MEZA\n\n(GitHub)[https://github.com/IrasemaPM/Proyecto_psi]\n\n\nModelo de inventario para alimentos pedecederos\nDAVID PEÑA PERALTA\n[6]\n(gh_page)[https://dust1920.github.io/InventoryManagement/]\n\n\nA inventory model\nJAZMIN SARAHI FLORES GOMEZ\n[4]\n(gh_page)[https://flordejazmin.github.io/Proyectoinventario/]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project proposal</span>"
    ]
  },
  {
    "objectID": "07-Project/project_proposal.html#configuration-to-build-with-spanish-language",
    "href": "07-Project/project_proposal.html#configuration-to-build-with-spanish-language",
    "title": "Project proposal",
    "section": "Configuration to build with Spanish language",
    "text": "Configuration to build with Spanish language\nAdapt to your project accordingly to your .png files another sources.\n\n\n_quarto.yml\n\n  project:\n  type: book\n  output-dir: _book\n\nwebsite:\n  favicon: FCFMLOGO.png\n  reader-mode: true\n  search:\n    location: sidebar\n    type: overlay\n  comments:\n    hypothesis: true\n\nbook:\n  title: \"Análisis comparativo del desempeño en métodos para el pronóstico de series temporales\"\n  reader-mode: true\n  language: es\n  date: \"02/14/2024\"\n  output-file: \"Tesis_JSLG\"\n  # image: logofcfm.png\n  # cover-image: FCFMLOGO.png\n  sharing: [twitter, facebook]\n  downloads: [pdf, epub]\n  # favicon: logofcfm.png\n  sidebar:\n  #  logo: LOGO50.png\n    style: floating\n    collapse-level: 2\n    border: true\n    search: true\n  open-graph: true\n  twitter-card: true\n  #repo-url: https://github.com/Jennlg/Tesis\n  repo-actions: [edit, issue, source]\n  page-navigation: true\n  chapters:\n    - index.qmd\n    - intro.qmd\n    - objetivos.qmd\n\n    - part: 'Preliminares'\n      chapters:\n        - tconjuntos.qmd\n        - probabilidad.qmd\n        - estadistica.qmd\n        - procesos.qmd\n    - part: 'Series de tiempo'\n      chapters:\n        - series.qmd\n    - part: 'Redes neuronales'\n      chapters:\n        - redes.qmd\n    - part: estudio.qmd\n      chapters:\n        - metodologia.qmd\n        - confirmados.qmd\n        - muertes.qmd\n    - conclusiones.qmd\n\n    - references.qmd\n\ncomments:\n    hypothesis: true\n\nbibliography: references.bib\n\nformat:\n  html:\n    theme:\n      dark: darkly\n      light: cerulean\n    highlight-style: a11y\n    lang: es\n    html-math-method: mathjax\n    grid:\n      sidebar-width: 300px\n      body-width: 900px\n      margin-width: 300px\n      gutter-width: 1.5rem\n    code-copy: true\n    code-fold: true\n  pdf:\n    lang: es\n    include-in-header:\n      - packa.tex\n    template-partials:\n      - before-body.tex\n    documentclass: scrreprt\n    papersize: us-letter\n    #titlegraphic: FCFMLOGO.png\n    institution: Universidad Autónoma de Chiapas\n    email: jennifer.lopez67@unach.mx\n    keep-tex: true\n  epub:\n    cover-image: FCFMLOGO.png\neditor: visual\n\nWe also need the following .tex in the root folder\n\\usepackage{upgreek}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\newcommand{\\dashedbox}[1]{\n  \\begin{tikzpicture}\n    \\node[draw, dashed, rounded corners=5pt, inner sep=10pt] {\n      \\begin{minipage}{0.8\\textwidth} % Establece el ancho del minipage\n        #1\n      \\end{minipage}\n    };\n  \\end{tikzpicture}\n}",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project proposal</span>"
    ]
  },
  {
    "objectID": "07-Project/project_proposal.html#bibliography--2",
    "href": "07-Project/project_proposal.html#bibliography--2",
    "title": "Project proposal",
    "section": "Refrences",
    "text": "Refrences\n\n\n[1] D.P. Bertsekas, Dynamic programming and optimal control. Vol. I, Third, Athena Scientific, Belmont, MA, 2005.\n\n\n[2] E. Bilgin, Mastering reinforcement learning with python: Build next-generation, self-learning models using reinforcement learning techniques and best practices, Packt Publishing, 2020.\n\n\n[3] T. Decroos, L. Bransen, J.V. Haaren, J. Davis, VAEP: An Objective Approach to Valuing On-the-Ball Actions in Soccer (Extended Abstract), Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. (2020) 4696–4700.\n\n\n[4] D. Levhari, L.J. Mirman, The great fish war: An example using a dynamic cournot-nash solution, Essays in the Economics of Renewable Resources, LJ Mirman and DF Spulber (Eds.), North-Holland. (1982) 243–258.\n\n\n[5] J. Rust, Optimal Replacement of GMC Bus Engines: An Empirical Model of Harold Zurcher, Econometrica. 55 (1987) 999.\n\n\n[6] J. Stachurski., Dynamic programming volume 1, GitHub Repository. (2024).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Project proposal</span>"
    ]
  },
  {
    "objectID": "08-Evaluation/rubric_evaluation.html",
    "href": "08-Evaluation/rubric_evaluation.html",
    "title": "Evaluation Rubric",
    "section": "",
    "text": "Evaluation Rubric\n\n\n\n\n\n\n\n\n\n\nStege 01 MDP F ormulation\n\nStage 02\nImpl ementation\n\nStage 03\nPr esentation\n\n\n\n\n\nIn troduction\n15\nCode\n70\nYou tube pre sesntation\n90\n\n\nF ormulation of the Mrakov decision process\n15\nPEP 08\n20\nLink reference inside related quarto web-page.\n10\n\n\nModel dynamics\n15\nDoc strings\n10\n\n\n\n\nD escription and jus tification of the Cost (reward)\n15\n\n\n\n\n\n\nJus tification of the actions\n15\n\n\n\n\n\n\nReferences via bibtex\n10\n\n\n\n\n\n\nOutput c ompilation for HTML and PDF formats.\n5\n\n\n\n\n\n\nThe compiled version has to be mounted in GitHub pages or Quarto Pub\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStudent\nProject\nStage 1\nStage 02\nStage 03\n\n\n\n\nGABRIEL MIRANDA GAMEZ\nDynamic Portfolio Analysis\n100\n\n\n\n\nIRASEMA PEDROZA MEZA\nA MDP model for the collective behavior in vaccination campaigns\n90\n\n\n\n\nDAVID PEÑA PERALTA\nModelo de inventario para alimentos pedecederos\n100\n\n\n\n\nJAZMIN SARAHI FLORES GOMEZ\nA inventory model\n100\n\n\n\n\n\n\n\n\n\n\n\n\nProject | | | |\nStudent | | | |\n|————- |—————————————————————– |————– |————– | | | Stage 01 | Stage 02 | Stage 03 | | | Introduction | | | | | Formulation of the Mrakov decision process | | | | | Model dynamics | | | | | Description and justification of the Cost (reward) | | | | | Justification of the action | | | | | References via bibtex. | | | | | Output compilation for HTML and PDF formats. | | | | | The compiled version has to be mounted ing GitHub or Quarto Pub | | |",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Evaluation Rubric</span>"
    ]
  },
  {
    "objectID": "homeworks/home_works_list.html",
    "href": "homeworks/home_works_list.html",
    "title": "List of Home Works and due dates",
    "section": "",
    "text": "Homework 001 due date: september 20, 2024-12:00:00",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>List of Home Works and due dates</span>"
    ]
  },
  {
    "objectID": "homeworks/home_works_list.html#bibliography",
    "href": "homeworks/home_works_list.html#bibliography",
    "title": "List of Home Works and due dates",
    "section": "Bibliography",
    "text": "Bibliography",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>List of Home Works and due dates</span>"
    ]
  },
  {
    "objectID": "homeworks/home_works_list.html#bibliography-1",
    "href": "homeworks/home_works_list.html#bibliography-1",
    "title": "List of Home Works and due dates",
    "section": "Bibliography",
    "text": "Bibliography",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>List of Home Works and due dates</span>"
    ]
  },
  {
    "objectID": "homeworks/home_works_list.html#bibliography-2",
    "href": "homeworks/home_works_list.html#bibliography-2",
    "title": "List of Home Works and due dates",
    "section": "Bibliography",
    "text": "Bibliography",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>List of Home Works and due dates</span>"
    ]
  },
  {
    "objectID": "homeworks/home_works_list.html#bibliography-3",
    "href": "homeworks/home_works_list.html#bibliography-3",
    "title": "List of Home Works and due dates",
    "section": "Bibliography",
    "text": "Bibliography\n\n\n[1] R.S. Sutton, A.G. Barto, Reinforcement learning: An introduction, Second, MIT Press, Cambridge, MA, 2018.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>List of Home Works and due dates</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "For the cured R-quarto material see https://github.com/SaulDiazInfante/intro-quarto-unison-2024.\n\n\n[1] D.P. Bertsekas, Dynamic programming and optimal\ncontrol. Vol. I, Third, Athena Scientific,\nBelmont, MA, 2005.\n\n\n[2] E.\nBilgin, Mastering\nreinforcement learning with python: Build next-generation, self-learning\nmodels using reinforcement learning techniques and best practices,\nPackt Publishing, 2020.\n\n\n[3] P.\nBrandimarte, Numerical methods in finance and economics: A MATLAB-based\nintroduction, 2nd ed., John Wiley & Sons, Hoboken, New Jersey,\n2013.\n\n\n[4] S.L. Brunton, J.N. Kutz, Data-driven science and\nengineering, Cambridge University Press, Cambridge, 2019.\n\n\n[5] T.\nDecroos, L. Bransen, J.V. Haaren, J. Davis, VAEP: An Objective Approach to Valuing On-the-Ball\nActions in Soccer (Extended Abstract), Proceedings of the\nTwenty-Ninth International Joint Conference on Artificial Intelligence.\n(2020) 4696–4700.\n\n\n[6] D.\nLevhari, L.J. Mirman, The great fish war: An example using a dynamic\ncournot-nash solution, Essays in the Economics of Renewable Resources,\nLJ Mirman and DF Spulber (Eds.), North-Holland. (1982) 243–258.\n\n\n[7] J.\nRust, Optimal Replacement of GMC Bus Engines: An Empirical\nModel of Harold Zurcher, Econometrica. 55 (1987) 999.\n\n\n[8] J.\nStachurski., Dynamic programming volume 1, GitHub Repository.\n(2024).\n\n\n[9] R.S. Sutton, A.G. Barto, Reinforcement\nlearning: An introduction, Second, MIT Press, Cambridge, MA, 2018.\n\n\n[10] C.\nSzepesvári, Algorithms for\nreinforcement learning, Springer, Cham, 2022.",
    "crumbs": [
      "References"
    ]
  }
]