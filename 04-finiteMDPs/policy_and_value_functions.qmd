In reinforcement learning (RL), **value functions** play a central role in determining the effectiveness of an agent's behavior by estimating "how good" it is for the agent to be in a particular state or perform a specific action. Here, "how good" is quantified by **future rewards** the agent expects to accumulate, which is also known as the **expected return**. The expected return typically refers to the sum of future rewards, discounted over time, that an agent can expect to obtain from a particular state or state-action pair.

### Key Concepts:

#### 1. Value Functions: 
 
 A value function is a mathematical function that estimates the future rewards expected when starting in a specific state or taking a particular action in a state. Two types of value functions commonly appear in RL:

##### State value function $(V)$ 

This estimates the value of a state, which is the expected return starting from that state and following a certain policy thereafter. 
The **state value function** $V^\pi(s)$ under a policy $\pi$ represents the expected return starting from state $s$ and following the policy $\pi$ thereafter. Mathematically, it is defined as:

$$
	V^\pi(s) = \mathbb{E}_\pi \left[ G_t \mid S_t = s \right]
$$

where:

- $G_t$ is the **return** from time $t$, typically defined as the sum of discounted rewards:
$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$
- $S_t$ is the state at time $t$, and $R_{t+1}$ is the reward received after transitioning from state $S_t$ to $S_{t+1}$.
- $\gamma$ is the **discount factor** that determines the present value of future rewards, where $0 \leq \gamma \leq 1$.

The goal is to calculate $V^\pi(s)$, which gives the expected return if the agent starts in state $s$ and follows policy $\pi$.

##### Action Value Function $q_{\pi}(s, a)$

The **action value function** $q_{\pi}(s, a)$ gives the expected return for taking action $a$ in state $s$ and then following the policy $\pi$. It is defined as:

$$
q_{\pi}(s, a) = \mathbb{E}_\pi \left[ G_t \mid S_t = s, A_t = a \right]
$$
This function tells us how good it is to take a particular action $a$ in a state $s$, assuming we follow policy $\pi$ afterwards.

This estimates the value of taking a specific action in a given state, which represents the expected return from that state-action pair, following a particular policy from that point onwards.

#### 2.Expected Return:
   - The expected return is the total amount of reward an agent can anticipate from a particular point in time, considering both immediate and future rewards, usually discounted by a factor $\gamma$ (the discount factor). This discount factor weights the importance of future rewards relative to immediate rewards.

#### 3. **Policies**:
   - A policy $\pi$ is the strategy or decision-making rule that defines the actions an agent will take in any given state. The value functions are always associated with a particular policy, meaning the future rewards depend on the actions dictated by the policy.

   Value functions are thus dependent on the agent's **policy**, which determines the sequence of actions that the agent will take as it interacts with the environment. Policies can be deterministic (where each state leads to a fixed action) or stochastic (where actions are selected according to a probability distribution).



#### 4. **Bellman Equations for $V^\pi(s)$ and $q_{\pi}(s, a)$**:

To compute the value functions, we use the **Bellman equation**, which expresses the value of a state or state-action pair in terms of the immediate reward plus the discounted value of the next state.

For the **state value function**, the Bellman equation is:

$$
V^\pi(s) = \sum_{a} \pi(a \mid s) \sum_{s'} P(s' \mid s, a) \left[ R(s, a, s') + \gamma V^\pi(s') \right]
$$

where:

- $\pi(a \mid s)$ is the probability of taking action $a$ in state $s$ under policy $\pi$,
- $P(s' \mid s, a)$ is the probability of transitioning to state $s'$ after taking action $a$ in state $s$,
- $R(s, a, s')$ is the reward received when transitioning from $s$ to $s'$ after taking action $a$.

For the **action value function**, the Bellman equation is:

$$
q_{\pi}(s, a) = \sum_{s'} P(s' \mid s, a) \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a' \mid s') Q^\pi(s', a') \right]
$$
These equations give a recursive way of expressing value functions, which are central to many RL algorithms.


### $Q$-**Learning** (Off-policy learning algorithm)

**Q-learning** is a **model-free**, **off-policy** algorithm that seeks to find the optimal action-value function $Q^*(s, a)$, which represents the maximum expected return for taking action $a$ in state $s$ and following the optimal policy from that point onward.

The Q-learning update rule is:

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ R_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
$$

Where:

- $\alpha$ is the **learning rate**,
- $R_{t+1}$ is the immediate reward,
- $\gamma$ is the discount factor,
- $\max_{a'} Q(s_{t+1}, a')$ is the maximum estimated value of the next state $s_{t+1}$ over all possible actions $a'$.

The key feature of Q-learning is that it is **off-policy**, meaning that the learning happens regardless of the policy the agent is currently following. The agent can learn the optimal policy while exploring the environment using a different policy.

### **Policy Iteration** (On-policy learning algorithm)

**Policy iteration** is a classic **on-policy** algorithm that alternates between policy evaluation and policy improvement until the optimal policy is found.

- **Policy Evaluation**: Given a policy $\pi$, calculate the value function $V^\pi(s)$ for all states using the Bellman equation for the state-value function.
  
  This involves solving the system of equations for $V^\pi(s)$ either by iterating until convergence (infinite horizon) or using a matrix form if the state space is small.

- **Policy Improvement**: Using the current value function $V^\pi(s)$, improve the policy by choosing actions that maximize the expected return:

$$
\pi'(s) = \arg\max_a \sum_{s'} P(s' \mid s, a) \left[ R(s, a, s') + \gamma V^\pi(s') \right]
$$

These two steps repeat: after improving the policy, we re-evaluate the value function and then improve the policy again. The algorithm converges when the policy no longer changes, indicating that the optimal policy $\pi^*$ has been found.


### **Value Iteration**

**Value iteration** combines policy evaluation and policy improvement into a single step. Instead of fully evaluating the current policy, value iteration updates the value function directly using the Bellman optimality equation:

$$
V(s) \leftarrow \max_a \sum_{s'} P(s' \mid s, a) \left[ R(s, a, s') + \gamma V(s') \right]
$$

Once the value function has converged, the optimal policy $\pi^*$ is derived by choosing actions that maximize the value function.

### Summary:

In summary, value functions $V^\pi(s)$ and $q_{\pi}(s, a)$ estimate future rewards based on an agentâ€™s policy. These functions are fundamental to algorithms like **Q-learning** (off-policy) and **policy iteration** (on-policy). Q-learning directly updates the Q-values, aiming to discover the optimal action-value function, while policy iteration alternates between evaluating a policy and improving it.
