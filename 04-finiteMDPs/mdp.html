<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Saúl Díaz Infante Velasco">

<title>From Markov Decision Processes to Reinforcement Learning with Python - Finite Markov Decision Processes (MDPs)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../05-dynamicProgramming/dp_rl.html" rel="next">
<link href="../03-multiArmedBandit/multiarmed_bandits.html" rel="prev">
<link href="../cover_RL.jpeg" rel="icon" type="image/jpeg">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script src="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.js"></script>
<link href="../site_libs/quarto-contrib/pseudocode-2.4.1/pseudocode.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="application/json" class="js-hypothesis-config">
{
  "theme": "classic",
  "openSidebar": false
}
</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>
<script>
MathJax = {
  loader: {
    load: ['[tex]/boldsymbol']
  },
  tex: {
    tags: "all",
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true,
    packages: {
      '[+]': ['boldsymbol']
    }
  }
};
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">From Markov Decision Processes to Reinforcement Learning with Python</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-wide tools-end">
    <a href="https://github.com/SaulDiazInfante/RL-Course-2024-2.git" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../04-finiteMDPs/mdp.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Finite Markov Decision Processes (MDPs)</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
        <div class="sidebar-tools-main">
    <a href="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-save"></i></a>
</div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Preface</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../01-introduction/general_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Abstract</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../02-introductionToRL/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../03-multiArmedBandit/multiarmed_bandits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multi-armed Bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../04-finiteMDPs/mdp.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Finite Markov Decision Processes (MDPs)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../05-dynamicProgramming/dp_rl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Dynamic Programming (DP)</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../06-applications/applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Applications</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../07-Project/project_proposal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Project proposal</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../08-Evaluation/rubric_evaluation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Evaluation Rubric</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../homeworks/home_works_list.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">List of Home Works and due dates</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../homeworks/_hw_grades.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Homework grades</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul class="collapse">
  <li><a href="#the-agentenvironment-interface" id="toc-the-agentenvironment-interface" class="nav-link active" data-scroll-target="#the-agentenvironment-interface">The Agent–Environment Interface</a></li>
  <li><a href="#goals-and-rewards" id="toc-goals-and-rewards" class="nav-link" data-scroll-target="#goals-and-rewards">Goals and Rewards</a>
  <ul class="collapse">
  <li><a href="#key-points-of-the-reward-hypothesis" id="toc-key-points-of-the-reward-hypothesis" class="nav-link" data-scroll-target="#key-points-of-the-reward-hypothesis">Key Points of the Reward Hypothesis:</a></li>
  <li><a href="#importance-in-reinforcement-learning" id="toc-importance-in-reinforcement-learning" class="nav-link" data-scroll-target="#importance-in-reinforcement-learning">Importance in Reinforcement Learning:</a></li>
  </ul></li>
  <li><a href="#returns-and-episodes" id="toc-returns-and-episodes" class="nav-link" data-scroll-target="#returns-and-episodes">Returns and Episodes</a>
  <ul class="collapse">
  <li><a href="#the-reward-sequence" id="toc-the-reward-sequence" class="nav-link" data-scroll-target="#the-reward-sequence">1. <strong>The Reward Sequence</strong>:</a></li>
  <li><a href="#defining-the-return" id="toc-defining-the-return" class="nav-link" data-scroll-target="#defining-the-return">2. <strong>Defining the Return</strong>:</a></li>
  <li><a href="#discounted-return" id="toc-discounted-return" class="nav-link" data-scroll-target="#discounted-return">3. <strong>Discounted Return</strong>:</a></li>
  <li><a href="#maximizing-the-expected-return" id="toc-maximizing-the-expected-return" class="nav-link" data-scroll-target="#maximizing-the-expected-return">4. <strong>Maximizing the Expected Return</strong>:</a></li>
  <li><a href="#two-types-of-tasks" id="toc-two-types-of-tasks" class="nav-link" data-scroll-target="#two-types-of-tasks">5. <strong>Two Types of Tasks</strong>:</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion:</a></li>
  </ul></li>
  <li><a href="#unified-notation-for-episodic-and-continuing-tasks" id="toc-unified-notation-for-episodic-and-continuing-tasks" class="nav-link" data-scroll-target="#unified-notation-for-episodic-and-continuing-tasks">Unified Notation for Episodic and Continuing Tasks</a></li>
  <li><a href="#policies-and-value-functions" id="toc-policies-and-value-functions" class="nav-link" data-scroll-target="#policies-and-value-functions">Policies and Value Functions</a>
  <ul class="collapse">
  <li><a href="#pick-and-drop-example" id="toc-pick-and-drop-example" class="nav-link" data-scroll-target="#pick-and-drop-example">Pick and drop example</a></li>
  </ul></li>
  <li><a href="#optimal-policies-and-optimal-value-functions" id="toc-optimal-policies-and-optimal-value-functions" class="nav-link" data-scroll-target="#optimal-policies-and-optimal-value-functions">Optimal Policies and Optimal Value Functions</a></li>
  <li><a href="#optimality-and-approximation" id="toc-optimality-and-approximation" class="nav-link" data-scroll-target="#optimality-and-approximation">Optimality and Approximation</a></li>
  <li><a href="#summary-1" id="toc-summary-1" class="nav-link" data-scroll-target="#summary-1">Summary</a>
  <ul class="collapse">
  <li><a href="#bibliography" id="toc-bibliography" class="nav-link" data-scroll-target="#bibliography">Bibliography</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/SaulDiazInfante/RL-Course-2024-2.git/edit/main/04-finiteMDPs/mdp.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/SaulDiazInfante/RL-Course-2024-2.git/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Finite Markov Decision Processes (MDPs)</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Saúl Díaz Infante Velasco </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<p>That is what ChatGPT would answer to a 5-year-old kid. Alright, let’s imagine you have a little robot friend named Robo. Robo likes to explore and do different things, but Robo doesn’t always know what to do next. A Markov Decision Process (MDP) is like giving Robo a set of rules to help it decide what to do next based on where it is and what it knows.</p>
<p>Imagine Robo is in a room full of toys. Each toy is like a different choice Robo can make, like playing with blocks or reading a book. But Robo can’t see the whole room at once, so it has to decide what to do based on what it can see and remember.</p>
<p>In an MDP, Robo learns from its past experiences. If it finds that playing with blocks usually makes it happy, it’s more likely to choose that again next time. But if it tries reading a book and doesn’t like it, it might choose something else next time.</p>
<p>So, a Markov Decision Process helps Robo make decisions by learning from what it’s done before and what it can see around it, kind of like how you learn from playing with different toys and remembering which ones you like best.</p>
<section id="the-agentenvironment-interface" class="level2">
<h2 class="anchored" data-anchor-id="the-agentenvironment-interface">The Agent–Environment Interface</h2>
<div id="exm-roboClean" class="theorem example">
<p><span class="theorem-title"><strong>Example 1</strong></span> A mobile robot has the job of collecting empty soda cans in an office environment. It has sensors for detecting cans, and an arm and gripper that can pick them up and place them in an onboard bin; it runs on a rechargeable battery. The robot’s control system has components for interpreting sensory information, for navigating, and for controlling the arm and gripper. High-level decisions about how to search for cans are made by a reinforcement learning agent based on the current charge level of the battery. To make a simple example, we assume that only two charge levels can be distinguished, comprising a small state set <span class="math inline">\(\mathcal{S} = \{\texttt{high}, \texttt{low} \}\)</span>. In each state, the agent can decide whether to</p>
<ol type="1">
<li>actively <strong>search</strong> for a can for a certain period of time,</li>
<li>remain stationary and <strong>wait</strong> for someone to bring it a can, or</li>
<li>head back to its home base to <strong>recharge</strong> its battery.</li>
</ol>
<p>When the energy level is <strong>high</strong>, recharging would always be foolish h, so we do not include it in the action set for this state. The action sets are then <span class="math inline">\(\mathcal{A}(\texttt{high}) = \{\texttt{search}, \texttt{wait}\}\)</span> and <span class="math inline">\(\mathcal{A}(\texttt{low}) = \{\texttt{search}, \texttt{wait}, \texttt{recharge}\}\)</span>.</p>
<p>The rewards are zero most of the time, but become positive when the robot secures an empty can, or large and negative if the battery runs all the way down. The best way to find cans is to actively search for them, but this runs down the robot’s battery, whereas waiting does not. Whenever the robot is searching, the possibility exists that its battery will become depleted. In this case the robot must shut down and wait to be rescued (producing a low reward).</p>
<p>If the energy level is <strong>high</strong>, then a period of active search can always be completed without risk of depleting the battery. A period of searching that begins with a <strong>high</strong> energy level leaves the energy level high <strong>with</strong> probability <span class="math inline">\(\alpha\)</span> and reduces it to low with probability <span class="math inline">\(1 - \alpha\)</span>. On the other hand, a period of searching undertaken when the energy level is low leaves it low with probability <span class="math inline">\(\beta\)</span> and depletes the battery with probability <span class="math inline">\(1 - \beta\)</span>. In the latter case, the robot must be rescued, and the battery is then recharged back to <strong>high</strong>. Each can collected by the robot counts as a unit reward, whereas a reward of <span class="math inline">\(-3\)</span> results whenever the robot has to be rescued. Let <span class="math inline">\(r_{\texttt{search}}\)</span> and <span class="math inline">\(r_{\texttt{wait}}\)</span>, with <span class="math inline">\(r_{\texttt{search}} &gt; r_{\texttt{wait}}\)</span>, denote the expected numbers of cans the robot will collect (and hence the expected reward) while searching and while waiting respectively. Finally, suppose that no cans can be collected during a run home for recharging, and that no cans can be collected on a step in which the battery is depleted. This system is then a finite MDP, and we can write down the transition probabilities and the expected rewards, with dynamics as indicated in the table on the left:</p>
<div id="fig-robot-diagram" class="lightbox quarto-figure quarto-figure-center quarto-float anchored" data-fig-env="figure*">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-robot-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../assets/ch_03/recycling_robot_diagram.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" data-glightbox="description: .lightbox-desc-1"><img src="../assets/ch_03/recycling_robot_diagram.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-robot-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Recycling-robot’s graph. Taken from <span class="citation" data-cites="Sutton2018">[@Sutton2018]</span>
</figcaption>
</figure>
</div>
<div id="fig-robot-table" class="lightbox quarto-figure quarto-figure-center quarto-float anchored" data-fig-env="figure*">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-robot-table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../assets/ch_03/recycling_robot_table.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" data-glightbox="description: .lightbox-desc-2" title="Transitions and rewards for the recycling robot"><img src="../assets/ch_03/recycling_robot_table.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-robot-table-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2
</figcaption>
</figure>
</div>
</div>
</section>
<section id="goals-and-rewards" class="level1">
<h1>Goals and Rewards</h1>
<p>In reinforcement learning, <strong>the reward hypothesis</strong> serves as the foundation for defining the objectives of an agent operating within an environment. According to this hypothesis, the agent’s goal can be represented by the maximization of the <strong>expected cumulative reward</strong> over time, based on scalar feedback signals received from the environment.</p>
<section id="key-points-of-the-reward-hypothesis" class="level2">
<h2 class="anchored" data-anchor-id="key-points-of-the-reward-hypothesis">Key Points of the Reward Hypothesis:</h2>
<ol type="1">
<li><p><strong>Reward as a Scalar Signal</strong>: At every time step, the environment provides the agent with a simple numerical signal, <span class="math inline">\((R_t \in \mathbb{R}\)</span>, which represents the immediate reward based on the agent’s actions and the current state of the environment. This reward acts as feedback for the agent, helping it learn and adjust its strategy to achieve its ultimate goal.</p></li>
<li><p><strong>Maximizing Cumulative Reward</strong>: The agent’s goal is not just to maximize the immediate reward, but to focus on the long-term sum of rewards, known as the <strong>cumulative reward</strong>. This ensures that the agent does not become short-sighted by only pursuing short-term benefits, but rather seeks strategies that maximize its total reward across time.</p></li>
<li><p><strong>Expected Value of the Reward</strong>: Since reinforcement learning involves interaction in environments that can be stochastic (involving randomness or uncertainty), the agent aims to maximize the <strong>expected value</strong> of the cumulative reward, accounting for different possible future states and outcomes. This means the agent is interested in the average cumulative reward it would obtain over many possible sequences of interactions, rather than specific individual outcomes.</p></li>
<li><p><strong>Formalizing Goals and Purposes</strong>: According to the reward hypothesis, all goals, objectives, or purposes of the agent can be <strong>quantified</strong> by maximizing this cumulative scalar signal (reward). In other words, the “purpose” of the agent is simply to optimize the feedback it receives in the form of rewards, and this concept encapsulates everything the agent is designed to achieve.</p></li>
</ol>
</section>
<section id="importance-in-reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="importance-in-reinforcement-learning">Importance in Reinforcement Learning:</h2>
<p>This hypothesis is central to how reinforcement learning problems are structured. It reduces complex goals and objectives into a single, scalar value (the reward) that the agent can track and optimize over time. This abstraction makes it possible to design agents that can handle a wide variety of tasks, as long as those tasks can be expressed in terms of rewards provided by the environment.</p>
</section>
</section>
<section id="returns-and-episodes" class="level1">
<h1>Returns and Episodes</h1>
<p>To formalize the objective of learning in reinforcement learning, we introduce the concept of <strong>return</strong>. The <strong>return</strong>, denoted <span class="math inline">\(G_t\)</span>, is a function of the future rewards that the agent will receive after time step <span class="math inline">\(t\)</span>. The agent aims to maximize the <strong>expected return</strong> to achieve its goal. Let’s break down this formalization step by step:</p>
<section id="the-reward-sequence" class="level2">
<h2 class="anchored" data-anchor-id="the-reward-sequence">1. <strong>The Reward Sequence</strong>:</h2>
<p>At each time step <span class="math inline">\(t\)</span>, the agent receives a reward <span class="math inline">\(R_{t+1}, R_{t+2},\dots,\)</span> as a result of interacting with the environment. This sequence of rewards represents the feedback the agent receives over time based on its actions.</p>
</section>
<section id="defining-the-return" class="level2">
<h2 class="anchored" data-anchor-id="defining-the-return">2. <strong>Defining the Return</strong>:</h2>
<p>The <strong>return</strong> at time step <span class="math inline">\(t\)</span>, denoted <span class="math inline">\(G_t\)</span>, is the total accumulated reward from time step <span class="math inline">\(t\)</span> onward. In reinforcement learning, the return can be defined in different ways, but it generally involves summing the future rewards, often discounted to account for the uncertainty or diminishing value of rewards received further in the future.</p>
<p>The <strong>undiscounted return</strong> would simply be the sum of all future rewards: <span class="math display">\[
   G_t = R_{t+1} + R_{t+2} + R_{t+3} + \cdots = \sum_{k=0}^{\infty} R_{t+k+1}
\]</span> This sum may be infinite if the task never ends, which can be problematic. Thus, in many cases, a <strong>discount factor</strong> is applied to weight future rewards less than immediate rewards.</p>
<p>Likewise if the MDP has finite horizont <span class="math inline">\(T\)</span> then <span class="math display">\[G_t = R_{t+1} + R_{t+2} + R_{t+3} + \cdots + R_{T}\]</span></p>
</section>
<section id="discounted-return" class="level2">
<h2 class="anchored" data-anchor-id="discounted-return">3. <strong>Discounted Return</strong>:</h2>
<p>To address this issue, we introduce a discount factor <span class="math inline">\(\gamma\)</span>, where <span class="math inline">\(0\leq \gamma \leq 1\)</span>. The discount factor controls how much emphasis is placed on future rewards. When <span class="math inline">\(\gamma\)</span> is close to 1, future rewards are considered nearly as valuable as immediate rewards. When <span class="math inline">\(\gamma\)</span> is closer to 0, the agent focuses more on immediate rewards.</p>
<p>The <strong>discounted return</strong> is defined as: <span class="math display">\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots
    = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\]</span> Here, each future reward is multiplied by ( ^k ), where ( k ) is the number of time steps into the future. This ensures that the agent values immediate rewards more highly than rewards far into the future, which is often desirable in practical applications.</p>
</section>
<section id="maximizing-the-expected-return" class="level2">
<h2 class="anchored" data-anchor-id="maximizing-the-expected-return">4. <strong>Maximizing the Expected Return</strong>:</h2>
<p>Since the environment in reinforcement learning is often stochastic, the agent cannot guarantee a specific sequence of rewards, but it can aim to maximize the <strong>expected return</strong>. The expected return is the average return the agent would obtain by following a specific policy <span class="math inline">\(\pi\)</span>, which defines the agent’s behavior.</p>
<p>Formally, the agent’s objective is to maximize the expected return: <span class="math display">\[
    \mathbb{E}[G_t | \pi]
            = \mathbb{E}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \Big| \pi\right]
\]</span> where <span class="math inline">\(\pi\)</span> is the policy being followed. This equation tells us that the agent should choose actions in a way that maximizes the long-term expected reward, considering both immediate and future rewards.</p>
</section>
<section id="two-types-of-tasks" class="level2">
<h2 class="anchored" data-anchor-id="two-types-of-tasks">5. <strong>Two Types of Tasks</strong>:</h2>
<ul>
<li><strong>Finite-Horizon Tasks</strong>: These tasks have a fixed time limit, and the agent’s goal is to maximize the sum of rewards within that time limit. In such cases, the discount factor <span class="math inline">\(\gamma\)</span> may not be necessary, and the return is just the sum of the finite rewards received before the task ends.</li>
<li><strong>Infinite-Horizon Tasks</strong>: In tasks that continue indefinitely, the discount factor <span class="math inline">\(\gamma\)</span> ensures that the return remains finite by reducing the impact of rewards far into the future.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion:</h2>
<p>Thus, in reinforcement learning, the agent’s formal objective is to maximize the <strong>expected return</strong>, <span class="math inline">\(\mathbb{E}[G_t]\)</span>, where the return <span class="math inline">\(G_t\)</span> is the <strong>discounted sum</strong> of future rewards. The use of the discount factor <span class="math inline">\(\gamma\)</span> helps the agent focus more on immediate rewards, while still considering future rewards to some degree. This formalization ensures that the agent’s behavior is guided not just by immediate rewards but by a balanced approach to long-term success.</p>
</section>
</section>
<section id="unified-notation-for-episodic-and-continuing-tasks" class="level1">
<h1>Unified Notation for Episodic and Continuing Tasks</h1>
</section>
<section id="policies-and-value-functions" class="level1">
<h1>Policies and Value Functions</h1>
<p>In reinforcement learning (RL), <strong>value functions</strong> play a central role in determining the effectiveness of an agent’s behavior by estimating “how good” it is for the agent to be in a particular state or perform a specific action. Here, “how good” is quantified by <strong>future rewards</strong> the agent expects to accumulate, which is also known as the <strong>expected return</strong>. The expected return typically refers to the sum of future rewards, discounted over time, that an agent can expect to obtain from a particular state or state-action pair.</p>
<section id="key-concepts" class="level3">
<h3 class="anchored" data-anchor-id="key-concepts">Key Concepts:</h3>
<section id="value-functions" class="level4">
<h4 class="anchored" data-anchor-id="value-functions">1. Value Functions:</h4>
<p>A value function is a mathematical function that estimates the future rewards expected when starting in a specific state or taking a particular action in a state. Two types of value functions commonly appear in RL:</p>
<section id="state-value-function-v" class="level5">
<h5 class="anchored" data-anchor-id="state-value-function-v">State value function <span class="math inline">\((V)\)</span></h5>
<p>This estimates the value of a state, which is the expected return starting from that state and following a certain policy thereafter. The <strong>state value function</strong> <span class="math inline">\(V^\pi(s)\)</span> under a policy <span class="math inline">\(\pi\)</span> represents the expected return starting from state <span class="math inline">\(s\)</span> and following the policy <span class="math inline">\(\pi\)</span> thereafter. Mathematically, it is defined as:</p>
<p><span class="math display">\[
    V^\pi(s) = \mathbb{E}_\pi \left[ G_t \mid S_t = s \right]
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(G_t\)</span> is the <strong>return</strong> from time <span class="math inline">\(t\)</span>, typically defined as the sum of discounted rewards: <span class="math display">\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\]</span></li>
<li><span class="math inline">\(S_t\)</span> is the state at time <span class="math inline">\(t\)</span>, and <span class="math inline">\(R_{t+1}\)</span> is the reward received after transitioning from state <span class="math inline">\(S_t\)</span> to <span class="math inline">\(S_{t+1}\)</span>.</li>
<li><span class="math inline">\(\gamma\)</span> is the <strong>discount factor</strong> that determines the present value of future rewards, where <span class="math inline">\(0 \leq \gamma \leq 1\)</span>.</li>
</ul>
<p>The goal is to calculate <span class="math inline">\(V^\pi(s)\)</span>, which gives the expected return if the agent starts in state <span class="math inline">\(s\)</span> and follows policy <span class="math inline">\(\pi\)</span>.</p>
</section>
<section id="action-value-function-q_pis-a" class="level5">
<h5 class="anchored" data-anchor-id="action-value-function-q_pis-a">Action Value Function <span class="math inline">\(q_{\pi}(s, a)\)</span></h5>
<p>The <strong>action value function</strong> <span class="math inline">\(q_{\pi}(s, a)\)</span> gives the expected return for taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> and then following the policy <span class="math inline">\(\pi\)</span>. It is defined as:</p>
<p><span class="math display">\[
q_{\pi}(s, a) = \mathbb{E}_\pi \left[ G_t \mid S_t = s, A_t = a \right]
\]</span> This function tells us how good it is to take a particular action <span class="math inline">\(a\)</span> in a state <span class="math inline">\(s\)</span>, assuming we follow policy <span class="math inline">\(\pi\)</span> afterwards.</p>
<p>This estimates the value of taking a specific action in a given state, which represents the expected return from that state-action pair, following a particular policy from that point onwards.</p>
</section>
</section>
<section id="expected-return" class="level4">
<h4 class="anchored" data-anchor-id="expected-return">2.Expected Return:</h4>
<ul>
<li>The expected return is the total amount of reward an agent can anticipate from a particular point in time, considering both immediate and future rewards, usually discounted by a factor <span class="math inline">\(\gamma\)</span> (the discount factor). This discount factor weights the importance of future rewards relative to immediate rewards.</li>
</ul>
</section>
<section id="policies" class="level4">
<h4 class="anchored" data-anchor-id="policies">3. <strong>Policies</strong>:</h4>
<ul>
<li>A policy <span class="math inline">\(\pi\)</span> is the strategy or decision-making rule that defines the actions an agent will take in any given state. The value functions are always associated with a particular policy, meaning the future rewards depend on the actions dictated by the policy.</li>
</ul>
<p>Value functions are thus dependent on the agent’s <strong>policy</strong>, which determines the sequence of actions that the agent will take as it interacts with the environment. Policies can be deterministic (where each state leads to a fixed action) or stochastic (where actions are selected according to a probability distribution).</p>
</section>
<section id="bellman-equations-for-vpis-and-q_pis-a" class="level4">
<h4 class="anchored" data-anchor-id="bellman-equations-for-vpis-and-q_pis-a">4. <strong>Bellman Equations for <span class="math inline">\(V^\pi(s)\)</span> and <span class="math inline">\(q_{\pi}(s, a)\)</span></strong>:</h4>
<p>To compute the value functions, we use the <strong>Bellman equation</strong>, which expresses the value of a state or state-action pair in terms of the immediate reward plus the discounted value of the next state.</p>
<p>For the <strong>state value function</strong>, the Bellman equation is:</p>
<p><span class="math display">\[
V^\pi(s) = \sum_{a} \pi(a \mid s) \sum_{s'} P(s' \mid s, a) \left[ R(s, a, s') + \gamma V^\pi(s') \right]
\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\pi(a \mid s)\)</span> is the probability of taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> under policy <span class="math inline">\(\pi\)</span>,</li>
<li><span class="math inline">\(P(s' \mid s, a)\)</span> is the probability of transitioning to state <span class="math inline">\(s'\)</span> after taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span>,</li>
<li><span class="math inline">\(R(s, a, s')\)</span> is the reward received when transitioning from <span class="math inline">\(s\)</span> to <span class="math inline">\(s'\)</span> after taking action <span class="math inline">\(a\)</span>.</li>
</ul>
<p>For the <strong>action value function</strong>, the Bellman equation is:</p>
<p><span class="math display">\[
q_{\pi}(s, a) = \sum_{s'} P(s' \mid s, a) \left[ R(s, a, s') + \gamma \sum_{a'} \pi(a' \mid s') Q^\pi(s', a') \right]
\]</span> These equations give a recursive way of expressing value functions, which are central to many RL algorithms.</p>
</section>
</section>
<section id="q-learning-off-policy-learning-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="q-learning-off-policy-learning-algorithm"><span class="math inline">\(Q\)</span>-<strong>Learning</strong> (Off-policy learning algorithm)</h3>
<p><strong>Q-learning</strong> is a <strong>model-free</strong>, <strong>off-policy</strong> algorithm that seeks to find the optimal action-value function <span class="math inline">\(Q^*(s, a)\)</span>, which represents the maximum expected return for taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> and following the optimal policy from that point onward.</p>
<p>The Q-learning update rule is:</p>
<p><span class="math display">\[
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left[ R_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \right]
\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\alpha\)</span> is the <strong>learning rate</strong>,</li>
<li><span class="math inline">\(R_{t+1}\)</span> is the immediate reward,</li>
<li><span class="math inline">\(\gamma\)</span> is the discount factor,</li>
<li><span class="math inline">\(\max_{a'} Q(s_{t+1}, a')\)</span> is the maximum estimated value of the next state <span class="math inline">\(s_{t+1}\)</span> over all possible actions <span class="math inline">\(a'\)</span>.</li>
</ul>
<p>The key feature of Q-learning is that it is <strong>off-policy</strong>, meaning that the learning happens regardless of the policy the agent is currently following. The agent can learn the optimal policy while exploring the environment using a different policy.</p>
</section>
<section id="policy-iteration-on-policy-learning-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="policy-iteration-on-policy-learning-algorithm"><strong>Policy Iteration</strong> (On-policy learning algorithm)</h3>
<p><strong>Policy iteration</strong> is a classic <strong>on-policy</strong> algorithm that alternates between policy evaluation and policy improvement until the optimal policy is found.</p>
<ul>
<li><p><strong>Policy Evaluation</strong>: Given a policy <span class="math inline">\(\pi\)</span>, calculate the value function <span class="math inline">\(V^\pi(s)\)</span> for all states using the Bellman equation for the state-value function.</p>
<p>This involves solving the system of equations for <span class="math inline">\(V^\pi(s)\)</span> either by iterating until convergence (infinite horizon) or using a matrix form if the state space is small.</p></li>
<li><p><strong>Policy Improvement</strong>: Using the current value function <span class="math inline">\(V^\pi(s)\)</span>, improve the policy by choosing actions that maximize the expected return:</p></li>
</ul>
<p><span class="math display">\[
\pi'(s) = \arg\max_a \sum_{s'} P(s' \mid s, a) \left[ R(s, a, s') + \gamma V^\pi(s') \right]
\]</span></p>
<p>These two steps repeat: after improving the policy, we re-evaluate the value function and then improve the policy again. The algorithm converges when the policy no longer changes, indicating that the optimal policy <span class="math inline">\(\pi^*\)</span> has been found.</p>
</section>
<section id="value-iteration" class="level3">
<h3 class="anchored" data-anchor-id="value-iteration"><strong>Value Iteration</strong></h3>
<p><strong>Value iteration</strong> combines policy evaluation and policy improvement into a single step. Instead of fully evaluating the current policy, value iteration updates the value function directly using the Bellman optimality equation:</p>
<p><span class="math display">\[
V(s) \leftarrow \max_a \sum_{s'} P(s' \mid s, a) \left[ R(s, a, s') + \gamma V(s') \right]
\]</span></p>
<p>Once the value function has converged, the optimal policy <span class="math inline">\(\pi^*\)</span> is derived by choosing actions that maximize the value function.</p>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary:</h3>
<p>In summary, value functions <span class="math inline">\(V^\pi(s)\)</span> and <span class="math inline">\(q_{\pi}(s, a)\)</span> estimate future rewards based on an agent’s policy. These functions are fundamental to algorithms like <strong>Q-learning</strong> (off-policy) and <strong>policy iteration</strong> (on-policy). Q-learning directly updates the Q-values, aiming to discover the optimal action-value function, while policy iteration alternates between evaluating a policy and improving it.</p>
</section>
<section id="pick-and-drop-example" class="level2">
<h2 class="anchored" data-anchor-id="pick-and-drop-example">Pick and drop example</h2>
<div id="fig-pick_and_drop_exm" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-env="figure*">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pick_and_drop_exm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../assets/ch_03/drop_game.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" data-glightbox="description: .lightbox-desc-3" title="Pick and Drop game"><img src="../assets/ch_03/drop_game.png" class="img-fluid figure-img"></a></p>
<figcaption>Pick and Drop game</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pick_and_drop_exm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.1: Pick and Drop game. See python implementation below.
</figcaption>
</figure>
</div>
<div id="fig-pick_and_drop_rules" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-env="figure*">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pick_and_drop_rules-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../assets/ch_03/rules_drop_game.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" data-glightbox="description: .lightbox-desc-4" title="Rewards rules for the drop game"><img src="../assets/ch_03/rules_drop_game.png" class="img-fluid figure-img"></a></p>
<figcaption>Rewards rules for the drop game</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pick_and_drop_rules-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.2: Rewards rules for the drop game. See the above scheme for reference.
</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
We use the following class to simulate the pick and drop game accordingly the
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>above figure and rules.</p>
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>pick_and_drop_game.py</strong></pre>
</div>
<div class="sourceCode" id="cb1" data-filename="pick_and_drop_game.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">class</span> Field:</span>
<span id="cb1-2"><a href="#cb1-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size, item_pickup, item_dropout, start_position):</span>
<span id="cb1-3"><a href="#cb1-3"></a>        <span class="va">self</span>.size <span class="op">=</span> size</span>
<span id="cb1-4"><a href="#cb1-4"></a>        <span class="va">self</span>.item_pickup <span class="op">=</span> item_pickup</span>
<span id="cb1-5"><a href="#cb1-5"></a>        <span class="va">self</span>.item_dropout <span class="op">=</span> item_dropout</span>
<span id="cb1-6"><a href="#cb1-6"></a>        <span class="va">self</span>.position <span class="op">=</span> start_position</span>
<span id="cb1-7"><a href="#cb1-7"></a>        <span class="va">self</span>.item_in_car <span class="op">=</span> <span class="va">False</span></span>
<span id="cb1-8"><a href="#cb1-8"></a>    </span>
<span id="cb1-9"><a href="#cb1-9"></a>    <span class="kw">def</span> get_number_of_states(<span class="va">self</span>):</span>
<span id="cb1-10"><a href="#cb1-10"></a>        <span class="cf">return</span> <span class="va">self</span>.size <span class="op">*</span> <span class="va">self</span>.size <span class="op">*</span> <span class="va">self</span>.size <span class="op">*</span> <span class="va">self</span>.size <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb1-11"><a href="#cb1-11"></a>    </span>
<span id="cb1-12"><a href="#cb1-12"></a>    <span class="kw">def</span> get_state(<span class="va">self</span>):</span>
<span id="cb1-13"><a href="#cb1-13"></a>        state <span class="op">=</span> <span class="va">self</span>.position[<span class="dv">0</span>] <span class="op">*</span> <span class="va">self</span>.size <span class="op">*</span> <span class="va">self</span>.size <span class="op">*</span> <span class="va">self</span>.size <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb1-14"><a href="#cb1-14"></a>        state <span class="op">=</span> state <span class="op">+</span> <span class="va">self</span>.position[<span class="dv">1</span>] <span class="op">*</span> <span class="va">self</span>.size <span class="op">*</span> <span class="va">self</span>.size <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb1-15"><a href="#cb1-15"></a>        state <span class="op">=</span> state <span class="op">+</span> <span class="va">self</span>.item_pickup[<span class="dv">0</span>] <span class="op">*</span> <span class="va">self</span>.size <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb1-16"><a href="#cb1-16"></a>        state <span class="op">=</span> state <span class="op">+</span> <span class="va">self</span>.item_pickup[<span class="dv">1</span>] <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb1-17"><a href="#cb1-17"></a>        </span>
<span id="cb1-18"><a href="#cb1-18"></a>        <span class="cf">if</span> <span class="va">self</span>.item_in_car:</span>
<span id="cb1-19"><a href="#cb1-19"></a>            state <span class="op">=</span> state <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb1-20"><a href="#cb1-20"></a>        <span class="cf">return</span> state</span>
<span id="cb1-21"><a href="#cb1-21"></a>    </span>
<span id="cb1-22"><a href="#cb1-22"></a>    <span class="kw">def</span> make_action(<span class="va">self</span>, action):</span>
<span id="cb1-23"><a href="#cb1-23"></a>        (x, y) <span class="op">=</span> <span class="va">self</span>.position</span>
<span id="cb1-24"><a href="#cb1-24"></a>        <span class="cf">if</span> action <span class="op">==</span> <span class="dv">0</span>:  <span class="co"># down</span></span>
<span id="cb1-25"><a href="#cb1-25"></a>            <span class="cf">if</span> y <span class="op">==</span> <span class="va">self</span>.size <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb1-26"><a href="#cb1-26"></a>                <span class="cf">return</span> <span class="op">-</span><span class="dv">10</span>, <span class="va">False</span></span>
<span id="cb1-27"><a href="#cb1-27"></a>            <span class="cf">else</span>:</span>
<span id="cb1-28"><a href="#cb1-28"></a>                <span class="va">self</span>.position <span class="op">=</span> (x, y <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-29"><a href="#cb1-29"></a>                <span class="cf">return</span> <span class="op">-</span><span class="dv">1</span>, <span class="va">False</span></span>
<span id="cb1-30"><a href="#cb1-30"></a>        </span>
<span id="cb1-31"><a href="#cb1-31"></a>        <span class="cf">elif</span> action <span class="op">==</span> <span class="dv">1</span>:  <span class="co"># up</span></span>
<span id="cb1-32"><a href="#cb1-32"></a>            <span class="cf">if</span> y <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb1-33"><a href="#cb1-33"></a>                <span class="cf">return</span> <span class="op">-</span><span class="dv">10</span>, <span class="va">False</span></span>
<span id="cb1-34"><a href="#cb1-34"></a>            <span class="cf">else</span>:</span>
<span id="cb1-35"><a href="#cb1-35"></a>                <span class="va">self</span>.position <span class="op">=</span> (x, y <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb1-36"><a href="#cb1-36"></a>                <span class="cf">return</span> <span class="op">-</span><span class="dv">1</span>, <span class="va">False</span></span>
<span id="cb1-37"><a href="#cb1-37"></a>        </span>
<span id="cb1-38"><a href="#cb1-38"></a>        <span class="cf">elif</span> action <span class="op">==</span> <span class="dv">2</span>:  <span class="co"># left</span></span>
<span id="cb1-39"><a href="#cb1-39"></a>            <span class="cf">if</span> x <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb1-40"><a href="#cb1-40"></a>                <span class="cf">return</span> <span class="op">-</span><span class="dv">10</span>, <span class="va">False</span></span>
<span id="cb1-41"><a href="#cb1-41"></a>            <span class="cf">else</span>:</span>
<span id="cb1-42"><a href="#cb1-42"></a>                <span class="va">self</span>.position <span class="op">=</span> (x <span class="op">-</span> <span class="dv">1</span>, y)</span>
<span id="cb1-43"><a href="#cb1-43"></a>                <span class="cf">return</span> <span class="op">-</span><span class="dv">1</span>, <span class="va">False</span></span>
<span id="cb1-44"><a href="#cb1-44"></a>        </span>
<span id="cb1-45"><a href="#cb1-45"></a>        <span class="cf">elif</span> action <span class="op">==</span> <span class="dv">3</span>:  <span class="co"># right</span></span>
<span id="cb1-46"><a href="#cb1-46"></a>            <span class="cf">if</span> x <span class="op">==</span> <span class="va">self</span>.size <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb1-47"><a href="#cb1-47"></a>                <span class="cf">return</span> <span class="op">-</span><span class="dv">10</span>, <span class="va">False</span></span>
<span id="cb1-48"><a href="#cb1-48"></a>            <span class="cf">else</span>:</span>
<span id="cb1-49"><a href="#cb1-49"></a>                <span class="va">self</span>.position <span class="op">=</span> (x <span class="op">+</span> <span class="dv">1</span>, y)</span>
<span id="cb1-50"><a href="#cb1-50"></a>                <span class="cf">return</span> <span class="op">-</span><span class="dv">1</span>, <span class="va">False</span></span>
<span id="cb1-51"><a href="#cb1-51"></a>        </span>
<span id="cb1-52"><a href="#cb1-52"></a>        <span class="cf">elif</span> action <span class="op">==</span> <span class="dv">4</span>:  <span class="co"># pickup</span></span>
<span id="cb1-53"><a href="#cb1-53"></a>            <span class="cf">if</span> <span class="va">self</span>.item_in_car:</span>
<span id="cb1-54"><a href="#cb1-54"></a>                <span class="cf">return</span> <span class="op">-</span><span class="dv">10</span>, <span class="va">False</span></span>
<span id="cb1-55"><a href="#cb1-55"></a>            <span class="cf">elif</span> <span class="va">self</span>.item_pickup <span class="op">!=</span> (x, y):</span>
<span id="cb1-56"><a href="#cb1-56"></a>                <span class="cf">return</span> <span class="op">-</span><span class="dv">10</span>, <span class="va">False</span></span>
<span id="cb1-57"><a href="#cb1-57"></a>            <span class="cf">else</span>:</span>
<span id="cb1-58"><a href="#cb1-58"></a>                <span class="va">self</span>.item_in_car <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-59"><a href="#cb1-59"></a>                <span class="cf">return</span> <span class="dv">20</span>, <span class="va">False</span></span>
<span id="cb1-60"><a href="#cb1-60"></a>        </span>
<span id="cb1-61"><a href="#cb1-61"></a>        <span class="cf">elif</span> action <span class="op">==</span> <span class="dv">5</span>:  <span class="co"># dropout</span></span>
<span id="cb1-62"><a href="#cb1-62"></a>            <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.item_in_car:</span>
<span id="cb1-63"><a href="#cb1-63"></a>                <span class="cf">return</span> <span class="op">-</span><span class="dv">10</span>, <span class="va">False</span></span>
<span id="cb1-64"><a href="#cb1-64"></a>            <span class="cf">elif</span> <span class="va">self</span>.item_dropout <span class="op">!=</span> (x, y):</span>
<span id="cb1-65"><a href="#cb1-65"></a>                <span class="va">self</span>.item_pickup <span class="op">=</span> (x, y)</span>
<span id="cb1-66"><a href="#cb1-66"></a>                <span class="va">self</span>.item_in_car <span class="op">=</span> <span class="va">False</span></span>
<span id="cb1-67"><a href="#cb1-67"></a>                <span class="cf">return</span> <span class="op">-</span><span class="dv">10</span>, <span class="va">False</span></span>
<span id="cb1-68"><a href="#cb1-68"></a>            <span class="cf">else</span>:</span>
<span id="cb1-69"><a href="#cb1-69"></a>                <span class="va">self</span>.item_in_car <span class="op">=</span> <span class="va">False</span></span>
<span id="cb1-70"><a href="#cb1-70"></a>                <span class="cf">return</span> <span class="dv">20</span>, <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
To illustrate how works this class
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>test_pick_and_drop_game.py</strong></pre>
</div>
<div class="sourceCode" id="cb2" data-filename="test_pick_and_drop_game.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">from</span> pick_and_drop_game <span class="im">import</span> Field</span>
<span id="cb2-2"><a href="#cb2-2"></a>size <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>item_pickup <span class="op">=</span> (<span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb2-4"><a href="#cb2-4"></a>item_dropout <span class="op">=</span> (<span class="dv">9</span>, <span class="dv">9</span>)</span>
<span id="cb2-5"><a href="#cb2-5"></a>start_position <span class="op">=</span> (<span class="dv">9</span>, <span class="dv">0</span>)</span>
<span id="cb2-6"><a href="#cb2-6"></a></span>
<span id="cb2-7"><a href="#cb2-7"></a></span>
<span id="cb2-8"><a href="#cb2-8"></a></span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">'__main__'</span>:</span>
<span id="cb2-10"><a href="#cb2-10"></a>    field <span class="op">=</span> Field(size, item_pickup, item_dropout, start_position)</span>
<span id="cb2-11"><a href="#cb2-11"></a>    <span class="bu">print</span>(field.position)</span>
<span id="cb2-12"><a href="#cb2-12"></a>    </span>
<span id="cb2-13"><a href="#cb2-13"></a><span class="co"># manual solution</span></span>
<span id="cb2-14"><a href="#cb2-14"></a>field.make_action(<span class="dv">2</span>)</span>
<span id="cb2-15"><a href="#cb2-15"></a>field.make_action(<span class="dv">2</span>)</span>
<span id="cb2-16"><a href="#cb2-16"></a>field.make_action(<span class="dv">2</span>)</span>
<span id="cb2-17"><a href="#cb2-17"></a>field.make_action(<span class="dv">2</span>)</span>
<span id="cb2-18"><a href="#cb2-18"></a>field.make_action(<span class="dv">2</span>)</span>
<span id="cb2-19"><a href="#cb2-19"></a>field.make_action(<span class="dv">2</span>)</span>
<span id="cb2-20"><a href="#cb2-20"></a>field.make_action(<span class="dv">2</span>)</span>
<span id="cb2-21"><a href="#cb2-21"></a>field.make_action(<span class="dv">2</span>)</span>
<span id="cb2-22"><a href="#cb2-22"></a>field.make_action(<span class="dv">2</span>)</span>
<span id="cb2-23"><a href="#cb2-23"></a><span class="co"># pick</span></span>
<span id="cb2-24"><a href="#cb2-24"></a>field.make_action(<span class="dv">4</span>)</span>
<span id="cb2-25"><a href="#cb2-25"></a></span>
<span id="cb2-26"><a href="#cb2-26"></a>field.make_action(<span class="dv">0</span>)</span>
<span id="cb2-27"><a href="#cb2-27"></a>field.make_action(<span class="dv">0</span>)</span>
<span id="cb2-28"><a href="#cb2-28"></a>field.make_action(<span class="dv">0</span>)</span>
<span id="cb2-29"><a href="#cb2-29"></a>field.make_action(<span class="dv">0</span>)</span>
<span id="cb2-30"><a href="#cb2-30"></a>field.make_action(<span class="dv">0</span>)</span>
<span id="cb2-31"><a href="#cb2-31"></a>field.make_action(<span class="dv">0</span>)</span>
<span id="cb2-32"><a href="#cb2-32"></a>field.make_action(<span class="dv">0</span>)</span>
<span id="cb2-33"><a href="#cb2-33"></a>field.make_action(<span class="dv">0</span>)</span>
<span id="cb2-34"><a href="#cb2-34"></a>field.make_action(<span class="dv">0</span>)</span>
<span id="cb2-35"><a href="#cb2-35"></a></span>
<span id="cb2-36"><a href="#cb2-36"></a>field.make_action(<span class="dv">3</span>)</span>
<span id="cb2-37"><a href="#cb2-37"></a>field.make_action(<span class="dv">3</span>)</span>
<span id="cb2-38"><a href="#cb2-38"></a>field.make_action(<span class="dv">3</span>)</span>
<span id="cb2-39"><a href="#cb2-39"></a>field.make_action(<span class="dv">3</span>)</span>
<span id="cb2-40"><a href="#cb2-40"></a>field.make_action(<span class="dv">3</span>)</span>
<span id="cb2-41"><a href="#cb2-41"></a>field.make_action(<span class="dv">3</span>)</span>
<span id="cb2-42"><a href="#cb2-42"></a>field.make_action(<span class="dv">3</span>)</span>
<span id="cb2-43"><a href="#cb2-43"></a>field.make_action(<span class="dv">3</span>)</span>
<span id="cb2-44"><a href="#cb2-44"></a>field.make_action(<span class="dv">3</span>)</span>
<span id="cb2-45"><a href="#cb2-45"></a></span>
<span id="cb2-46"><a href="#cb2-46"></a>field.make_action(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Now we implement a random but naive solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>pick_and_drop_naive_random_solution.py</strong></pre>
</div>
<div class="sourceCode" id="cb3" data-filename="pick_and_drop_naive_random_solution.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot <span class="im">as</span> plt</span>
<span id="cb3-2"><a href="#cb3-2"></a></span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="im">from</span> pick_and_drop_game <span class="im">import</span> Field</span>
<span id="cb3-4"><a href="#cb3-4"></a><span class="im">import</span> random</span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-6"><a href="#cb3-6"></a></span>
<span id="cb3-7"><a href="#cb3-7"></a></span>
<span id="cb3-8"><a href="#cb3-8"></a><span class="kw">def</span> random_solution():</span>
<span id="cb3-9"><a href="#cb3-9"></a>    size <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb3-10"><a href="#cb3-10"></a>    item_pickup <span class="op">=</span> (<span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb3-11"><a href="#cb3-11"></a>    item_dropout <span class="op">=</span> (<span class="dv">9</span>, <span class="dv">9</span>)</span>
<span id="cb3-12"><a href="#cb3-12"></a>    start_position <span class="op">=</span> (<span class="dv">9</span>, <span class="dv">0</span>)</span>
<span id="cb3-13"><a href="#cb3-13"></a>    </span>
<span id="cb3-14"><a href="#cb3-14"></a>    field <span class="op">=</span> Field(size, item_pickup, item_dropout, start_position)</span>
<span id="cb3-15"><a href="#cb3-15"></a>    </span>
<span id="cb3-16"><a href="#cb3-16"></a>    done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb3-17"><a href="#cb3-17"></a>    steps <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-18"><a href="#cb3-18"></a>    </span>
<span id="cb3-19"><a href="#cb3-19"></a>    <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb3-20"><a href="#cb3-20"></a>        action <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="dv">5</span>)</span>
<span id="cb3-21"><a href="#cb3-21"></a>        reward, done <span class="op">=</span> field.make_action(action)</span>
<span id="cb3-22"><a href="#cb3-22"></a>        steps <span class="op">=</span> steps <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb3-23"><a href="#cb3-23"></a>    </span>
<span id="cb3-24"><a href="#cb3-24"></a>    <span class="cf">return</span> steps</span>
<span id="cb3-25"><a href="#cb3-25"></a></span>
<span id="cb3-26"><a href="#cb3-26"></a></span>
<span id="cb3-27"><a href="#cb3-27"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">'__main__'</span>:</span>
<span id="cb3-28"><a href="#cb3-28"></a>    steps <span class="op">=</span> random_solution()</span>
<span id="cb3-29"><a href="#cb3-29"></a>    <span class="bu">print</span>(steps)</span>
<span id="cb3-30"><a href="#cb3-30"></a>    sampling_size <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb3-31"><a href="#cb3-31"></a>    sample <span class="op">=</span> [random_solution() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(sampling_size)]</span>
<span id="cb3-32"><a href="#cb3-32"></a>    sample <span class="op">=</span> np.array(sample)</span>
<span id="cb3-33"><a href="#cb3-33"></a>    no_steps_mean <span class="op">=</span> sample.mean()</span>
<span id="cb3-34"><a href="#cb3-34"></a>    <span class="bu">print</span>(<span class="st">'Mean of # steps for reach goal </span><span class="sc">{:n}</span><span class="st">'</span>.<span class="bu">format</span>(no_steps_mean))</span>
<span id="cb3-35"><a href="#cb3-35"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Next we apply the <span class="math inline">\(Q-\)</span>learning algorithm for impove the above solution.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>pick_and_drop_q_learning_solution.py</strong></pre>
</div>
<div class="sourceCode" id="cb4" data-filename="pick_and_drop_q_learning_solution.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>    <span class="kw">def</span> q_learning_solution():</span>
<span id="cb4-2"><a href="#cb4-2"></a>    epsilon <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb4-3"><a href="#cb4-3"></a>    alpha <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb4-4"><a href="#cb4-4"></a>    gamma <span class="op">=</span> <span class="fl">0.6</span></span>
<span id="cb4-5"><a href="#cb4-5"></a>    </span>
<span id="cb4-6"><a href="#cb4-6"></a>    field <span class="op">=</span> Field(size, item_pickup, item_drop_out, start_position)</span>
<span id="cb4-7"><a href="#cb4-7"></a>    done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb4-8"><a href="#cb4-8"></a>    steps <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-9"><a href="#cb4-9"></a>    </span>
<span id="cb4-10"><a href="#cb4-10"></a>    <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb4-11"><a href="#cb4-11"></a>        state <span class="op">=</span> field.get_state()</span>
<span id="cb4-12"><a href="#cb4-12"></a>        <span class="cf">if</span> random.uniform(<span class="dv">0</span>, <span class="dv">1</span>) <span class="op">&lt;</span> epsilon:</span>
<span id="cb4-13"><a href="#cb4-13"></a>            action <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="dv">5</span>)  <span class="co"># Explore</span></span>
<span id="cb4-14"><a href="#cb4-14"></a>        <span class="cf">else</span>:</span>
<span id="cb4-15"><a href="#cb4-15"></a>            action <span class="op">=</span> np.argmax(q_table[state])  <span class="co"># Exploit</span></span>
<span id="cb4-16"><a href="#cb4-16"></a>        </span>
<span id="cb4-17"><a href="#cb4-17"></a>        reward, done <span class="op">=</span> field.make_action(action)</span>
<span id="cb4-18"><a href="#cb4-18"></a>        </span>
<span id="cb4-19"><a href="#cb4-19"></a>        new_state <span class="op">=</span> field.get_state()</span>
<span id="cb4-20"><a href="#cb4-20"></a>        new_state_max <span class="op">=</span> np.<span class="bu">max</span>(q_table[new_state])</span>
<span id="cb4-21"><a href="#cb4-21"></a>        </span>
<span id="cb4-22"><a href="#cb4-22"></a>        q_table[state, action] <span class="op">=</span> <span class="op">\</span></span>
<span id="cb4-23"><a href="#cb4-23"></a>            (<span class="dv">1</span> <span class="op">-</span> alpha) <span class="op">*</span> q_table[state, action] <span class="op">\</span></span>
<span id="cb4-24"><a href="#cb4-24"></a>            <span class="op">+</span> alpha <span class="op">*</span> (</span>
<span id="cb4-25"><a href="#cb4-25"></a>                    reward <span class="op">+</span> gamma <span class="op">*</span> new_state_max</span>
<span id="cb4-26"><a href="#cb4-26"></a>                    <span class="op">-</span> q_table[state, action]</span>
<span id="cb4-27"><a href="#cb4-27"></a>            )</span>
<span id="cb4-28"><a href="#cb4-28"></a>        steps <span class="op">=</span> steps <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb4-29"><a href="#cb4-29"></a>    <span class="cf">return</span> steps</span>
<span id="cb4-30"><a href="#cb4-30"></a></span>
<span id="cb4-31"><a href="#cb4-31"></a></span>
<span id="cb4-32"><a href="#cb4-32"></a>size <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb4-33"><a href="#cb4-33"></a>item_pickup <span class="op">=</span> (<span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb4-34"><a href="#cb4-34"></a>item_drop_out <span class="op">=</span> (<span class="dv">9</span>, <span class="dv">9</span>)</span>
<span id="cb4-35"><a href="#cb4-35"></a>start_position <span class="op">=</span> (<span class="dv">9</span>, <span class="dv">0</span>)</span>
<span id="cb4-36"><a href="#cb4-36"></a></span>
<span id="cb4-37"><a href="#cb4-37"></a>field <span class="op">=</span> Field(size, item_pickup, item_drop_out, start_position)</span>
<span id="cb4-38"><a href="#cb4-38"></a></span>
<span id="cb4-39"><a href="#cb4-39"></a>number_of_states <span class="op">=</span> field.get_number_of_states()</span>
<span id="cb4-40"><a href="#cb4-40"></a>number_of_actions <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb4-41"><a href="#cb4-41"></a>q_table <span class="op">=</span> np.zeros((number_of_states, number_of_actions))</span>
<span id="cb4-42"><a href="#cb4-42"></a>epsilon <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb4-43"><a href="#cb4-43"></a>alpha <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb4-44"><a href="#cb4-44"></a>gamma <span class="op">=</span> <span class="fl">0.6</span></span>
<span id="cb4-45"><a href="#cb4-45"></a>n_training <span class="op">=</span> <span class="dv">100000</span></span>
<span id="cb4-46"><a href="#cb4-46"></a><span class="co"># Training phase</span></span>
<span id="cb4-47"><a href="#cb4-47"></a></span>
<span id="cb4-48"><a href="#cb4-48"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_training):</span>
<span id="cb4-49"><a href="#cb4-49"></a>    field <span class="op">=</span> Field(size, item_pickup, item_drop_out, start_position)</span>
<span id="cb4-50"><a href="#cb4-50"></a>    done <span class="op">=</span> <span class="va">False</span></span>
<span id="cb4-51"><a href="#cb4-51"></a>    <span class="cf">while</span> <span class="kw">not</span> done:</span>
<span id="cb4-52"><a href="#cb4-52"></a>        state <span class="op">=</span> field.get_state()</span>
<span id="cb4-53"><a href="#cb4-53"></a>        <span class="cf">if</span> random.uniform(<span class="dv">0</span>, <span class="dv">1</span>) <span class="op">&lt;</span> epsilon:</span>
<span id="cb4-54"><a href="#cb4-54"></a>            action <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="dv">5</span>)  <span class="co"># Explore</span></span>
<span id="cb4-55"><a href="#cb4-55"></a>        <span class="cf">else</span>:</span>
<span id="cb4-56"><a href="#cb4-56"></a>            action <span class="op">=</span> np.argmax(q_table[state])  <span class="co"># Exploit</span></span>
<span id="cb4-57"><a href="#cb4-57"></a>        reward, done <span class="op">=</span> field.make_action(action)</span>
<span id="cb4-58"><a href="#cb4-58"></a>        new_state <span class="op">=</span> field.get_state()</span>
<span id="cb4-59"><a href="#cb4-59"></a>        new_state_max <span class="op">=</span> np.<span class="bu">max</span>(q_table[new_state])</span>
<span id="cb4-60"><a href="#cb4-60"></a>        <span class="co"># q_learning iteration as ascendant grad</span></span>
<span id="cb4-61"><a href="#cb4-61"></a>        q_table[state, action] <span class="op">=</span> <span class="op">\</span></span>
<span id="cb4-62"><a href="#cb4-62"></a>            (<span class="fl">1.0</span> <span class="op">-</span> alpha) <span class="op">*</span> q_table[state, action] <span class="op">\</span></span>
<span id="cb4-63"><a href="#cb4-63"></a>            <span class="op">+</span> alpha <span class="op">*</span> (</span>
<span id="cb4-64"><a href="#cb4-64"></a>                reward <span class="op">+</span> gamma <span class="op">*</span> new_state_max <span class="op">-</span> q_table[state, action]</span>
<span id="cb4-65"><a href="#cb4-65"></a>            )</span>
<span id="cb4-66"><a href="#cb4-66"></a></span>
<span id="cb4-67"><a href="#cb4-67"></a>q_learning_sampling <span class="op">=</span> [q_learning_solution() <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10000</span>)]</span>
<span id="cb4-68"><a href="#cb4-68"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb4-69"><a href="#cb4-69"></a>ax.hist(q_learning_sampling, bins<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb4-70"><a href="#cb4-70"></a>ax.set_title(<span class="st">'distribution of the No. of steps with the Q-Learning sol'</span>)</span>
<span id="cb4-71"><a href="#cb4-71"></a>ax.set_xlabel(<span class="st">'No. of steps'</span>)</span>
<span id="cb4-72"><a href="#cb4-72"></a>ax.set_ylabel(<span class="st">'Count'</span>)</span>
<span id="cb4-73"><a href="#cb4-73"></a>fig.savefig(<span class="st">"histogram_q_learning_solution.png"</span>)</span>
<span id="cb4-74"><a href="#cb4-74"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<div id="exm-grid_world" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.1 (Gridworld)</strong></span> &nbsp;</p>
<ul>
<li><p>Actions: <code>north</code>, <code>south</code>, <code>east</code>, <code>west</code></p></li>
<li><p>Rewards:</p>
<ul>
<li>Actions would take the agent off the grid leave its location unchanged, but also results in a reward of-1;</li>
<li>Other actions result in a reward of 0, except those that in states A and B;</li>
<li>From state A , all four actions yield a reward of +10 and take the agent toA’ ;</li>
<li>From state B , all four actions yield a reward of +5 and take the agentto B’ ;</li>
<li>The learning rate (<span class="math inline">\(\gamma\)</span>) for this example is 0.9</li>
</ul></li>
</ul>
<div id="fig-grid-world-exm" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-env="figure*">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-grid-world-exm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../assets/ch_04/grid_world_exm.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" data-glightbox="description: .lightbox-desc-5" title="Grid World example. Problem scheme , possible actions and sampling reward"><img src="../assets/ch_04/grid_world_exm.png" class="img-fluid figure-img"></a></p>
<figcaption>Grid World example. Problem scheme , possible actions and sampling reward</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-grid-world-exm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.3: Grid World example. Problem scheme , possible actions and sampling reward
</figcaption>
</figure>
</div>
<div class="{exm-grid_world}">
<p>The Bellman equation must hold for each state for the value function <span class="math inline">\(v_{\pi}\)</span> shown in <a href="#fig-grid-world-exm" class="quarto-xref">Figure&nbsp;<span class="quarto-unresolved-ref">fig-grid-world-exm</span></a> (right). Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, 0.4, and +0.7. (These numbers are accurate only to one decimal place.)</p>
</div>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
python implementation for the grid-world example
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="code-with-filename">
<div class="code-with-filename-file">
<pre><strong>gridworld.py</strong></pre>
</div>
<div class="sourceCode" id="cb5" data-filename="gridworld.py"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="co">#| lst-label: lst-import</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="co">#| lst-cap: Import pyplot</span></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="im">import</span> matplotlib</span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-6"><a href="#cb5-6"></a><span class="im">from</span> matplotlib.table <span class="im">import</span> Table</span>
<span id="cb5-7"><a href="#cb5-7"></a>matplotlib.use(<span class="st">'Agg'</span>)</span>
<span id="cb5-8"><a href="#cb5-8"></a></span>
<span id="cb5-9"><a href="#cb5-9"></a>WORLD_SIZE <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb5-10"><a href="#cb5-10"></a>A_POS <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb5-11"><a href="#cb5-11"></a>A_PRIME_POS <span class="op">=</span> [<span class="dv">4</span>, <span class="dv">1</span>]</span>
<span id="cb5-12"><a href="#cb5-12"></a>B_POS <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">3</span>]</span>
<span id="cb5-13"><a href="#cb5-13"></a>B_PRIME_POS <span class="op">=</span> [<span class="dv">2</span>, <span class="dv">3</span>]</span>
<span id="cb5-14"><a href="#cb5-14"></a>DISCOUNT <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb5-15"><a href="#cb5-15"></a></span>
<span id="cb5-16"><a href="#cb5-16"></a><span class="co"># left, up, right, down</span></span>
<span id="cb5-17"><a href="#cb5-17"></a>ACTIONS <span class="op">=</span> [np.array([<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>]),</span>
<span id="cb5-18"><a href="#cb5-18"></a>           np.array([<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>]),</span>
<span id="cb5-19"><a href="#cb5-19"></a>           np.array([<span class="dv">0</span>, <span class="dv">1</span>]),</span>
<span id="cb5-20"><a href="#cb5-20"></a>           np.array([<span class="dv">1</span>, <span class="dv">0</span>])]</span>
<span id="cb5-21"><a href="#cb5-21"></a>ACTIONS_FIGS <span class="op">=</span> [<span class="st">'←'</span>, <span class="st">'↑'</span>, <span class="st">'→'</span>, <span class="st">'↓'</span>]</span>
<span id="cb5-22"><a href="#cb5-22"></a>ACTION_PROB <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb5-23"><a href="#cb5-23"></a></span>
<span id="cb5-24"><a href="#cb5-24"></a><span class="kw">def</span> step(state, action):</span>
<span id="cb5-25"><a href="#cb5-25"></a>    <span class="cf">if</span> state <span class="op">==</span> A_POS:</span>
<span id="cb5-26"><a href="#cb5-26"></a>        <span class="cf">return</span> A_PRIME_POS, <span class="dv">10</span></span>
<span id="cb5-27"><a href="#cb5-27"></a>    <span class="cf">if</span> state <span class="op">==</span> B_POS:</span>
<span id="cb5-28"><a href="#cb5-28"></a>        <span class="cf">return</span> B_PRIME_POS, <span class="dv">5</span></span>
<span id="cb5-29"><a href="#cb5-29"></a></span>
<span id="cb5-30"><a href="#cb5-30"></a>    next_state <span class="op">=</span> (np.array(state) <span class="op">+</span> action).tolist()</span>
<span id="cb5-31"><a href="#cb5-31"></a>    x, y <span class="op">=</span> next_state</span>
<span id="cb5-32"><a href="#cb5-32"></a>    <span class="cf">if</span> x <span class="op">&lt;</span> <span class="dv">0</span> <span class="kw">or</span> x <span class="op">&gt;=</span> WORLD_SIZE <span class="kw">or</span> y <span class="op">&lt;</span> <span class="dv">0</span> <span class="kw">or</span> y <span class="op">&gt;=</span> WORLD_SIZE:</span>
<span id="cb5-33"><a href="#cb5-33"></a>        reward <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span></span>
<span id="cb5-34"><a href="#cb5-34"></a>        next_state <span class="op">=</span> state</span>
<span id="cb5-35"><a href="#cb5-35"></a>    <span class="cf">else</span>:</span>
<span id="cb5-36"><a href="#cb5-36"></a>        reward <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-37"><a href="#cb5-37"></a>    <span class="cf">return</span> next_state, reward</span>
<span id="cb5-38"><a href="#cb5-38"></a></span>
<span id="cb5-39"><a href="#cb5-39"></a></span>
<span id="cb5-40"><a href="#cb5-40"></a><span class="kw">def</span> draw_image(image):</span>
<span id="cb5-41"><a href="#cb5-41"></a>    fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb5-42"><a href="#cb5-42"></a>    ax.set_axis_off()</span>
<span id="cb5-43"><a href="#cb5-43"></a>    tb <span class="op">=</span> Table(ax, bbox<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb5-44"><a href="#cb5-44"></a></span>
<span id="cb5-45"><a href="#cb5-45"></a>    nrows, ncols <span class="op">=</span> image.shape</span>
<span id="cb5-46"><a href="#cb5-46"></a>    width, height <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> ncols, <span class="fl">1.0</span> <span class="op">/</span> nrows</span>
<span id="cb5-47"><a href="#cb5-47"></a></span>
<span id="cb5-48"><a href="#cb5-48"></a>    <span class="co"># Add cells</span></span>
<span id="cb5-49"><a href="#cb5-49"></a>    <span class="cf">for</span> (i, j), val <span class="kw">in</span> np.ndenumerate(image):</span>
<span id="cb5-50"><a href="#cb5-50"></a></span>
<span id="cb5-51"><a href="#cb5-51"></a>        <span class="co"># add state labels</span></span>
<span id="cb5-52"><a href="#cb5-52"></a>        <span class="cf">if</span> [i, j] <span class="op">==</span> A_POS:</span>
<span id="cb5-53"><a href="#cb5-53"></a>            val <span class="op">=</span> <span class="bu">str</span>(val) <span class="op">+</span> <span class="st">" (A)"</span></span>
<span id="cb5-54"><a href="#cb5-54"></a>        <span class="cf">if</span> [i, j] <span class="op">==</span> A_PRIME_POS:</span>
<span id="cb5-55"><a href="#cb5-55"></a>            val <span class="op">=</span> <span class="bu">str</span>(val) <span class="op">+</span> <span class="st">" (A')"</span></span>
<span id="cb5-56"><a href="#cb5-56"></a>        <span class="cf">if</span> [i, j] <span class="op">==</span> B_POS:</span>
<span id="cb5-57"><a href="#cb5-57"></a>            val <span class="op">=</span> <span class="bu">str</span>(val) <span class="op">+</span> <span class="st">" (B)"</span></span>
<span id="cb5-58"><a href="#cb5-58"></a>        <span class="cf">if</span> [i, j] <span class="op">==</span> B_PRIME_POS:</span>
<span id="cb5-59"><a href="#cb5-59"></a>            val <span class="op">=</span> <span class="bu">str</span>(val) <span class="op">+</span> <span class="st">" (B')"</span></span>
<span id="cb5-60"><a href="#cb5-60"></a>        </span>
<span id="cb5-61"><a href="#cb5-61"></a>        tb.add_cell(i, j, width, height, text<span class="op">=</span>val,</span>
<span id="cb5-62"><a href="#cb5-62"></a>                    loc<span class="op">=</span><span class="st">'center'</span>, facecolor<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb5-63"><a href="#cb5-63"></a>        </span>
<span id="cb5-64"><a href="#cb5-64"></a>    <span class="co"># Row and column labels...</span></span>
<span id="cb5-65"><a href="#cb5-65"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(image)):</span>
<span id="cb5-66"><a href="#cb5-66"></a>        tb.add_cell(i, <span class="op">-</span><span class="dv">1</span>, width, height, text<span class="op">=</span>i<span class="op">+</span><span class="dv">1</span>, loc<span class="op">=</span><span class="st">'right'</span>,</span>
<span id="cb5-67"><a href="#cb5-67"></a>                    edgecolor<span class="op">=</span><span class="st">'none'</span>, facecolor<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb5-68"><a href="#cb5-68"></a>        tb.add_cell(<span class="op">-</span><span class="dv">1</span>, i, width, height<span class="op">/</span><span class="dv">2</span>, text<span class="op">=</span>i<span class="op">+</span><span class="dv">1</span>, loc<span class="op">=</span><span class="st">'center'</span>,</span>
<span id="cb5-69"><a href="#cb5-69"></a>                    edgecolor<span class="op">=</span><span class="st">'none'</span>, facecolor<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb5-70"><a href="#cb5-70"></a></span>
<span id="cb5-71"><a href="#cb5-71"></a>    ax.add_table(tb)</span>
<span id="cb5-72"><a href="#cb5-72"></a></span>
<span id="cb5-73"><a href="#cb5-73"></a><span class="kw">def</span> draw_policy(optimal_values):</span>
<span id="cb5-74"><a href="#cb5-74"></a>    fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb5-75"><a href="#cb5-75"></a>    ax.set_axis_off()</span>
<span id="cb5-76"><a href="#cb5-76"></a>    tb <span class="op">=</span> Table(ax, bbox<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb5-77"><a href="#cb5-77"></a></span>
<span id="cb5-78"><a href="#cb5-78"></a>    nrows, ncols <span class="op">=</span> optimal_values.shape</span>
<span id="cb5-79"><a href="#cb5-79"></a>    width, height <span class="op">=</span> <span class="fl">1.0</span> <span class="op">/</span> ncols, <span class="fl">1.0</span> <span class="op">/</span> nrows</span>
<span id="cb5-80"><a href="#cb5-80"></a></span>
<span id="cb5-81"><a href="#cb5-81"></a>    <span class="co"># Add cells</span></span>
<span id="cb5-82"><a href="#cb5-82"></a>    <span class="cf">for</span> (i, j), val <span class="kw">in</span> np.ndenumerate(optimal_values):</span>
<span id="cb5-83"><a href="#cb5-83"></a>        next_vals<span class="op">=</span>[]</span>
<span id="cb5-84"><a href="#cb5-84"></a>        <span class="cf">for</span> action <span class="kw">in</span> ACTIONS:</span>
<span id="cb5-85"><a href="#cb5-85"></a>            next_state, _ <span class="op">=</span> step([i, j], action)</span>
<span id="cb5-86"><a href="#cb5-86"></a>            next_vals.append(optimal_values[next_state[<span class="dv">0</span>],next_state[<span class="dv">1</span>]])</span>
<span id="cb5-87"><a href="#cb5-87"></a></span>
<span id="cb5-88"><a href="#cb5-88"></a>        best_actions<span class="op">=</span>np.where(next_vals <span class="op">==</span> np.<span class="bu">max</span>(next_vals))[<span class="dv">0</span>]</span>
<span id="cb5-89"><a href="#cb5-89"></a>        val<span class="op">=</span><span class="st">''</span></span>
<span id="cb5-90"><a href="#cb5-90"></a>        <span class="cf">for</span> ba <span class="kw">in</span> best_actions:</span>
<span id="cb5-91"><a href="#cb5-91"></a>            val<span class="op">+=</span>ACTIONS_FIGS[ba]</span>
<span id="cb5-92"><a href="#cb5-92"></a>        </span>
<span id="cb5-93"><a href="#cb5-93"></a>        <span class="co"># add state labels</span></span>
<span id="cb5-94"><a href="#cb5-94"></a>        <span class="cf">if</span> [i, j] <span class="op">==</span> A_POS:</span>
<span id="cb5-95"><a href="#cb5-95"></a>            val <span class="op">=</span> <span class="bu">str</span>(val) <span class="op">+</span> <span class="st">" (A)"</span></span>
<span id="cb5-96"><a href="#cb5-96"></a>        <span class="cf">if</span> [i, j] <span class="op">==</span> A_PRIME_POS:</span>
<span id="cb5-97"><a href="#cb5-97"></a>            val <span class="op">=</span> <span class="bu">str</span>(val) <span class="op">+</span> <span class="st">" (A')"</span></span>
<span id="cb5-98"><a href="#cb5-98"></a>        <span class="cf">if</span> [i, j] <span class="op">==</span> B_POS:</span>
<span id="cb5-99"><a href="#cb5-99"></a>            val <span class="op">=</span> <span class="bu">str</span>(val) <span class="op">+</span> <span class="st">" (B)"</span></span>
<span id="cb5-100"><a href="#cb5-100"></a>        <span class="cf">if</span> [i, j] <span class="op">==</span> B_PRIME_POS:</span>
<span id="cb5-101"><a href="#cb5-101"></a>            val <span class="op">=</span> <span class="bu">str</span>(val) <span class="op">+</span> <span class="st">" (B')"</span></span>
<span id="cb5-102"><a href="#cb5-102"></a>        </span>
<span id="cb5-103"><a href="#cb5-103"></a>        tb.add_cell(i, j, width, height, text<span class="op">=</span>val,</span>
<span id="cb5-104"><a href="#cb5-104"></a>                loc<span class="op">=</span><span class="st">'center'</span>, facecolor<span class="op">=</span><span class="st">'white'</span>)</span>
<span id="cb5-105"><a href="#cb5-105"></a></span>
<span id="cb5-106"><a href="#cb5-106"></a>    <span class="co"># Row and column labels...</span></span>
<span id="cb5-107"><a href="#cb5-107"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(optimal_values)):</span>
<span id="cb5-108"><a href="#cb5-108"></a>        tb.add_cell(i, <span class="op">-</span><span class="dv">1</span>, width, height, text<span class="op">=</span>i<span class="op">+</span><span class="dv">1</span>, loc<span class="op">=</span><span class="st">'right'</span>,</span>
<span id="cb5-109"><a href="#cb5-109"></a>                    edgecolor<span class="op">=</span><span class="st">'none'</span>, facecolor<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb5-110"><a href="#cb5-110"></a>        tb.add_cell(<span class="op">-</span><span class="dv">1</span>, i, width, height<span class="op">/</span><span class="dv">2</span>, text<span class="op">=</span>i<span class="op">+</span><span class="dv">1</span>, loc<span class="op">=</span><span class="st">'center'</span>,</span>
<span id="cb5-111"><a href="#cb5-111"></a>                   edgecolor<span class="op">=</span><span class="st">'none'</span>, facecolor<span class="op">=</span><span class="st">'none'</span>)</span>
<span id="cb5-112"><a href="#cb5-112"></a></span>
<span id="cb5-113"><a href="#cb5-113"></a>    ax.add_table(tb)</span>
<span id="cb5-114"><a href="#cb5-114"></a></span>
<span id="cb5-115"><a href="#cb5-115"></a></span>
<span id="cb5-116"><a href="#cb5-116"></a><span class="kw">def</span> figure_3_2():</span>
<span id="cb5-117"><a href="#cb5-117"></a>    value <span class="op">=</span> np.zeros((WORLD_SIZE, WORLD_SIZE))</span>
<span id="cb5-118"><a href="#cb5-118"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb5-119"><a href="#cb5-119"></a>        <span class="co"># keep iteration until convergence</span></span>
<span id="cb5-120"><a href="#cb5-120"></a>        new_value <span class="op">=</span> np.zeros_like(value)</span>
<span id="cb5-121"><a href="#cb5-121"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(WORLD_SIZE):</span>
<span id="cb5-122"><a href="#cb5-122"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(WORLD_SIZE):</span>
<span id="cb5-123"><a href="#cb5-123"></a>                <span class="cf">for</span> action <span class="kw">in</span> ACTIONS:</span>
<span id="cb5-124"><a href="#cb5-124"></a>                    (next_i, next_j), reward <span class="op">=</span> step([i, j], action)</span>
<span id="cb5-125"><a href="#cb5-125"></a>                    <span class="co"># bellman equation</span></span>
<span id="cb5-126"><a href="#cb5-126"></a>                    new_value[i, j] <span class="op">+=</span> <span class="op">\</span></span>
<span id="cb5-127"><a href="#cb5-127"></a>                        ACTION_PROB <span class="op">*</span> (</span>
<span id="cb5-128"><a href="#cb5-128"></a>                                reward <span class="op">+</span> DISCOUNT <span class="op">*</span> value[next_i, next_j]</span>
<span id="cb5-129"><a href="#cb5-129"></a>                        )</span>
<span id="cb5-130"><a href="#cb5-130"></a>                    </span>
<span id="cb5-131"><a href="#cb5-131"></a>        <span class="cf">if</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(value <span class="op">-</span> new_value)) <span class="op">&lt;</span> <span class="fl">1e-4</span>:</span>
<span id="cb5-132"><a href="#cb5-132"></a>            draw_image(np.<span class="bu">round</span>(new_value, decimals<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb5-133"><a href="#cb5-133"></a>            plt.savefig(<span class="st">'../images/figure_3_2.png'</span>)</span>
<span id="cb5-134"><a href="#cb5-134"></a>            plt.close()</span>
<span id="cb5-135"><a href="#cb5-135"></a>            <span class="cf">break</span></span>
<span id="cb5-136"><a href="#cb5-136"></a>        value <span class="op">=</span> new_value</span>
<span id="cb5-137"><a href="#cb5-137"></a></span>
<span id="cb5-138"><a href="#cb5-138"></a><span class="kw">def</span> figure_3_2_linear_system():</span>
<span id="cb5-139"><a href="#cb5-139"></a>    <span class="co">'''</span></span>
<span id="cb5-140"><a href="#cb5-140"></a><span class="co">    Here we solve the linear system of equations to find the exact solution.</span></span>
<span id="cb5-141"><a href="#cb5-141"></a><span class="co">    We do this by filling the coefficients for each of the states with their respective right side constant.</span></span>
<span id="cb5-142"><a href="#cb5-142"></a><span class="co">    '''</span></span>
<span id="cb5-143"><a href="#cb5-143"></a>    A <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="op">*</span> np.eye(WORLD_SIZE <span class="op">*</span> WORLD_SIZE)</span>
<span id="cb5-144"><a href="#cb5-144"></a>    b <span class="op">=</span> np.zeros(WORLD_SIZE <span class="op">*</span> WORLD_SIZE)</span>
<span id="cb5-145"><a href="#cb5-145"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(WORLD_SIZE):</span>
<span id="cb5-146"><a href="#cb5-146"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(WORLD_SIZE):</span>
<span id="cb5-147"><a href="#cb5-147"></a>            s <span class="op">=</span> [i, j]  <span class="co"># current state</span></span>
<span id="cb5-148"><a href="#cb5-148"></a>            index_s <span class="op">=</span> np.ravel_multi_index(s, (WORLD_SIZE, WORLD_SIZE))</span>
<span id="cb5-149"><a href="#cb5-149"></a>            <span class="cf">for</span> a <span class="kw">in</span> ACTIONS:</span>
<span id="cb5-150"><a href="#cb5-150"></a>                s_, r <span class="op">=</span> step(s, a)</span>
<span id="cb5-151"><a href="#cb5-151"></a>                index_s_ <span class="op">=</span> np.ravel_multi_index(s_, (WORLD_SIZE, WORLD_SIZE))</span>
<span id="cb5-152"><a href="#cb5-152"></a></span>
<span id="cb5-153"><a href="#cb5-153"></a>                A[index_s, index_s_] <span class="op">+=</span> ACTION_PROB <span class="op">*</span> DISCOUNT</span>
<span id="cb5-154"><a href="#cb5-154"></a>                b[index_s] <span class="op">-=</span> ACTION_PROB <span class="op">*</span> r</span>
<span id="cb5-155"><a href="#cb5-155"></a></span>
<span id="cb5-156"><a href="#cb5-156"></a>    x <span class="op">=</span> np.linalg.solve(A, b)</span>
<span id="cb5-157"><a href="#cb5-157"></a>    draw_image(np.<span class="bu">round</span>(x.reshape(WORLD_SIZE, WORLD_SIZE), decimals<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb5-158"><a href="#cb5-158"></a>    plt.savefig(<span class="st">'../images/figure_3_2_linear_system.png'</span>)</span>
<span id="cb5-159"><a href="#cb5-159"></a>    plt.close()</span>
<span id="cb5-160"><a href="#cb5-160"></a></span>
<span id="cb5-161"><a href="#cb5-161"></a><span class="kw">def</span> figure_3_5():</span>
<span id="cb5-162"><a href="#cb5-162"></a>    value <span class="op">=</span> np.zeros((WORLD_SIZE, WORLD_SIZE))</span>
<span id="cb5-163"><a href="#cb5-163"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb5-164"><a href="#cb5-164"></a>        <span class="co"># keep iteration until convergence</span></span>
<span id="cb5-165"><a href="#cb5-165"></a>        new_value <span class="op">=</span> np.zeros_like(value)</span>
<span id="cb5-166"><a href="#cb5-166"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(WORLD_SIZE):</span>
<span id="cb5-167"><a href="#cb5-167"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(WORLD_SIZE):</span>
<span id="cb5-168"><a href="#cb5-168"></a>                values <span class="op">=</span> []</span>
<span id="cb5-169"><a href="#cb5-169"></a>                <span class="cf">for</span> action <span class="kw">in</span> ACTIONS:</span>
<span id="cb5-170"><a href="#cb5-170"></a>                    (next_i, next_j), reward <span class="op">=</span> step([i, j], action)</span>
<span id="cb5-171"><a href="#cb5-171"></a>                    <span class="co"># value iteration</span></span>
<span id="cb5-172"><a href="#cb5-172"></a>                    values.append(reward <span class="op">+</span> DISCOUNT <span class="op">*</span> value[next_i, next_j])</span>
<span id="cb5-173"><a href="#cb5-173"></a>                new_value[i, j] <span class="op">=</span> np.<span class="bu">max</span>(values)</span>
<span id="cb5-174"><a href="#cb5-174"></a>        <span class="cf">if</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(new_value <span class="op">-</span> value)) <span class="op">&lt;</span> <span class="fl">1e-4</span>:</span>
<span id="cb5-175"><a href="#cb5-175"></a>            draw_image(np.<span class="bu">round</span>(new_value, decimals<span class="op">=</span><span class="dv">2</span>))</span>
<span id="cb5-176"><a href="#cb5-176"></a>            plt.savefig(<span class="st">'../images/figure_3_5.png'</span>)</span>
<span id="cb5-177"><a href="#cb5-177"></a>            plt.close()</span>
<span id="cb5-178"><a href="#cb5-178"></a>            draw_policy(new_value)</span>
<span id="cb5-179"><a href="#cb5-179"></a>            plt.savefig(<span class="st">'../images/figure_3_5_policy.png'</span>)</span>
<span id="cb5-180"><a href="#cb5-180"></a>            plt.close()</span>
<span id="cb5-181"><a href="#cb5-181"></a>            <span class="cf">break</span></span>
<span id="cb5-182"><a href="#cb5-182"></a>        value <span class="op">=</span> new_value</span>
<span id="cb5-183"><a href="#cb5-183"></a></span>
<span id="cb5-184"><a href="#cb5-184"></a></span>
<span id="cb5-185"><a href="#cb5-185"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">'__main__'</span>:</span>
<span id="cb5-186"><a href="#cb5-186"></a>    figure_3_2_linear_system()</span>
<span id="cb5-187"><a href="#cb5-187"></a>    figure_3_2()</span>
<span id="cb5-188"><a href="#cb5-188"></a>    figure_3_5()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</div>
<div id="exm-golf" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.2</strong></span> The state is the location of the ball. The value of a state is the negative of the number of strokes to the hole from that location. Our actions are how we aim and swing at the ball, of course, and which club we select. Let us take the former as given and consider just the choice of club, which we assume is either a putter or a driver. The upper part of Figure shows a possible state-value function, <span class="math inline">\(v_{\text{putt}} (s)\)</span>, for the policy that always uses the putter.</p>
</div>
<div id="fig-golf-exm" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-env="figure*">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-golf-exm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="../assets/ch_04/golf_exm.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" data-glightbox="description: .lightbox-desc-6" title="Golf example"><img src="../assets/ch_04/golf_exm.png" title="Golf example." class="img-fluid figure-img" data-fig-env="figure"></a></p>
<figcaption>Golf example</figcaption>
</figure>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-golf-exm-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4.4: Gol example. At top the reward value of each state (ball curren position).
</figcaption>
</figure>
</div>
</section>
</section>
<section id="optimal-policies-and-optimal-value-functions" class="level1">
<h1>Optimal Policies and Optimal Value Functions</h1>
</section>
<section id="optimality-and-approximation" class="level1">
<h1>Optimality and Approximation</h1>
</section>
<section id="summary-1" class="level1 page-columns page-full">
<h1>Summary</h1>
<section id="bibliography" class="level2 sectionbibliography page-columns page-full">
<h2 class="sectionbibliography anchored" data-anchor-id="bibliography">Bibliography</h2>
<div id="refs--7" class="sectionrefs references csl-bib-body" data-entry-spacing="0" role="list">

</div>


<!-- -->

<div class="hidden" aria-hidden="true">
<span class="glightbox-desc lightbox-desc-1">Figure&nbsp;1: Recycling-robot’s graph. Taken from <span class="citation" data-cites="Sutton2018--7">[<a href="../references.html#ref-Sutton2018--7" role="doc-biblioref">1</a>]</span></span>
<span class="glightbox-desc lightbox-desc-2">Figure&nbsp;2: </span>
<span class="glightbox-desc lightbox-desc-3">Pick and Drop game</span>
<span class="glightbox-desc lightbox-desc-4">Rewards rules for the drop game</span>
<span class="glightbox-desc lightbox-desc-5">Grid World example. Problem scheme , possible actions and sampling reward</span>
<span class="glightbox-desc lightbox-desc-6">Golf example</span>
</div>
<div class="no-row-height column-margin column-container"><div id="ref-Sutton2018--7" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">R.S. Sutton, A.G. Barto, Reinforcement learning: An introduction, Second, MIT Press, Cambridge, MA, 2018.</div>
</div></div></section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/sauldiazinfante\.github\.io\/RL-Course-2024-2\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../03-multiArmedBandit/multiarmed_bandits.html" class="pagination-link" aria-label="Multi-armed Bandits">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Multi-armed Bandits</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../05-dynamicProgramming/dp_rl.html" class="pagination-link" aria-label="Dynamic Programming (DP)">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Dynamic Programming (DP)</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb6" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb6-1"><a href="#cb6-1"></a><span class="co">---</span></span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="an">title:</span><span class="co"> "Finite Markov Decision Processes (MDPs)"</span></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="an">author:</span><span class="co"> "Saúl Díaz Infante Velasco"</span></span>
<span id="cb6-4"><a href="#cb6-4"></a><span class="an">format:</span></span>
<span id="cb6-5"><a href="#cb6-5"></a><span class="co">  html:</span></span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="co">    grid:</span></span>
<span id="cb6-7"><a href="#cb6-7"></a><span class="co">      margin-width: 350px</span></span>
<span id="cb6-8"><a href="#cb6-8"></a><span class="co">  pdf: default</span></span>
<span id="cb6-9"><a href="#cb6-9"></a><span class="co">section-bibiliograpies.cleanup-first: true</span></span>
<span id="cb6-10"><a href="#cb6-10"></a><span class="an">reference-location:</span><span class="co"> margin</span></span>
<span id="cb6-11"><a href="#cb6-11"></a><span class="an">citation-location:</span><span class="co"> margin</span></span>
<span id="cb6-12"><a href="#cb6-12"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb6-13"><a href="#cb6-13"></a><span class="an">number-depth:</span><span class="co"> 3</span></span>
<span id="cb6-14"><a href="#cb6-14"></a><span class="co">---</span></span>
<span id="cb6-15"><a href="#cb6-15"></a></span>
<span id="cb6-16"><a href="#cb6-16"></a>That is what ChatGPT would answer to a 5-year-old kid. Alright, let's imagine</span>
<span id="cb6-17"><a href="#cb6-17"></a>you have a little robot friend named Robo. Robo likes to explore and do</span>
<span id="cb6-18"><a href="#cb6-18"></a>different things, but Robo doesn't always know what to do next. A Markov</span>
<span id="cb6-19"><a href="#cb6-19"></a>Decision Process (MDP) is like giving Robo a set of rules to help it decide what</span>
<span id="cb6-20"><a href="#cb6-20"></a>to do next based on where it is and what it knows.</span>
<span id="cb6-21"><a href="#cb6-21"></a></span>
<span id="cb6-22"><a href="#cb6-22"></a>Imagine Robo is in a room full of toys. Each toy is like a different choice Robo</span>
<span id="cb6-23"><a href="#cb6-23"></a>can make, like playing with blocks or reading a book. But Robo can't see the</span>
<span id="cb6-24"><a href="#cb6-24"></a>whole room at once, so it has to decide what to do based on what it can see and</span>
<span id="cb6-25"><a href="#cb6-25"></a>remember.</span>
<span id="cb6-26"><a href="#cb6-26"></a></span>
<span id="cb6-27"><a href="#cb6-27"></a>In an MDP, Robo learns from its past experiences. If it finds that playing with</span>
<span id="cb6-28"><a href="#cb6-28"></a>blocks usually makes it happy, it's more likely to choose that again next time.</span>
<span id="cb6-29"><a href="#cb6-29"></a>But if it tries reading a book and doesn't like it, it might choose something</span>
<span id="cb6-30"><a href="#cb6-30"></a>else next time.</span>
<span id="cb6-31"><a href="#cb6-31"></a></span>
<span id="cb6-32"><a href="#cb6-32"></a>So, a Markov Decision Process helps Robo make decisions by learning from what</span>
<span id="cb6-33"><a href="#cb6-33"></a>it's done before and what it can see around it, kind of like how you learn from</span>
<span id="cb6-34"><a href="#cb6-34"></a>playing with different toys and remembering which ones you like best.</span>
<span id="cb6-35"><a href="#cb6-35"></a></span>
<span id="cb6-36"><a href="#cb6-36"></a><span class="fu">## The Agent–Environment Interface</span></span>
<span id="cb6-37"><a href="#cb6-37"></a></span>
<span id="cb6-38"><a href="#cb6-38"></a>::: {#exm-roboClean}</span>
<span id="cb6-39"><a href="#cb6-39"></a>A mobile robot has the job of collecting empty soda cans in an office</span>
<span id="cb6-40"><a href="#cb6-40"></a>environment. It has sensors for detecting cans, and an arm and gripper that can</span>
<span id="cb6-41"><a href="#cb6-41"></a>pick them up and place them in an onboard bin; it runs on a rechargeable</span>
<span id="cb6-42"><a href="#cb6-42"></a>battery. The robot’s control system has components for interpreting sensory</span>
<span id="cb6-43"><a href="#cb6-43"></a>information, for navigating, and for controlling the arm and gripper. High-level</span>
<span id="cb6-44"><a href="#cb6-44"></a>decisions about how to search for cans are made by a reinforcement learning</span>
<span id="cb6-45"><a href="#cb6-45"></a>agent based on the current charge level of the battery. To make a simple</span>
<span id="cb6-46"><a href="#cb6-46"></a>example, we assume that only two charge levels can be distinguished, comprising</span>
<span id="cb6-47"><a href="#cb6-47"></a>a small state set $\mathcal{S} = <span class="sc">\{</span>\texttt{high}, \texttt{low} <span class="sc">\}</span>$. In each</span>
<span id="cb6-48"><a href="#cb6-48"></a>state, the agent can decide whether to</span>
<span id="cb6-49"><a href="#cb6-49"></a></span>
<span id="cb6-50"><a href="#cb6-50"></a><span class="ss">1.  </span>actively **search** for a can for a certain period of time,</span>
<span id="cb6-51"><a href="#cb6-51"></a><span class="ss">2.  </span>remain stationary and **wait** for someone to bring it a can, or</span>
<span id="cb6-52"><a href="#cb6-52"></a><span class="ss">3.  </span>head back to its home base to **recharge** its battery.</span>
<span id="cb6-53"><a href="#cb6-53"></a></span>
<span id="cb6-54"><a href="#cb6-54"></a>When the energy level is **high**, recharging would always be foolish h, so we</span>
<span id="cb6-55"><a href="#cb6-55"></a>do not include it in the action set for this state. The action sets are then</span>
<span id="cb6-56"><a href="#cb6-56"></a>$\mathcal{A}(\texttt{high}) = <span class="sc">\{</span>\texttt{search}, \texttt{wait}<span class="sc">\}</span>$ and</span>
<span id="cb6-57"><a href="#cb6-57"></a>$\mathcal{A}(\texttt{low}) = <span class="sc">\{</span>\texttt{search}, \texttt{wait}, \texttt{recharge}<span class="sc">\}</span>$.</span>
<span id="cb6-58"><a href="#cb6-58"></a></span>
<span id="cb6-59"><a href="#cb6-59"></a>The rewards are zero most of the time, but become positive when the robot</span>
<span id="cb6-60"><a href="#cb6-60"></a>secures an empty can, or large and negative if the battery runs all the way</span>
<span id="cb6-61"><a href="#cb6-61"></a>down. The best way to find cans is to actively search for them, but this runs</span>
<span id="cb6-62"><a href="#cb6-62"></a>down the robot’s battery, whereas waiting does not. Whenever the robot is</span>
<span id="cb6-63"><a href="#cb6-63"></a>searching, the possibility exists that its battery will become depleted. In this</span>
<span id="cb6-64"><a href="#cb6-64"></a>case the robot must shut down and wait to be rescued (producing a low reward).</span>
<span id="cb6-65"><a href="#cb6-65"></a></span>
<span id="cb6-66"><a href="#cb6-66"></a>If the energy level is **high**, then a period of active search can always be</span>
<span id="cb6-67"><a href="#cb6-67"></a>completed without risk of depleting the battery. A period of searching that</span>
<span id="cb6-68"><a href="#cb6-68"></a>begins with a **high** energy level leaves the energy level high **with**</span>
<span id="cb6-69"><a href="#cb6-69"></a>probability $\alpha$ and reduces it to low with probability $1 - \alpha$. On the</span>
<span id="cb6-70"><a href="#cb6-70"></a>other hand, a period of searching undertaken when the energy level is low leaves</span>
<span id="cb6-71"><a href="#cb6-71"></a>it low with probability $\beta$ and depletes the battery with probability</span>
<span id="cb6-72"><a href="#cb6-72"></a>$1 - \beta$. In the latter case, the robot must be rescued, and the battery is</span>
<span id="cb6-73"><a href="#cb6-73"></a>then recharged back to **high**. Each can collected by the robot counts as a</span>
<span id="cb6-74"><a href="#cb6-74"></a>unit reward, whereas a reward of $-3$ results whenever the robot has to be</span>
<span id="cb6-75"><a href="#cb6-75"></a>rescued. Let $r_{\texttt{search}}$ and $r_{\texttt{wait}}$, with</span>
<span id="cb6-76"><a href="#cb6-76"></a>$r_{\texttt{search}} &gt; r_{\texttt{wait}}$, denote the expected numbers of cans</span>
<span id="cb6-77"><a href="#cb6-77"></a>the robot will collect (and hence the expected reward) while searching and while</span>
<span id="cb6-78"><a href="#cb6-78"></a>waiting respectively. Finally, suppose that no cans can be collected during a</span>
<span id="cb6-79"><a href="#cb6-79"></a>run home for recharging, and that no cans can be collected on a step in which</span>
<span id="cb6-80"><a href="#cb6-80"></a>the battery is depleted. This system is then a finite MDP, and we can write down</span>
<span id="cb6-81"><a href="#cb6-81"></a>the transition probabilities and the expected rewards, with dynamics as</span>
<span id="cb6-82"><a href="#cb6-82"></a>indicated in the table on the left:</span>
<span id="cb6-83"><a href="#cb6-83"></a></span>
<span id="cb6-84"><a href="#cb6-84"></a>::: {#fig-robot-diagram fig-env="figure*"}</span>
<span id="cb6-85"><a href="#cb6-85"></a><span class="al">![](../assets/ch_03/recycling_robot_diagram.png)</span>{.lightbox}</span>
<span id="cb6-86"><a href="#cb6-86"></a></span>
<span id="cb6-87"><a href="#cb6-87"></a>Recycling-robot's graph. Taken from <span class="co">[</span><span class="ot">@Sutton2018</span><span class="co">]</span></span>
<span id="cb6-88"><a href="#cb6-88"></a>:::</span>
<span id="cb6-89"><a href="#cb6-89"></a></span>
<span id="cb6-90"><a href="#cb6-90"></a>::: {#fig-robot-table fig-env="figure*"}</span>
<span id="cb6-91"><a href="#cb6-91"></a>![Transitions and rewards for the recycling</span>
<span id="cb6-92"><a href="#cb6-92"></a>robot](/assets/ch_03/recycling_robot_table.png){.lightbox}</span>
<span id="cb6-93"><a href="#cb6-93"></a>Transitions and rewards for the recycling</span>
<span id="cb6-94"><a href="#cb6-94"></a>robot</span>
<span id="cb6-95"><a href="#cb6-95"></a>:::</span>
<span id="cb6-96"><a href="#cb6-96"></a></span>
<span id="cb6-97"><a href="#cb6-97"></a>:::</span>
<span id="cb6-98"><a href="#cb6-98"></a></span>
<span id="cb6-99"><a href="#cb6-99"></a><span class="fu"># Goals and Rewards</span></span>
<span id="cb6-100"><a href="#cb6-100"></a></span>
<span id="cb6-101"><a href="#cb6-101"></a>In reinforcement learning, **the reward hypothesis** serves as the foundation</span>
<span id="cb6-102"><a href="#cb6-102"></a>for defining the objectives of an agent operating within an environment.</span>
<span id="cb6-103"><a href="#cb6-103"></a>According to this hypothesis, the agent's goal can be represented by the</span>
<span id="cb6-104"><a href="#cb6-104"></a>maximization of the **expected cumulative reward** over time, based on scalar</span>
<span id="cb6-105"><a href="#cb6-105"></a>feedback signals received from the environment.</span>
<span id="cb6-106"><a href="#cb6-106"></a></span>
<span id="cb6-107"><a href="#cb6-107"></a><span class="fu">## Key Points of the Reward Hypothesis:</span></span>
<span id="cb6-108"><a href="#cb6-108"></a></span>
<span id="cb6-109"><a href="#cb6-109"></a><span class="ss">1.  </span>**Reward as a Scalar Signal**: At every time step, the environment provides</span>
<span id="cb6-110"><a href="#cb6-110"></a>    the agent with a simple numerical signal, $(R_t \in \mathbb{R}$, which</span>
<span id="cb6-111"><a href="#cb6-111"></a>    represents the immediate reward based on the agent’s actions and the current</span>
<span id="cb6-112"><a href="#cb6-112"></a>    state of the environment. This reward acts as feedback for the agent,</span>
<span id="cb6-113"><a href="#cb6-113"></a>    helping it learn and adjust its strategy to achieve its ultimate goal.</span>
<span id="cb6-114"><a href="#cb6-114"></a></span>
<span id="cb6-115"><a href="#cb6-115"></a><span class="ss">2.  </span>**Maximizing Cumulative Reward**: The agent’s goal is not just to maximize</span>
<span id="cb6-116"><a href="#cb6-116"></a>    the immediate reward, but to focus on the long-term sum of rewards, known as</span>
<span id="cb6-117"><a href="#cb6-117"></a>    the **cumulative reward**. This ensures that the agent does not become</span>
<span id="cb6-118"><a href="#cb6-118"></a>    short-sighted by only pursuing short-term benefits, but rather seeks</span>
<span id="cb6-119"><a href="#cb6-119"></a>    strategies that maximize its total reward across time.</span>
<span id="cb6-120"><a href="#cb6-120"></a></span>
<span id="cb6-121"><a href="#cb6-121"></a><span class="ss">3.  </span>**Expected Value of the Reward**: Since reinforcement learning involves</span>
<span id="cb6-122"><a href="#cb6-122"></a>    interaction in environments that can be stochastic (involving randomness or</span>
<span id="cb6-123"><a href="#cb6-123"></a>    uncertainty), the agent aims to maximize the **expected value** of the</span>
<span id="cb6-124"><a href="#cb6-124"></a>    cumulative reward, accounting for different possible future states and</span>
<span id="cb6-125"><a href="#cb6-125"></a>    outcomes. This means the agent is interested in the average cumulative</span>
<span id="cb6-126"><a href="#cb6-126"></a>    reward it would obtain over many possible sequences of interactions, rather</span>
<span id="cb6-127"><a href="#cb6-127"></a>    than specific individual outcomes.</span>
<span id="cb6-128"><a href="#cb6-128"></a></span>
<span id="cb6-129"><a href="#cb6-129"></a><span class="ss">4.  </span>**Formalizing Goals and Purposes**: According to the reward hypothesis, all</span>
<span id="cb6-130"><a href="#cb6-130"></a>    goals, objectives, or purposes of the agent can be **quantified** by</span>
<span id="cb6-131"><a href="#cb6-131"></a>    maximizing this cumulative scalar signal (reward). In other words, the</span>
<span id="cb6-132"><a href="#cb6-132"></a>    "purpose" of the agent is simply to optimize the feedback it receives in the</span>
<span id="cb6-133"><a href="#cb6-133"></a>    form of rewards, and this concept encapsulates everything the agent is</span>
<span id="cb6-134"><a href="#cb6-134"></a>    designed to achieve.</span>
<span id="cb6-135"><a href="#cb6-135"></a></span>
<span id="cb6-136"><a href="#cb6-136"></a><span class="fu">## Importance in Reinforcement Learning:</span></span>
<span id="cb6-137"><a href="#cb6-137"></a></span>
<span id="cb6-138"><a href="#cb6-138"></a>This hypothesis is central to how reinforcement learning problems are</span>
<span id="cb6-139"><a href="#cb6-139"></a>structured. It reduces complex goals and objectives into a single, scalar value</span>
<span id="cb6-140"><a href="#cb6-140"></a>(the reward) that the agent can track and optimize over time. This abstraction</span>
<span id="cb6-141"><a href="#cb6-141"></a>makes it possible to design agents that can handle a wide variety of tasks, as</span>
<span id="cb6-142"><a href="#cb6-142"></a>long as those tasks can be expressed in terms of rewards provided by the</span>
<span id="cb6-143"><a href="#cb6-143"></a>environment.</span>
<span id="cb6-144"><a href="#cb6-144"></a></span>
<span id="cb6-145"><a href="#cb6-145"></a><span class="fu"># Returns and Episodes</span></span>
<span id="cb6-146"><a href="#cb6-146"></a></span>
<span id="cb6-147"><a href="#cb6-147"></a>To formalize the objective of learning in reinforcement learning, we introduce</span>
<span id="cb6-148"><a href="#cb6-148"></a>the concept of **return**. The **return**, denoted $G_t$, is a function of the</span>
<span id="cb6-149"><a href="#cb6-149"></a>future rewards that the agent will receive after time step $t$. The agent aims</span>
<span id="cb6-150"><a href="#cb6-150"></a>to maximize the **expected return** to achieve its goal. Let’s break down this</span>
<span id="cb6-151"><a href="#cb6-151"></a>formalization step by step:</span>
<span id="cb6-152"><a href="#cb6-152"></a></span>
<span id="cb6-153"><a href="#cb6-153"></a><span class="fu">## 1. **The Reward Sequence**:</span></span>
<span id="cb6-154"><a href="#cb6-154"></a></span>
<span id="cb6-155"><a href="#cb6-155"></a>At each time step $t$, the agent receives a reward $R_{t+1}, R_{t+2},\dots,$ as</span>
<span id="cb6-156"><a href="#cb6-156"></a>a result of interacting with the environment. This sequence of rewards</span>
<span id="cb6-157"><a href="#cb6-157"></a>represents the feedback the agent receives over time based on its actions.</span>
<span id="cb6-158"><a href="#cb6-158"></a></span>
<span id="cb6-159"><a href="#cb6-159"></a><span class="fu">## 2. **Defining the Return**:</span></span>
<span id="cb6-160"><a href="#cb6-160"></a></span>
<span id="cb6-161"><a href="#cb6-161"></a>The **return** at time step $t$, denoted $G_t$, is the total accumulated reward</span>
<span id="cb6-162"><a href="#cb6-162"></a>from time step $t$ onward. In reinforcement learning, the return can be defined</span>
<span id="cb6-163"><a href="#cb6-163"></a>in different ways, but it generally involves summing the future rewards, often</span>
<span id="cb6-164"><a href="#cb6-164"></a>discounted to account for the uncertainty or diminishing value of rewards</span>
<span id="cb6-165"><a href="#cb6-165"></a>received further in the future.</span>
<span id="cb6-166"><a href="#cb6-166"></a></span>
<span id="cb6-167"><a href="#cb6-167"></a>The **undiscounted return** would simply be the sum of all future rewards: $$</span>
<span id="cb6-168"><a href="#cb6-168"></a>   G_t = R_{t+1} + R_{t+2} + R_{t+3} + \cdots = \sum_{k=0}^{\infty} R_{t+k+1}</span>
<span id="cb6-169"><a href="#cb6-169"></a>$$ This sum may be infinite if the task never ends, which can be problematic.</span>
<span id="cb6-170"><a href="#cb6-170"></a>Thus, in many cases, a **discount factor** is applied to weight future rewards</span>
<span id="cb6-171"><a href="#cb6-171"></a>less than immediate rewards.</span>
<span id="cb6-172"><a href="#cb6-172"></a></span>
<span id="cb6-173"><a href="#cb6-173"></a>Likewise if the MDP has finite horizont $T$ then</span>
<span id="cb6-174"><a href="#cb6-174"></a>$$G_t = R_{t+1} + R_{t+2} + R_{t+3} + \cdots + R_{T}$$</span>
<span id="cb6-175"><a href="#cb6-175"></a></span>
<span id="cb6-176"><a href="#cb6-176"></a><span class="fu">## 3. **Discounted Return**:</span></span>
<span id="cb6-177"><a href="#cb6-177"></a></span>
<span id="cb6-178"><a href="#cb6-178"></a>To address this issue, we introduce a discount factor $\gamma$, where</span>
<span id="cb6-179"><a href="#cb6-179"></a>$0\leq \gamma \leq 1$. The discount factor controls how much emphasis is placed</span>
<span id="cb6-180"><a href="#cb6-180"></a>on future rewards. When $\gamma$ is close to 1, future rewards are considered</span>
<span id="cb6-181"><a href="#cb6-181"></a>nearly as valuable as immediate rewards. When $\gamma$ is closer to 0, the agent</span>
<span id="cb6-182"><a href="#cb6-182"></a>focuses more on immediate rewards.</span>
<span id="cb6-183"><a href="#cb6-183"></a></span>
<span id="cb6-184"><a href="#cb6-184"></a>The **discounted return** is defined as: $$</span>
<span id="cb6-185"><a href="#cb6-185"></a>G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots </span>
<span id="cb6-186"><a href="#cb6-186"></a>    = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}</span>
<span id="cb6-187"><a href="#cb6-187"></a>$$ Here, each future reward is multiplied by ( \gamma\^k ), where ( k ) is the</span>
<span id="cb6-188"><a href="#cb6-188"></a>number of time steps into the future. This ensures that the agent values</span>
<span id="cb6-189"><a href="#cb6-189"></a>immediate rewards more highly than rewards far into the future, which is often</span>
<span id="cb6-190"><a href="#cb6-190"></a>desirable in practical applications.</span>
<span id="cb6-191"><a href="#cb6-191"></a></span>
<span id="cb6-192"><a href="#cb6-192"></a><span class="fu">## 4. **Maximizing the Expected Return**:</span></span>
<span id="cb6-193"><a href="#cb6-193"></a></span>
<span id="cb6-194"><a href="#cb6-194"></a>Since the environment in reinforcement learning is often stochastic, the agent</span>
<span id="cb6-195"><a href="#cb6-195"></a>cannot guarantee a specific sequence of rewards, but it can aim to maximize the</span>
<span id="cb6-196"><a href="#cb6-196"></a>**expected return**. The expected return is the average return the agent would</span>
<span id="cb6-197"><a href="#cb6-197"></a>obtain by following a specific policy $\pi$, which defines the agent’s behavior.</span>
<span id="cb6-198"><a href="#cb6-198"></a></span>
<span id="cb6-199"><a href="#cb6-199"></a>Formally, the agent’s objective is to maximize the expected return: $$</span>
<span id="cb6-200"><a href="#cb6-200"></a>    \mathbb{E}<span class="co">[</span><span class="ot">G_t | \pi</span><span class="co">]</span> </span>
<span id="cb6-201"><a href="#cb6-201"></a>            = \mathbb{E}\left<span class="co">[</span><span class="ot">\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \Big| \pi\right</span><span class="co">]</span></span>
<span id="cb6-202"><a href="#cb6-202"></a>$$ where $\pi$ is the policy being followed. This equation tells us that the</span>
<span id="cb6-203"><a href="#cb6-203"></a>agent should choose actions in a way that maximizes the long-term expected</span>
<span id="cb6-204"><a href="#cb6-204"></a>reward, considering both immediate and future rewards.</span>
<span id="cb6-205"><a href="#cb6-205"></a></span>
<span id="cb6-206"><a href="#cb6-206"></a><span class="fu">## 5. **Two Types of Tasks**:</span></span>
<span id="cb6-207"><a href="#cb6-207"></a></span>
<span id="cb6-208"><a href="#cb6-208"></a><span class="ss">-   </span>**Finite-Horizon Tasks**: These tasks have a fixed time limit, and the</span>
<span id="cb6-209"><a href="#cb6-209"></a>    agent’s goal is to maximize the sum of rewards within that time limit. In</span>
<span id="cb6-210"><a href="#cb6-210"></a>    such cases, the discount factor $\gamma$ may not be necessary, and the</span>
<span id="cb6-211"><a href="#cb6-211"></a>    return is just the sum of the finite rewards received before the task ends.</span>
<span id="cb6-212"><a href="#cb6-212"></a><span class="ss">-   </span>**Infinite-Horizon Tasks**: In tasks that continue indefinitely, the</span>
<span id="cb6-213"><a href="#cb6-213"></a>    discount factor $\gamma$ ensures that the return remains finite by reducing</span>
<span id="cb6-214"><a href="#cb6-214"></a>    the impact of rewards far into the future.</span>
<span id="cb6-215"><a href="#cb6-215"></a></span>
<span id="cb6-216"><a href="#cb6-216"></a><span class="fu">## Conclusion:</span></span>
<span id="cb6-217"><a href="#cb6-217"></a></span>
<span id="cb6-218"><a href="#cb6-218"></a>Thus, in reinforcement learning, the agent's formal objective is to maximize the</span>
<span id="cb6-219"><a href="#cb6-219"></a>**expected return**, $\mathbb{E}<span class="co">[</span><span class="ot">G_t</span><span class="co">]</span>$, where the return $G_t$ is the</span>
<span id="cb6-220"><a href="#cb6-220"></a>**discounted sum** of future rewards. The use of the discount factor $\gamma$</span>
<span id="cb6-221"><a href="#cb6-221"></a>helps the agent focus more on immediate rewards, while still considering future</span>
<span id="cb6-222"><a href="#cb6-222"></a>rewards to some degree. This formalization ensures that the agent’s behavior is</span>
<span id="cb6-223"><a href="#cb6-223"></a>guided not just by immediate rewards but by a balanced approach to long-term</span>
<span id="cb6-224"><a href="#cb6-224"></a>success.</span>
<span id="cb6-225"><a href="#cb6-225"></a></span>
<span id="cb6-226"><a href="#cb6-226"></a><span class="fu"># Unified Notation for Episodic and Continuing Tasks</span></span>
<span id="cb6-227"><a href="#cb6-227"></a></span>
<span id="cb6-228"><a href="#cb6-228"></a><span class="fu"># Policies and Value Functions</span></span>
<span id="cb6-229"><a href="#cb6-229"></a></span>
<span id="cb6-230"><a href="#cb6-230"></a>{{&lt; include policy_and_value_functions.qmd &gt;}}</span>
<span id="cb6-231"><a href="#cb6-231"></a></span>
<span id="cb6-232"><a href="#cb6-232"></a><span class="fu">## Pick and drop example</span></span>
<span id="cb6-233"><a href="#cb6-233"></a></span>
<span id="cb6-234"><a href="#cb6-234"></a>{{&lt; include pick_and_drop_exm.qmd &gt;}}</span>
<span id="cb6-235"><a href="#cb6-235"></a></span>
<span id="cb6-236"><a href="#cb6-236"></a>{{&lt; include gridworld_exm.qmd &gt;}}</span>
<span id="cb6-237"><a href="#cb6-237"></a></span>
<span id="cb6-238"><a href="#cb6-238"></a>{{&lt; include golf_exm.qmd &gt;}}</span>
<span id="cb6-239"><a href="#cb6-239"></a></span>
<span id="cb6-240"><a href="#cb6-240"></a></span>
<span id="cb6-241"><a href="#cb6-241"></a><span class="fu"># Optimal Policies and Optimal Value Functions</span></span>
<span id="cb6-242"><a href="#cb6-242"></a></span>
<span id="cb6-243"><a href="#cb6-243"></a><span class="fu"># Optimality and Approximation</span></span>
<span id="cb6-244"><a href="#cb6-244"></a></span>
<span id="cb6-245"><a href="#cb6-245"></a><span class="fu"># Summary</span></span>
<span id="cb6-246"><a href="#cb6-246"></a></span>
<span id="cb6-247"><a href="#cb6-247"></a><span class="fu">## Bibliography {.sectionbibliography}</span></span>
<span id="cb6-248"><a href="#cb6-248"></a></span>
<span id="cb6-249"><a href="#cb6-249"></a></span>
<span id="cb6-250"><a href="#cb6-250"></a>::: {.sectionrefs}</span>
<span id="cb6-251"><a href="#cb6-251"></a>:::</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>This notes for RL, are the first draft of for the course: From Markov Decision Processes to Reinforcement Learning</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/SaulDiazInfante/RL-Course-2024-2.git/edit/main/04-finiteMDPs/mdp.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/SaulDiazInfante/RL-Course-2024-2.git/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","loop":false,"openEffect":"zoom","selector":".lightbox","descPosition":"bottom"});
window.onload = () => {
  lightboxQuarto.on('slide_before_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    const href = trigger.getAttribute('href');
    if (href !== null) {
      const imgEl = window.document.querySelector(`a[href="${href}"] img`);
      if (imgEl !== null) {
        const srcAttr = imgEl.getAttribute("src");
        if (srcAttr && srcAttr.startsWith("data:")) {
          slideConfig.href = srcAttr;
        }
      }
    } 
  });

  lightboxQuarto.on('slide_after_load', (data) => {
    const { slideIndex, slideNode, slideConfig, player, trigger } = data;
    if (window.Quarto?.typesetMath) {
      window.Quarto.typesetMath(slideNode);
    }
  });

};
          </script>
    <script type="text/javascript">
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let pseudocodeOptions = {
          indentSize: el.dataset.indentSize || "1.2em",
          commentDelimiter: el.dataset.commentDelimiter || "//",
          lineNumber: el.dataset.lineNumber === "true" ? true : false,
          lineNumberPunc: el.dataset.lineNumberPunc || ":",
          noEnd: el.dataset.noEnd === "true" ? true : false,
          titlePrefix: el.dataset.captionPrefix || "Algorithm"
        };
        pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
      });
    })(document);
    (function(d) {
      d.querySelectorAll(".pseudocode-container").forEach(function(el) {
        let captionSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
        if (captionSpan !== null) {
          let captionPrefix = el.dataset.captionPrefix + " ";
          let captionNumber = "";
          if (el.dataset.pseudocodeNumber) {
            captionNumber = el.dataset.pseudocodeNumber + " ";
            if (el.dataset.chapterLevel) {
              captionNumber = el.dataset.chapterLevel + "." + captionNumber;
            }
          }
          captionSpan.innerHTML = captionPrefix + captionNumber;
        }
      });
    })(document);
    </script>
  




</body></html>