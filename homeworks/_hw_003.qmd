## Homework 003 due date: November 08, 2024-12:00:00

::: {#exr-hw_003_01}

 Suppose $\gamma= 0.5$ and the following sequence of rewards is received 
 $R_1 = 1$,$R_2 = 2$, $R_3 = 6$, $R_4 = 3$, and $R_5 = 2$, with $T = 5$. 
 What are $G_0 , G_1 , \cdots, G_5$? [^_hw_003-1]

:::

::: {#exr--hw_003_02}

Give a table analogous to that in [p.53, @Sutton2018], but for
$p(s_0 , r |s, a)$. It should have columns for $s$, $a$, $s_0$ , $r$, and
$p(s_0 , r |s, a)$, and a row for every 4-tuple for which
$p(s_0 , r |s, a) > 0$.

:::


::: {#exr--hw_003_03}

If the current state is $S_t$ , and actions are selected according to a stochastic
policy $\pi$, then what is the expectation of $R_{t+1}$ in terms of $\pi$ and the four-argument
function $p$ in [(3.2) p.48, @Sutton2018]?

:::

Let the value function of a state $s$ under a policy $\pi$ denoted and defined by
$$
\begin{aligned}
	v_{\pi}(s):=  &
		\mathbb{E_{\pi}}
		\left[
			G_t | S_t = s
		\right]
		\\
		= &
		\mathbb{E_{\pi}}
		\left[
			\sum_{k=0}^{\infty}
				R_{t+1+k}
				\Big |
				S_t = s
		\right], \quad \text{for all } s\in \mathcal{S}.
\end{aligned}
$$
Similarly  we denote and define the value of taking action $a$ in state $s$ under a 
policy $\pi$ by
$$
\begin{aligned}
	q_{\pi}(s,a):=  &
		\mathbb{E_{\pi}}
		\left[
			G_t | S_t = s, A_t = a
		\right]
		\\
		= &
		\mathbb{E_{\pi}}
		\left[
			\sum_{k=0}^{\infty}
				R_{t+1+k}
				\Big |
				S_t = s, A_t = a
		\right], \quad \text{for all } (s,a) \in \mathcal{S}\times \mathcal{A}(s).
\end{aligned}
$$

::: {#exr--hw_003_04}

Give an equation for $v_{\pi}$ in terms of $q_{\pi}$ and $\pi$.

:::

::: {#exr--hw_003_05}

1. Give an equation for $q_{\pi}$ in terms of $v_{\pi}$ and the four-argument 
$p(\cdot,\cdot|\cdot,\cdot)$
2. Give the Bellman equation for $q_{\pi}$ for the recycling robot.

:::

::: {#exr--hw_003_06}

Document the numerical experiment regarding to the Gridworld example

:::


[^_hw_003-1]: Hint: Work backwards.
 