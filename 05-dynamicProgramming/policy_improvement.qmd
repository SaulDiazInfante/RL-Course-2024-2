---
format:
  html:
    grid:
      margin-width: 350px
  pdf: default
reference-location: margin
citation-location: margin
number-sections: true

number-depth: 3
---

Our reason for computing the value function for a policy is to help find better 
policies. Suppose we have determined the value function $v_{\pi}$ for an 
arbitrary deterministic policy $\pi$. For some state $s$ we would like to know
whether or not we should change the policy to deterministically choose an action
$a$ such that 

$$
	\begin{equation*}
		a 
			\neq \pi(s).
	\end{equation*}
$$ 

## The policy improvement Theorem

Because we know how good is to follow the current policy 
$\pi$ from $s$--that is $v_{\pi}(s)$--we can estimate if it better or not to 
change to a new policy $\pi^{\prime}$ which is equal the the original policy 
$\pi$ except at state $s$. To this end we can sellect $a = \pi^{\prime}(s)$ always
$s$ appears and there after use the original policy $\pi$. The new value of this 
argument results

$$
\begin{aligned}
	q_{\pi}(s, a) 
		:=& 
			\mathbb{E} 
				\left[
					R_{t+1} + \gamma v_{\pi}(S_{t+1})
					| S_t=s ,A_t=a
				\right]
		\\
		=&
			\sum_{s^{\prime},r}
				p(s^{\prime}, r | s,a)
				\left[
					r + \gamma v_{\pi}(s^{\prime})
				\right].
	\end{aligned}
$$ {#eq-policy-improvement}

Regarding this line of thinking we have the following result

:::{#thm-policy-improvement}

## policy improvement
Let $\pi$ $\pi^{\prime}$ two deterministic given policies such that for all
$s \in \mathcal{S}$, 
$$
	q_{\pi}(s, \pi^{\prime})(s) \geq v_{\pi}(s).
$$ {#eq-policy_improvement_hyp}

Then the policy $\pi^{\prime}$ satisfies

$$
	v_{\pi^{\prime}} (s) \geq  v_{\pi} (s) \qquad \text{for all } s\in\mathcal{S}.
$$ {#eq-policy_improvement_thesis}



:::